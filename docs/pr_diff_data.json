{
  "pr": 5,
  "base_branch": "main",
  "head_branch": "factory/v1",
  "head_sha": "efbf3d4",
  "total_files": 95,
  "total_additions": 5492,
  "total_deletions": 675,
  "files": {
    ".claude/launch.json": {
      "additions": 11,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/.claude/launch.json b/.claude/launch.json\nnew file mode 100644\nindex 0000000..9784083\n--- /dev/null\n+++ b/.claude/launch.json\n@@ -0,0 +1,11 @@\n+{\n+  \"version\": \"0.0.1\",\n+  \"configurations\": [\n+    {\n+      \"name\": \"dashboard\",\n+      \"runtimeExecutable\": \"streamlit\",\n+      \"runtimeArgs\": [\"run\", \"src/dashboard/app.py\"],\n+      \"port\": 8501\n+    }\n+  ]\n+}\n",
      "raw": "{\n  \"version\": \"0.0.1\",\n  \"configurations\": [\n    {\n      \"name\": \"dashboard\",\n      \"runtimeExecutable\": \"streamlit\",\n      \"runtimeArgs\": [\"run\", \"src/dashboard/app.py\"],\n      \"port\": 8501\n    }\n  ]\n}\n",
      "base": ""
    },
    ".claude/skills/factory-orchestrate/SKILL.md": {
      "additions": 161,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/.claude/skills/factory-orchestrate/SKILL.md b/.claude/skills/factory-orchestrate/SKILL.md\nnew file mode 100644\nindex 0000000..228e3fb\n--- /dev/null\n+++ b/.claude/skills/factory-orchestrate/SKILL.md\n@@ -0,0 +1,161 @@\n+---\n+name: factory-orchestrate\n+description: Run the dark factory convergence loop. Use when the user says \"run a factory crank\", \"start the factory\", \"orchestrate a crank\", or similar. Orchestrates Codex via browser, manages holdout isolation, runs validation gates, and performs LLM-as-judge evaluation.\n+allowed-tools: Bash, Read, Write, Glob, Grep, Edit\n+---\n+\n+# Dark Factory Orchestration \u2014 Claude Code as Orchestrator\n+\n+You are the factory orchestrator. You run the convergence loop that turns specs into working software through iterative AI coding + validation.\n+\n+## Prerequisites\n+\n+- Chrome is logged into Codex (ChatGPT Plus account)\n+- Repository: `joeyfezster/building_ai_w_ai`\n+- Branch to base work on: confirm with user or default to `factory/v1`\n+- Satisfaction threshold: confirm with user or default to 80%\n+- Max iterations: confirm with user or default to 5\n+\n+## The Loop\n+\n+For each iteration:\n+\n+### Step 1: Create Factory Branch\n+```bash\n+# First crank \u2014 create from base branch\n+git checkout -b df-crank-v01-{descriptor} {base_branch}\n+git push -u origin df-crank-v01-{descriptor}\n+```\n+\n+Branch naming: `df-crank-vXX-{descriptor}` where XX is the crank version.\n+\n+### Step 2: Strip Holdout\n+```bash\n+python scripts/strip_holdout.py\n+git push\n+```\n+\n+This deterministically removes `/scenarios/` and comments out scenario Makefile targets. Codex literally cannot see evaluation criteria.\n+\n+Verify: `ls scenarios/` should fail (directory gone).\n+\n+### Step 3: Invoke Codex via Browser\n+\n+Open the Codex UI in Chrome. Provide:\n+- **Repository**: `joeyfezster/building_ai_w_ai`\n+- **Base branch**: `df-crank-vXX-{descriptor}` (the stripped branch)\n+- **Prompt**: Contents of `.github/codex/prompts/factory_fix.md` + the latest feedback file (`artifacts/factory/feedback_iter_N.md`)\n+- **Versions**: 1\n+\n+Codex will create its own branch (named `codex-...`). Wait for it to finish.\n+\n+### Step 4: Gate 0 \u2014 Adversarial Code Review\n+\n+Before merging Codex's changes, review them adversarially:\n+\n+1. Fetch Codex's branch: `git fetch origin`\n+2. Read the diff: `git diff df-crank-vXX...origin/codex-{branch}`\n+3. Review using the checklist in `.github/codex/prompts/adversarial_review.md`:\n+   - Stam tests (mocking subject, stub assertions, tautologies)\n+   - Gaming (hardcoded lookups, overfitted implementations, test-detection)\n+   - Architectural dishonesty (import redirection, dependency skipping)\n+   - Spec violations (check against `/specs/`)\n+   - Integration gaps\n+\n+**If CRITICAL findings**: Do NOT merge. Compile findings as feedback and loop back to Step 3 with specific remediation instructions.\n+\n+**If clean or WARNING-only**: Proceed to Step 5.\n+\n+### Step 5: Merge Codex Changes\n+```bash\n+git merge origin/codex-{branch} --no-ff -m \"factory: merge codex iteration N\"\n+```\n+\n+### Step 6: Restore Holdout\n+```bash\n+python scripts/restore_holdout.py\n+git add scenarios/ Makefile\n+git commit -m \"factory: restore holdout scenarios for evaluation\"\n+```\n+\n+### Step 7: Gate 1 \u2014 Deterministic Validation\n+```bash\n+make lint && make typecheck && make test\n+```\n+\n+\"test\" = the FULL pytest suite, including any tests Codex wrote (already reviewed in Gate 0).\n+\n+**If fail**: Compile feedback (`python scripts/compile_feedback.py --iteration N`), loop to Step 3.\n+\n+### Step 8: Gate 2 \u2014 Non-Functional Requirements\n+```bash\n+make nfr-check\n+```\n+\n+This runs all implemented NFR checks (code quality, complexity, dead code, security).\n+\n+Gate 2 is **non-blocking** but findings are tracked and feed into:\n+- The feedback for the next Codex iteration\n+- Your LLM-as-judge evaluation in Step 10\n+\n+### Step 9: Gate 3 \u2014 Behavioral Scenarios\n+```bash\n+python scripts/run_scenarios.py --timeout 180\n+```\n+\n+Produces `artifacts/factory/scenario_results.json` with satisfaction score.\n+\n+### Step 10: LLM-as-Judge \u2014 Holistic Evaluation\n+\n+You ARE the judge. Don't just check `satisfaction_score >= threshold`. Reason through:\n+\n+1. **Satisfaction trajectory**: Is the score improving across iterations? Plateaued? Regressing?\n+2. **Failure patterns**: Are the same scenarios failing repeatedly? Different ones each time?\n+3. **Fix quality**: Do Codex's changes look like real solutions or gaming attempts? (Gate 0 caught the obvious ones, but look for subtle patterns across iterations)\n+4. **Gate 2 NFR findings**: Even though non-blocking, are there concerning patterns? Growing complexity? Dropping coverage?\n+5. **Systemic issues**: Is there something the score doesn't capture? An architectural problem that will cause future failures?\n+\n+**If satisfied**: Proceed to Step 11.\n+**If not satisfied**: Compile feedback with your holistic assessment, loop to Step 3.\n+\n+### Step 11: Create PR (Accept/Merge Gate)\n+```bash\n+gh pr create \\\n+  --title \"[Factory] df-crank-vXX converged at {score}%\" \\\n+  --body \"$(cat <<'EOF'\n+## Dark Factory \u2014 Converged\n+\n+**Satisfaction score: {score}%**\n+**Iterations: {N}**\n+**Gate 2 NFR status: {summary}**\n+\n+### Accept/Merge Gate\n+This PR was produced by the dark factory convergence loop, orchestrated by Claude Code.\n+\n+**Before merging, verify:**\n+- [ ] Satisfaction score meets your quality bar\n+- [ ] Review latest feedback for residual warnings\n+- [ ] Gate 2 NFR findings are acceptable\n+- [ ] No unexpected files or dependencies introduced\n+\n+**To merge:** Approve and merge. The factory branch can then be deleted.\n+**To reject:** Close this PR and either adjust scenarios/specs or trigger another crank.\n+EOF\n+)\" \\\n+  --label factory-converged --label accept-merge-gate\n+```\n+\n+### Stall Protocol\n+\n+If after 3+ iterations:\n+- Same scenario fails with same error \u2192 the spec or scenario may need adjustment. Escalate to the project lead.\n+- Score oscillates without converging \u2192 architectural issue. Escalate.\n+- Gate 0 keeps finding critical issues \u2192 attractor needs stronger constraints. Update `factory_fix.md`.\n+\n+## Reference Files\n+\n+- **Attractor prompt**: `.github/codex/prompts/factory_fix.md`\n+- **Adversarial review**: `.github/codex/prompts/adversarial_review.md`\n+- **Code quality standards**: `docs/code_quality_standards.md`\n+- **Specs**: `specs/*.md`\n+- **Factory docs**: `docs/dark_factory.md`\n",
      "raw": "---\nname: factory-orchestrate\ndescription: Run the dark factory convergence loop. Use when the user says \"run a factory crank\", \"start the factory\", \"orchestrate a crank\", or similar. Orchestrates Codex via browser, manages holdout isolation, runs validation gates, and performs LLM-as-judge evaluation.\nallowed-tools: Bash, Read, Write, Glob, Grep, Edit\n---\n\n# Dark Factory Orchestration \u2014 Claude Code as Orchestrator\n\nYou are the factory orchestrator. You run the convergence loop that turns specs into working software through iterative AI coding + validation.\n\n## Prerequisites\n\n- Chrome is logged into Codex (ChatGPT Plus account)\n- Repository: `joeyfezster/building_ai_w_ai`\n- Branch to base work on: confirm with user or default to `factory/v1`\n- Satisfaction threshold: confirm with user or default to 80%\n- Max iterations: confirm with user or default to 5\n\n## The Loop\n\nFor each iteration:\n\n### Step 1: Create Factory Branch\n```bash\n# First crank \u2014 create from base branch\ngit checkout -b df-crank-v01-{descriptor} {base_branch}\ngit push -u origin df-crank-v01-{descriptor}\n```\n\nBranch naming: `df-crank-vXX-{descriptor}` where XX is the crank version.\n\n### Step 2: Strip Holdout\n```bash\npython scripts/strip_holdout.py\ngit push\n```\n\nThis deterministically removes `/scenarios/` and comments out scenario Makefile targets. Codex literally cannot see evaluation criteria.\n\nVerify: `ls scenarios/` should fail (directory gone).\n\n### Step 3: Invoke Codex via Browser\n\nOpen the Codex UI in Chrome. Provide:\n- **Repository**: `joeyfezster/building_ai_w_ai`\n- **Base branch**: `df-crank-vXX-{descriptor}` (the stripped branch)\n- **Prompt**: Contents of `.github/codex/prompts/factory_fix.md` + the latest feedback file (`artifacts/factory/feedback_iter_N.md`)\n- **Versions**: 1\n\nCodex will create its own branch (named `codex-...`). Wait for it to finish.\n\n### Step 4: Gate 0 \u2014 Adversarial Code Review\n\nBefore merging Codex's changes, review them adversarially:\n\n1. Fetch Codex's branch: `git fetch origin`\n2. Read the diff: `git diff df-crank-vXX...origin/codex-{branch}`\n3. Review using the checklist in `.github/codex/prompts/adversarial_review.md`:\n   - Stam tests (mocking subject, stub assertions, tautologies)\n   - Gaming (hardcoded lookups, overfitted implementations, test-detection)\n   - Architectural dishonesty (import redirection, dependency skipping)\n   - Spec violations (check against `/specs/`)\n   - Integration gaps\n\n**If CRITICAL findings**: Do NOT merge. Compile findings as feedback and loop back to Step 3 with specific remediation instructions.\n\n**If clean or WARNING-only**: Proceed to Step 5.\n\n### Step 5: Merge Codex Changes\n```bash\ngit merge origin/codex-{branch} --no-ff -m \"factory: merge codex iteration N\"\n```\n\n### Step 6: Restore Holdout\n```bash\npython scripts/restore_holdout.py\ngit add scenarios/ Makefile\ngit commit -m \"factory: restore holdout scenarios for evaluation\"\n```\n\n### Step 7: Gate 1 \u2014 Deterministic Validation\n```bash\nmake lint && make typecheck && make test\n```\n\n\"test\" = the FULL pytest suite, including any tests Codex wrote (already reviewed in Gate 0).\n\n**If fail**: Compile feedback (`python scripts/compile_feedback.py --iteration N`), loop to Step 3.\n\n### Step 8: Gate 2 \u2014 Non-Functional Requirements\n```bash\nmake nfr-check\n```\n\nThis runs all implemented NFR checks (code quality, complexity, dead code, security).\n\nGate 2 is **non-blocking** but findings are tracked and feed into:\n- The feedback for the next Codex iteration\n- Your LLM-as-judge evaluation in Step 10\n\n### Step 9: Gate 3 \u2014 Behavioral Scenarios\n```bash\npython scripts/run_scenarios.py --timeout 180\n```\n\nProduces `artifacts/factory/scenario_results.json` with satisfaction score.\n\n### Step 10: LLM-as-Judge \u2014 Holistic Evaluation\n\nYou ARE the judge. Don't just check `satisfaction_score >= threshold`. Reason through:\n\n1. **Satisfaction trajectory**: Is the score improving across iterations? Plateaued? Regressing?\n2. **Failure patterns**: Are the same scenarios failing repeatedly? Different ones each time?\n3. **Fix quality**: Do Codex's changes look like real solutions or gaming attempts? (Gate 0 caught the obvious ones, but look for subtle patterns across iterations)\n4. **Gate 2 NFR findings**: Even though non-blocking, are there concerning patterns? Growing complexity? Dropping coverage?\n5. **Systemic issues**: Is there something the score doesn't capture? An architectural problem that will cause future failures?\n\n**If satisfied**: Proceed to Step 11.\n**If not satisfied**: Compile feedback with your holistic assessment, loop to Step 3.\n\n### Step 11: Create PR (Accept/Merge Gate)\n```bash\ngh pr create \\\n  --title \"[Factory] df-crank-vXX converged at {score}%\" \\\n  --body \"$(cat <<'EOF'\n## Dark Factory \u2014 Converged\n\n**Satisfaction score: {score}%**\n**Iterations: {N}**\n**Gate 2 NFR status: {summary}**\n\n### Accept/Merge Gate\nThis PR was produced by the dark factory convergence loop, orchestrated by Claude Code.\n\n**Before merging, verify:**\n- [ ] Satisfaction score meets your quality bar\n- [ ] Review latest feedback for residual warnings\n- [ ] Gate 2 NFR findings are acceptable\n- [ ] No unexpected files or dependencies introduced\n\n**To merge:** Approve and merge. The factory branch can then be deleted.\n**To reject:** Close this PR and either adjust scenarios/specs or trigger another crank.\nEOF\n)\" \\\n  --label factory-converged --label accept-merge-gate\n```\n\n### Stall Protocol\n\nIf after 3+ iterations:\n- Same scenario fails with same error \u2192 the spec or scenario may need adjustment. Escalate to the project lead.\n- Score oscillates without converging \u2192 architectural issue. Escalate.\n- Gate 0 keeps finding critical issues \u2192 attractor needs stronger constraints. Update `factory_fix.md`.\n\n## Reference Files\n\n- **Attractor prompt**: `.github/codex/prompts/factory_fix.md`\n- **Adversarial review**: `.github/codex/prompts/adversarial_review.md`\n- **Code quality standards**: `docs/code_quality_standards.md`\n- **Specs**: `specs/*.md`\n- **Factory docs**: `docs/dark_factory.md`\n",
      "base": ""
    },
    ".devcontainer/Dockerfile": {
      "additions": 1,
      "deletions": 2,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/.devcontainer/Dockerfile b/.devcontainer/Dockerfile\nindex 753e366..4ac1772 100644\n--- a/.devcontainer/Dockerfile\n+++ b/.devcontainer/Dockerfile\n@@ -1,4 +1,3 @@\n FROM python:3.12-slim\n-\n+RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*\n WORKDIR /workspace\n-RUN python -m pip install --upgrade pip\n",
      "raw": "FROM python:3.12-slim\nRUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*\nWORKDIR /workspace\n",
      "base": "FROM python:3.12-slim\n\nWORKDIR /workspace\nRUN python -m pip install --upgrade pip\n"
    },
    ".devcontainer/devcontainer.json": {
      "additions": 3,
      "deletions": 7,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/.devcontainer/devcontainer.json b/.devcontainer/devcontainer.json\nindex b00c559..40d2a2e 100644\n--- a/.devcontainer/devcontainer.json\n+++ b/.devcontainer/devcontainer.json\n@@ -1,13 +1,9 @@\n {\n-  \"name\": \"retro-rl-milestones\",\n-  \"build\": {\n-    \"dockerfile\": \"Dockerfile\"\n-  },\n+  \"name\": \"minipong-rl\",\n+  \"build\": {\"dockerfile\": \"Dockerfile\"},\n   \"customizations\": {\n     \"vscode\": {\n-      \"settings\": {\n-        \"python.defaultInterpreterPath\": \"/usr/local/bin/python\"\n-      }\n+      \"extensions\": [\"ms-python.python\"]\n     }\n   }\n }\n",
      "raw": "{\n  \"name\": \"minipong-rl\",\n  \"build\": {\"dockerfile\": \"Dockerfile\"},\n  \"customizations\": {\n    \"vscode\": {\n      \"extensions\": [\"ms-python.python\"]\n    }\n  }\n}\n",
      "base": "{\n  \"name\": \"retro-rl-milestones\",\n  \"build\": {\n    \"dockerfile\": \"Dockerfile\"\n  },\n  \"customizations\": {\n    \"vscode\": {\n      \"settings\": {\n        \"python.defaultInterpreterPath\": \"/usr/local/bin/python\"\n      }\n    }\n  }\n}\n"
    },
    ".gitattributes": {
      "additions": 1,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/.gitattributes b/.gitattributes\nnew file mode 100644\nindex 0000000..176a458\n--- /dev/null\n+++ b/.gitattributes\n@@ -0,0 +1 @@\n+* text=auto\n",
      "raw": "* text=auto\n",
      "base": ""
    },
    ".githooks/pre-commit": {
      "additions": 43,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/.githooks/pre-commit b/.githooks/pre-commit\nnew file mode 100755\nindex 0000000..c192202\n--- /dev/null\n+++ b/.githooks/pre-commit\n@@ -0,0 +1,43 @@\n+#!/usr/bin/env bash\n+# Pre-commit hook: runs ruff and mypy on staged Python files.\n+# Install: make install-hooks (or: git config core.hooksPath .githooks)\n+#\n+# This replaces the pre-commit tool \u2014 same guarantees, no virtualenv dependency.\n+# Uses whatever Python environment is active (conda py312 expected).\n+\n+set -e\n+\n+# Collect staged Python files\n+STAGED_PY=$(git diff --cached --name-only --diff-filter=ACM | grep '\\.py$' || true)\n+\n+if [ -z \"$STAGED_PY\" ]; then\n+    exit 0\n+fi\n+\n+echo \"\ud83d\udd0d Pre-commit: checking $(echo \"$STAGED_PY\" | wc -l | tr -d ' ') Python file(s)...\"\n+\n+# \u2500\u2500 Lint \u2500\u2500\n+echo \"  ruff check...\"\n+if ! echo \"$STAGED_PY\" | xargs ruff check --no-fix; then\n+    echo \"\u274c ruff check failed. Fix issues and re-stage.\"\n+    exit 1\n+fi\n+\n+# \u2500\u2500 Format check (no auto-fix, just verify) \u2500\u2500\n+echo \"  ruff format --check...\"\n+if ! echo \"$STAGED_PY\" | xargs ruff format --check 2>/dev/null; then\n+    echo \"\u274c ruff format check failed. Run 'ruff format' and re-stage.\"\n+    exit 1\n+fi\n+\n+# \u2500\u2500 Type check (factory scripts only \u2014 product code needs full deps) \u2500\u2500\n+FACTORY_SCRIPTS=$(echo \"$STAGED_PY\" | grep '^scripts/' || true)\n+if [ -n \"$FACTORY_SCRIPTS\" ]; then\n+    echo \"  mypy (factory scripts)...\"\n+    if ! echo \"$FACTORY_SCRIPTS\" | xargs mypy --ignore-missing-imports 2>/dev/null; then\n+        echo \"\u274c mypy failed on factory scripts. Fix type errors and re-stage.\"\n+        exit 1\n+    fi\n+fi\n+\n+echo \"\u2705 Pre-commit checks passed.\"\n",
      "raw": "#!/usr/bin/env bash\n# Pre-commit hook: runs ruff and mypy on staged Python files.\n# Install: make install-hooks (or: git config core.hooksPath .githooks)\n#\n# This replaces the pre-commit tool \u2014 same guarantees, no virtualenv dependency.\n# Uses whatever Python environment is active (conda py312 expected).\n\nset -e\n\n# Collect staged Python files\nSTAGED_PY=$(git diff --cached --name-only --diff-filter=ACM | grep '\\.py$' || true)\n\nif [ -z \"$STAGED_PY\" ]; then\n    exit 0\nfi\n\necho \"\ud83d\udd0d Pre-commit: checking $(echo \"$STAGED_PY\" | wc -l | tr -d ' ') Python file(s)...\"\n\n# \u2500\u2500 Lint \u2500\u2500\necho \"  ruff check...\"\nif ! echo \"$STAGED_PY\" | xargs ruff check --no-fix; then\n    echo \"\u274c ruff check failed. Fix issues and re-stage.\"\n    exit 1\nfi\n\n# \u2500\u2500 Format check (no auto-fix, just verify) \u2500\u2500\necho \"  ruff format --check...\"\nif ! echo \"$STAGED_PY\" | xargs ruff format --check 2>/dev/null; then\n    echo \"\u274c ruff format check failed. Run 'ruff format' and re-stage.\"\n    exit 1\nfi\n\n# \u2500\u2500 Type check (factory scripts only \u2014 product code needs full deps) \u2500\u2500\nFACTORY_SCRIPTS=$(echo \"$STAGED_PY\" | grep '^scripts/' || true)\nif [ -n \"$FACTORY_SCRIPTS\" ]; then\n    echo \"  mypy (factory scripts)...\"\n    if ! echo \"$FACTORY_SCRIPTS\" | xargs mypy --ignore-missing-imports 2>/dev/null; then\n        echo \"\u274c mypy failed on factory scripts. Fix type errors and re-stage.\"\n        exit 1\n    fi\nfi\n\necho \"\u2705 Pre-commit checks passed.\"\n",
      "base": ""
    },
    ".github/codex/prompts/adversarial_review.md": {
      "additions": 86,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/.github/codex/prompts/adversarial_review.md b/.github/codex/prompts/adversarial_review.md\nnew file mode 100644\nindex 0000000..73c14ce\n--- /dev/null\n+++ b/.github/codex/prompts/adversarial_review.md\n@@ -0,0 +1,86 @@\n+# Adversarial Code Review \u2014 Reviewer Instructions\n+\n+You are the adversarial reviewer in a dark factory convergence loop. Your job is to catch problems that automated validation cannot \u2014 especially attempts (intentional or emergent) to game the system.\n+\n+## Your Mindset\n+\n+Assume the code was written by an agent optimizing for a satisfaction score. That agent has incentives to take shortcuts. Your job is to find those shortcuts before they ship.\n+\n+You are NOT reviewing for style or preference. You are reviewing for **correctness, honesty, and generality**.\n+\n+## What You're Looking For\n+\n+### 1. Stam Tests (Empty/Fake Tests)\n+\n+Tests that pass by construction and prove nothing:\n+\n+- **Mocking the subject.** A test that patches the function it claims to test. Example: `@patch('src.envs.minipong.MiniPongEnv.step')` in a test of `MiniPongEnv.step`.\n+- **Stub assertions.** Tests that assert `True`, assert against hardcoded expected values without running real logic, or have no assertions at all.\n+- **Tautological tests.** Tests where the expected value is computed by the same code being tested. Example: `assert compute(x) == compute(x)`.\n+- **Excessive mocking.** Tests where more than 50% of the setup is mocks/patches \u2014 at that point you're testing the mocking framework, not the code.\n+- **Tests that catch nothing.** Would the test still pass if you deleted the implementation and replaced it with `pass`? If yes, it's stam.\n+\n+### 2. Gaming the System\n+\n+Implementations that optimize for passing tests rather than solving the problem:\n+\n+- **Hardcoded lookup tables** that match known test inputs. Example: a function that returns correct results for seeds 0-10 but fails for seed 11.\n+- **Overfitted implementations.** Code that checks `if input == specific_value: return specific_output` instead of implementing general logic.\n+- **Output caching without computation.** Functions that read from a pre-computed file instead of actually computing the result.\n+- **Test-detection.** Code that behaves differently when it detects it's running in a test environment (checking for pytest, CI env vars, etc.).\n+- **Assertion-matching.** Fixing a specific assertion failure by hardcoding the expected value rather than fixing the underlying logic.\n+\n+### 3. Architectural Dishonesty\n+\n+Structural shortcuts that undermine the system design:\n+\n+- **Import redirection.** Defining a local class/function with the same name as the one being tested to avoid importing the real one.\n+- **Dependency skipping.** Catching ImportError and silently degrading to a no-op implementation.\n+- **Config shortcuts.** Test-only configuration that makes training trivially fast (1 step, 1 episode) without testing anything meaningful.\n+- **Docker hollow builds.** Dockerfiles that skip dependencies to pass `docker build` but would fail at runtime.\n+- **Dead code.** Functions that exist to satisfy import checks but are never called in any real code path.\n+\n+### 4. Specification Violations\n+\n+Behavior that contradicts the specs even if tests pass:\n+\n+- **Observation space mismatch.** Specs say 84\u00d784 uint8, but implementation uses a different shape/dtype.\n+- **Action space mismatch.** Specs say 3 discrete actions, implementation has more or fewer.\n+- **Non-determinism.** Specs require deterministic replay with fixed seed, but implementation has unseeded random calls.\n+- **Missing artifacts.** Training should produce checkpoints, metrics, videos \u2014 check they're actually written, not just that the function exists.\n+\n+### 5. Integration Gaps\n+\n+Components that work in isolation but fail together:\n+\n+- **Interface mismatches.** Module A calls module B with arguments B doesn't expect.\n+- **Path assumptions.** Code that assumes it runs from repo root vs. from a subdirectory.\n+- **Environment variable dependencies.** Code that silently fails without required env vars instead of erroring clearly.\n+- **Version mismatches.** Requirements specifying one version but code using APIs from a different version.\n+\n+## Review Output Format\n+\n+For each finding, report:\n+\n+```\n+FINDING: [one-line summary]\n+SEVERITY: CRITICAL | WARNING | NIT\n+FILE: [path]\n+LINE: [line number or range]\n+EVIDENCE: [what you found]\n+IMPACT: [why this matters]\n+FIX: [what the attractor should do differently]\n+```\n+\n+Severity guide:\n+- **CRITICAL**: The code is wrong, dishonest, or will fail in production. Blocks merge.\n+- **WARNING**: The code is fragile, undertested, or architecturally questionable. Should be fixed before merge.\n+- **NIT**: Style, naming, or minor improvements. Can be deferred.\n+\n+## Your Constraints\n+\n+- You are reviewing **product code** (src/, tests/, configs/) \u2014 not factory infrastructure.\n+- You can read specs to understand expected behavior.\n+- You do NOT have access to scenarios (holdout set).\n+- Focus on findings, not praise. If something is correct, move on.\n+- Be specific. \"This test is weak\" is not useful. \"This test mocks the DQN forward pass, so it doesn't test whether the network actually produces valid Q-values\" is useful.\n",
      "raw": "# Adversarial Code Review \u2014 Reviewer Instructions\n\nYou are the adversarial reviewer in a dark factory convergence loop. Your job is to catch problems that automated validation cannot \u2014 especially attempts (intentional or emergent) to game the system.\n\n## Your Mindset\n\nAssume the code was written by an agent optimizing for a satisfaction score. That agent has incentives to take shortcuts. Your job is to find those shortcuts before they ship.\n\nYou are NOT reviewing for style or preference. You are reviewing for **correctness, honesty, and generality**.\n\n## What You're Looking For\n\n### 1. Stam Tests (Empty/Fake Tests)\n\nTests that pass by construction and prove nothing:\n\n- **Mocking the subject.** A test that patches the function it claims to test. Example: `@patch('src.envs.minipong.MiniPongEnv.step')` in a test of `MiniPongEnv.step`.\n- **Stub assertions.** Tests that assert `True`, assert against hardcoded expected values without running real logic, or have no assertions at all.\n- **Tautological tests.** Tests where the expected value is computed by the same code being tested. Example: `assert compute(x) == compute(x)`.\n- **Excessive mocking.** Tests where more than 50% of the setup is mocks/patches \u2014 at that point you're testing the mocking framework, not the code.\n- **Tests that catch nothing.** Would the test still pass if you deleted the implementation and replaced it with `pass`? If yes, it's stam.\n\n### 2. Gaming the System\n\nImplementations that optimize for passing tests rather than solving the problem:\n\n- **Hardcoded lookup tables** that match known test inputs. Example: a function that returns correct results for seeds 0-10 but fails for seed 11.\n- **Overfitted implementations.** Code that checks `if input == specific_value: return specific_output` instead of implementing general logic.\n- **Output caching without computation.** Functions that read from a pre-computed file instead of actually computing the result.\n- **Test-detection.** Code that behaves differently when it detects it's running in a test environment (checking for pytest, CI env vars, etc.).\n- **Assertion-matching.** Fixing a specific assertion failure by hardcoding the expected value rather than fixing the underlying logic.\n\n### 3. Architectural Dishonesty\n\nStructural shortcuts that undermine the system design:\n\n- **Import redirection.** Defining a local class/function with the same name as the one being tested to avoid importing the real one.\n- **Dependency skipping.** Catching ImportError and silently degrading to a no-op implementation.\n- **Config shortcuts.** Test-only configuration that makes training trivially fast (1 step, 1 episode) without testing anything meaningful.\n- **Docker hollow builds.** Dockerfiles that skip dependencies to pass `docker build` but would fail at runtime.\n- **Dead code.** Functions that exist to satisfy import checks but are never called in any real code path.\n\n### 4. Specification Violations\n\nBehavior that contradicts the specs even if tests pass:\n\n- **Observation space mismatch.** Specs say 84\u00d784 uint8, but implementation uses a different shape/dtype.\n- **Action space mismatch.** Specs say 3 discrete actions, implementation has more or fewer.\n- **Non-determinism.** Specs require deterministic replay with fixed seed, but implementation has unseeded random calls.\n- **Missing artifacts.** Training should produce checkpoints, metrics, videos \u2014 check they're actually written, not just that the function exists.\n\n### 5. Integration Gaps\n\nComponents that work in isolation but fail together:\n\n- **Interface mismatches.** Module A calls module B with arguments B doesn't expect.\n- **Path assumptions.** Code that assumes it runs from repo root vs. from a subdirectory.\n- **Environment variable dependencies.** Code that silently fails without required env vars instead of erroring clearly.\n- **Version mismatches.** Requirements specifying one version but code using APIs from a different version.\n\n## Review Output Format\n\nFor each finding, report:\n\n```\nFINDING: [one-line summary]\nSEVERITY: CRITICAL | WARNING | NIT\nFILE: [path]\nLINE: [line number or range]\nEVIDENCE: [what you found]\nIMPACT: [why this matters]\nFIX: [what the attractor should do differently]\n```\n\nSeverity guide:\n- **CRITICAL**: The code is wrong, dishonest, or will fail in production. Blocks merge.\n- **WARNING**: The code is fragile, undertested, or architecturally questionable. Should be fixed before merge.\n- **NIT**: Style, naming, or minor improvements. Can be deferred.\n\n## Your Constraints\n\n- You are reviewing **product code** (src/, tests/, configs/) \u2014 not factory infrastructure.\n- You can read specs to understand expected behavior.\n- You do NOT have access to scenarios (holdout set).\n- Focus on findings, not praise. If something is correct, move on.\n- Be specific. \"This test is weak\" is not useful. \"This test mocks the DQN forward pass, so it doesn't test whether the network actually produces valid Q-values\" is useful.\n",
      "base": ""
    },
    ".github/codex/prompts/factory_fix.md": {
      "additions": 51,
      "deletions": 3,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/.github/codex/prompts/factory_fix.md b/.github/codex/prompts/factory_fix.md\nindex 6483734..0ddb5d5 100644\n--- a/.github/codex/prompts/factory_fix.md\n+++ b/.github/codex/prompts/factory_fix.md\n@@ -25,6 +25,11 @@ Read the feedback file for this iteration to understand what's broken:\n - `/.github/codex/prompts/factory_fix.md` (this file)\n - `/CLAUDE.md`\n - `/specs/` (read-only \u2014 these are your requirements)\n+- `/agents/` (pre-factory reference, not product code)\n+- `/scripts/strip_holdout.py` (holdout isolation gate)\n+- `/scripts/restore_holdout.py` (holdout restoration)\n+- `/scripts/nfr_checks.py` (Gate 2 NFR checker)\n+- `/scripts/check_test_quality.py` (anti-stam scanner)\n \n **DO modify** source code in:\n - `src/` \u2014 all Python source\n@@ -35,6 +40,49 @@ Read the feedback file for this iteration to understand what's broken:\n - `infra/docker/` \u2014 Dockerfiles\n - `pyproject.toml` \u2014 project configuration\n \n+## Validation Guidelines\n+\n+Before considering any change complete, ensure:\n+\n+### Hard Constraints\n+- No proprietary ROM dependencies \u2014 MiniPong is self-contained\n+- Policy consumes pixels only (84\u00d784 uint8 observations)\n+- `make validate` must pass (lint + typecheck + test + docker + env-smoke)\n+- `make verify-learning` must pass for any training-related change\n+\n+### Definition of Done\n+- Functional requirements from `/specs/` are implemented\n+- Architectural consistency maintained (no ad-hoc patterns)\n+- Integration checks pass end-to-end\n+- Required artifacts generated and linked (checkpoints, metrics, videos)\n+\n+### Quality Checklist\n+- [ ] `make lint` passes (ruff check)\n+- [ ] `make typecheck` passes (mypy src)\n+- [ ] `make test` passes (pytest)\n+- [ ] No new dead imports or unused code introduced\n+- [ ] Changes are minimal and surgical \u2014 fix what's broken, don't refactor\n+\n+## Anti-Gaming Rules\n+\n+You are evaluated by an external holdout system you cannot see. These rules exist because the factory has adversarial review \u2014 attempts to game the system will be caught and will waste iterations.\n+\n+### Tests Must Be Real\n+- **No stam tests.** Every test must exercise real behavior through real code paths. A test that passes by construction proves nothing.\n+- **No mocking the system under test.** Mocks are for isolating external dependencies (network, filesystem, third-party APIs) \u2014 never for bypassing the logic you're supposed to be testing.\n+- **No stub implementations.** Functions must contain real logic, not `return True`, `return 0`, `pass`, or hardcoded lookup tables that happen to match test cases.\n+- **No patching away the thing being tested.** If a test patches the function it claims to test, it tests nothing.\n+\n+### Implementations Must Be General\n+- **No hardcoded special cases** that coincidentally pass known test inputs. Example: `is_prime(x): return x in {2, 3, 5, 7, 11, 13}` is not a prime checker.\n+- **No output-matching shortcuts.** If a function is supposed to compute something, it must actually compute it \u2014 not return a cached/hardcoded result.\n+- **No overfitting to error messages.** If a scenario fails with a specific assertion, fix the root cause \u2014 don't just make that specific assertion pass while breaking the general case.\n+\n+### Integration Must Be Honest\n+- If a test file requires imports from `src/`, those imports must exercise the real module, not a local redefinition.\n+- Configuration files must reflect actual runtime parameters, not test-only shortcuts.\n+- Docker builds must include all real dependencies \u2014 don't skip packages to speed up builds if the code needs them at runtime.\n+\n ## Your Approach\n \n 1. Read the latest feedback file to understand all failures\n@@ -43,9 +91,9 @@ Read the feedback file for this iteration to understand what's broken:\n    - Import errors and missing modules first\n    - File/artifact production issues next\n    - Behavioral correctness last\n-4. Keep changes minimal and surgical \u2014 fix what's broken, don't refactor\n-5. Run `make lint` and `make typecheck` before finishing to ensure you haven't introduced new issues\n-6. Do NOT add new test files that duplicate scenario evaluation logic\n+4. Validate locally: run `make lint && make typecheck` before finishing\n+5. Do NOT add new test files that duplicate scenario evaluation logic\n+6. Do NOT refactor code that isn't related to the current failures\n \n ## Success Criteria\n \n",
      "raw": "# Factory Fix \u2014 Codex Prompt Template\n\nYou are the coding agent (Attractor) in a dark factory convergence loop. Your job is to fix failures identified by the factory's validation system.\n\n## Your Context\n\nRead the component specifications in `/specs/` to understand what the system should do:\n- `specs/system.md` \u2014 overall system architecture\n- `specs/env.md` \u2014 MiniPong environment requirements\n- `specs/rl.md` \u2014 DQN algorithm requirements\n- `specs/training.md` \u2014 training pipeline requirements\n- `specs/dashboard.md` \u2014 dashboard requirements\n- `specs/proof.md` \u2014 learning proof and video proof requirements\n\nRead the feedback file for this iteration to understand what's broken:\n- `artifacts/factory/feedback_iter_*.md` \u2014 latest feedback with full error output\n\n## Your Constraints\n\n**NEVER read, modify, or delete these files:**\n- Anything in `/scenarios/` (you should not even see this directory)\n- `/scripts/run_scenarios.py`\n- `/scripts/compile_feedback.py`\n- `/.github/workflows/factory.yaml`\n- `/.github/codex/prompts/factory_fix.md` (this file)\n- `/CLAUDE.md`\n- `/specs/` (read-only \u2014 these are your requirements)\n- `/agents/` (pre-factory reference, not product code)\n- `/scripts/strip_holdout.py` (holdout isolation gate)\n- `/scripts/restore_holdout.py` (holdout restoration)\n- `/scripts/nfr_checks.py` (Gate 2 NFR checker)\n- `/scripts/check_test_quality.py` (anti-stam scanner)\n\n**DO modify** source code in:\n- `src/` \u2014 all Python source\n- `tests/` \u2014 test files\n- `configs/` \u2014 configuration files\n- `Makefile` \u2014 build targets\n- `requirements.in` / `requirements-dev.in` \u2014 dependencies\n- `infra/docker/` \u2014 Dockerfiles\n- `pyproject.toml` \u2014 project configuration\n\n## Validation Guidelines\n\nBefore considering any change complete, ensure:\n\n### Hard Constraints\n- No proprietary ROM dependencies \u2014 MiniPong is self-contained\n- Policy consumes pixels only (84\u00d784 uint8 observations)\n- `make validate` must pass (lint + typecheck + test + docker + env-smoke)\n- `make verify-learning` must pass for any training-related change\n\n### Definition of Done\n- Functional requirements from `/specs/` are implemented\n- Architectural consistency maintained (no ad-hoc patterns)\n- Integration checks pass end-to-end\n- Required artifacts generated and linked (checkpoints, metrics, videos)\n\n### Quality Checklist\n- [ ] `make lint` passes (ruff check)\n- [ ] `make typecheck` passes (mypy src)\n- [ ] `make test` passes (pytest)\n- [ ] No new dead imports or unused code introduced\n- [ ] Changes are minimal and surgical \u2014 fix what's broken, don't refactor\n\n## Anti-Gaming Rules\n\nYou are evaluated by an external holdout system you cannot see. These rules exist because the factory has adversarial review \u2014 attempts to game the system will be caught and will waste iterations.\n\n### Tests Must Be Real\n- **No stam tests.** Every test must exercise real behavior through real code paths. A test that passes by construction proves nothing.\n- **No mocking the system under test.** Mocks are for isolating external dependencies (network, filesystem, third-party APIs) \u2014 never for bypassing the logic you're supposed to be testing.\n- **No stub implementations.** Functions must contain real logic, not `return True`, `return 0`, `pass`, or hardcoded lookup tables that happen to match test cases.\n- **No patching away the thing being tested.** If a test patches the function it claims to test, it tests nothing.\n\n### Implementations Must Be General\n- **No hardcoded special cases** that coincidentally pass known test inputs. Example: `is_prime(x): return x in {2, 3, 5, 7, 11, 13}` is not a prime checker.\n- **No output-matching shortcuts.** If a function is supposed to compute something, it must actually compute it \u2014 not return a cached/hardcoded result.\n- **No overfitting to error messages.** If a scenario fails with a specific assertion, fix the root cause \u2014 don't just make that specific assertion pass while breaking the general case.\n\n### Integration Must Be Honest\n- If a test file requires imports from `src/`, those imports must exercise the real module, not a local redefinition.\n- Configuration files must reflect actual runtime parameters, not test-only shortcuts.\n- Docker builds must include all real dependencies \u2014 don't skip packages to speed up builds if the code needs them at runtime.\n\n## Your Approach\n\n1. Read the latest feedback file to understand all failures\n2. Read the relevant specs to understand expected behavior\n3. Fix failures in priority order:\n   - Import errors and missing modules first\n   - File/artifact production issues next\n   - Behavioral correctness last\n4. Validate locally: run `make lint && make typecheck` before finishing\n5. Do NOT add new test files that duplicate scenario evaluation logic\n6. Do NOT refactor code that isn't related to the current failures\n\n## Success Criteria\n\nThe factory will re-run validation after your changes. Your goal is to increase the satisfaction score (fraction of scenarios passing). Aim for convergence, not perfection in a single iteration.\n",
      "base": "# Factory Fix \u2014 Codex Prompt Template\n\nYou are the coding agent (Attractor) in a dark factory convergence loop. Your job is to fix failures identified by the factory's validation system.\n\n## Your Context\n\nRead the component specifications in `/specs/` to understand what the system should do:\n- `specs/system.md` \u2014 overall system architecture\n- `specs/env.md` \u2014 MiniPong environment requirements\n- `specs/rl.md` \u2014 DQN algorithm requirements\n- `specs/training.md` \u2014 training pipeline requirements\n- `specs/dashboard.md` \u2014 dashboard requirements\n- `specs/proof.md` \u2014 learning proof and video proof requirements\n\nRead the feedback file for this iteration to understand what's broken:\n- `artifacts/factory/feedback_iter_*.md` \u2014 latest feedback with full error output\n\n## Your Constraints\n\n**NEVER read, modify, or delete these files:**\n- Anything in `/scenarios/` (you should not even see this directory)\n- `/scripts/run_scenarios.py`\n- `/scripts/compile_feedback.py`\n- `/.github/workflows/factory.yaml`\n- `/.github/codex/prompts/factory_fix.md` (this file)\n- `/CLAUDE.md`\n- `/specs/` (read-only \u2014 these are your requirements)\n\n**DO modify** source code in:\n- `src/` \u2014 all Python source\n- `tests/` \u2014 test files\n- `configs/` \u2014 configuration files\n- `Makefile` \u2014 build targets\n- `requirements.in` / `requirements-dev.in` \u2014 dependencies\n- `infra/docker/` \u2014 Dockerfiles\n- `pyproject.toml` \u2014 project configuration\n\n## Your Approach\n\n1. Read the latest feedback file to understand all failures\n2. Read the relevant specs to understand expected behavior\n3. Fix failures in priority order:\n   - Import errors and missing modules first\n   - File/artifact production issues next\n   - Behavioral correctness last\n4. Keep changes minimal and surgical \u2014 fix what's broken, don't refactor\n5. Run `make lint` and `make typecheck` before finishing to ensure you haven't introduced new issues\n6. Do NOT add new test files that duplicate scenario evaluation logic\n\n## Success Criteria\n\nThe factory will re-run validation after your changes. Your goal is to increase the satisfaction score (fraction of scenarios passing). Aim for convergence, not perfection in a single iteration.\n"
    },
    ".github/workflows/ci.yaml": {
      "additions": 118,
      "deletions": 14,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/.github/workflows/ci.yaml b/.github/workflows/ci.yaml\nindex 5d69e2b..05f582f 100644\n--- a/.github/workflows/ci.yaml\n+++ b/.github/workflows/ci.yaml\n@@ -5,22 +5,126 @@ on:\n   pull_request:\n \n jobs:\n-  lint-test:\n+  # \u2500\u2500 Product Code Validation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+  validate:\n     runs-on: ubuntu-latest\n     steps:\n-      - name: Checkout\n-        uses: actions/checkout@v4\n-      - name: Set up Python\n-        uses: actions/setup-python@v5\n+      - uses: actions/checkout@v4\n+      - uses: actions/setup-python@v5\n         with:\n           python-version: \"3.12\"\n-      - name: Install dependencies\n+      - run: python -m pip install --upgrade pip\n+      - run: pip install -r requirements.txt -r requirements-dev.txt\n+      - run: make lint\n+      - run: make typecheck\n+      - run: make test\n+      - run: docker build -f infra/docker/Dockerfile.train --build-arg BASE_IMAGE=python:3.12-slim -t minipong-train .\n+      - run: docker run --rm minipong-train python -m src.train.train_dqn --help\n+\n+  # \u2500\u2500 Factory Infrastructure Validation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+  # Tests the factory's own components \u2014 the validation system\n+  # must itself be validated, or it can't be trusted.\n+  factory-self-test:\n+    runs-on: ubuntu-latest\n+    steps:\n+      - uses: actions/checkout@v4\n+      - uses: actions/setup-python@v5\n+        with:\n+          python-version: \"3.12\"\n+      - run: python -m pip install --upgrade pip\n+      - run: pip install pytest\n+\n+      # Lint factory scripts\n+      - name: Lint factory scripts\n+        run: |\n+          pip install ruff\n+          ruff check scripts/run_scenarios.py scripts/compile_feedback.py\n+\n+      # Type-check factory scripts\n+      - name: Type-check factory scripts\n+        run: |\n+          pip install mypy\n+          mypy scripts/run_scenarios.py scripts/compile_feedback.py --ignore-missing-imports\n+\n+      # Run factory component tests\n+      - name: Test scenario runner\n+        run: pytest tests/test_factory_run_scenarios.py -v\n+\n+      - name: Test feedback compiler\n+        run: pytest tests/test_factory_compile_feedback.py -v\n+\n+      # Verify scenario format consistency\n+      - name: Validate scenario file format\n         run: |\n-          python -m pip install --upgrade pip\n-          pip install -r requirements.txt -r requirements-dev.txt\n-      - name: Ruff\n-        run: ruff check .\n-      - name: Mypy\n-        run: mypy src\n-      - name: Pytest\n-        run: pytest -q\n+          python -c \"\n+          from pathlib import Path\n+          import re\n+          import sys\n+\n+          required_sections = [\n+              'Category', 'Preconditions', 'Behavioral Expectation',\n+              'Evaluation Method', 'Pass Criteria', 'Evidence Required'\n+          ]\n+\n+          errors = []\n+          for f in sorted(Path('scenarios').glob('*.md')):\n+              content = f.read_text()\n+              for section in required_sections:\n+                  if f'## {section}' not in content:\n+                      errors.append(f'{f.name}: missing ## {section}')\n+              # Verify evaluation method has a code block\n+              if '\\`\\`\\`bash' not in content and '\\`\\`\\`sh' not in content:\n+                  errors.append(f'{f.name}: no bash code block in Evaluation Method')\n+\n+          if errors:\n+              print('Scenario format errors:')\n+              for e in errors:\n+                  print(f'  - {e}')\n+              sys.exit(1)\n+          else:\n+              print(f'All {len(list(Path(\\\"scenarios\\\").glob(\\\"*.md\\\")))} scenarios have valid format')\n+          \"\n+\n+      # Verify factory-protected files list is consistent\n+      - name: Verify factory-protected files referenced correctly\n+        run: |\n+          python -c \"\n+          from pathlib import Path\n+          import sys\n+\n+          # These files must exist and be referenced in both\n+          # CLAUDE.md and factory_fix.md as protected\n+          protected_dirs = ['scenarios', 'scripts', 'agents', 'specs']\n+          protected_files = [\n+              '.github/workflows/factory.yaml',\n+              '.github/codex/prompts/factory_fix.md',\n+              'CLAUDE.md',\n+          ]\n+\n+          errors = []\n+          for d in protected_dirs:\n+              if not Path(d).exists():\n+                  errors.append(f'Protected dir {d}/ does not exist')\n+\n+          for f in protected_files:\n+              if not Path(f).exists():\n+                  errors.append(f'Protected file {f} does not exist')\n+\n+          # Check CLAUDE.md references them\n+          claude_md = Path('CLAUDE.md').read_text()\n+          for d in protected_dirs:\n+              if f'/{d}/' not in claude_md:\n+                  errors.append(f'CLAUDE.md does not reference /{d}/')\n+\n+          if errors:\n+              print('Factory integrity errors:')\n+              for e in errors:\n+                  print(f'  - {e}')\n+              sys.exit(1)\n+          else:\n+              print('Factory integrity check passed')\n+          \"\n+\n+      # Anti-gaming test quality check\n+      - name: Check test quality (anti-stam, anti-gaming)\n+        run: python scripts/check_test_quality.py\n",
      "raw": "name: CI\n\non:\n  push:\n  pull_request:\n\njobs:\n  # \u2500\u2500 Product Code Validation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n      - run: python -m pip install --upgrade pip\n      - run: pip install -r requirements.txt -r requirements-dev.txt\n      - run: make lint\n      - run: make typecheck\n      - run: make test\n      - run: docker build -f infra/docker/Dockerfile.train --build-arg BASE_IMAGE=python:3.12-slim -t minipong-train .\n      - run: docker run --rm minipong-train python -m src.train.train_dqn --help\n\n  # \u2500\u2500 Factory Infrastructure Validation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  # Tests the factory's own components \u2014 the validation system\n  # must itself be validated, or it can't be trusted.\n  factory-self-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n      - run: python -m pip install --upgrade pip\n      - run: pip install pytest\n\n      # Lint factory scripts\n      - name: Lint factory scripts\n        run: |\n          pip install ruff\n          ruff check scripts/run_scenarios.py scripts/compile_feedback.py\n\n      # Type-check factory scripts\n      - name: Type-check factory scripts\n        run: |\n          pip install mypy\n          mypy scripts/run_scenarios.py scripts/compile_feedback.py --ignore-missing-imports\n\n      # Run factory component tests\n      - name: Test scenario runner\n        run: pytest tests/test_factory_run_scenarios.py -v\n\n      - name: Test feedback compiler\n        run: pytest tests/test_factory_compile_feedback.py -v\n\n      # Verify scenario format consistency\n      - name: Validate scenario file format\n        run: |\n          python -c \"\n          from pathlib import Path\n          import re\n          import sys\n\n          required_sections = [\n              'Category', 'Preconditions', 'Behavioral Expectation',\n              'Evaluation Method', 'Pass Criteria', 'Evidence Required'\n          ]\n\n          errors = []\n          for f in sorted(Path('scenarios').glob('*.md')):\n              content = f.read_text()\n              for section in required_sections:\n                  if f'## {section}' not in content:\n                      errors.append(f'{f.name}: missing ## {section}')\n              # Verify evaluation method has a code block\n              if '\\`\\`\\`bash' not in content and '\\`\\`\\`sh' not in content:\n                  errors.append(f'{f.name}: no bash code block in Evaluation Method')\n\n          if errors:\n              print('Scenario format errors:')\n              for e in errors:\n                  print(f'  - {e}')\n              sys.exit(1)\n          else:\n              print(f'All {len(list(Path(\\\"scenarios\\\").glob(\\\"*.md\\\")))} scenarios have valid format')\n          \"\n\n      # Verify factory-protected files list is consistent\n      - name: Verify factory-protected files referenced correctly\n        run: |\n          python -c \"\n          from pathlib import Path\n          import sys\n\n          # These files must exist and be referenced in both\n          # CLAUDE.md and factory_fix.md as protected\n          protected_dirs = ['scenarios', 'scripts', 'agents', 'specs']\n          protected_files = [\n              '.github/workflows/factory.yaml',\n              '.github/codex/prompts/factory_fix.md',\n              'CLAUDE.md',\n          ]\n\n          errors = []\n          for d in protected_dirs:\n              if not Path(d).exists():\n                  errors.append(f'Protected dir {d}/ does not exist')\n\n          for f in protected_files:\n              if not Path(f).exists():\n                  errors.append(f'Protected file {f} does not exist')\n\n          # Check CLAUDE.md references them\n          claude_md = Path('CLAUDE.md').read_text()\n          for d in protected_dirs:\n              if f'/{d}/' not in claude_md:\n                  errors.append(f'CLAUDE.md does not reference /{d}/')\n\n          if errors:\n              print('Factory integrity errors:')\n              for e in errors:\n                  print(f'  - {e}')\n              sys.exit(1)\n          else:\n              print('Factory integrity check passed')\n          \"\n\n      # Anti-gaming test quality check\n      - name: Check test quality (anti-stam, anti-gaming)\n        run: python scripts/check_test_quality.py\n",
      "base": "name: CI\n\non:\n  push:\n  pull_request:\n\njobs:\n  lint-test:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt -r requirements-dev.txt\n      - name: Ruff\n        run: ruff check .\n      - name: Mypy\n        run: mypy src\n      - name: Pytest\n        run: pytest -q\n"
    },
    ".github/workflows/factory.yaml": {
      "additions": 173,
      "deletions": 83,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/.github/workflows/factory.yaml b/.github/workflows/factory.yaml\nindex aa47faf..b3441a3 100644\n--- a/.github/workflows/factory.yaml\n+++ b/.github/workflows/factory.yaml\n@@ -21,12 +21,17 @@ on:\n   push:\n     branches:\n       - \"factory/**\"\n+      - \"df-crank-**\"\n \n permissions:\n   contents: write\n   pull-requests: write\n   issues: write\n \n+concurrency:\n+  group: factory-${{ github.ref }}\n+  cancel-in-progress: false\n+\n env:\n   OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n \n@@ -73,141 +78,226 @@ jobs:\n           THRESHOLD=${{ inputs.satisfaction_threshold || '0.80' }}\n           CURRENT_ITER=$(cat artifacts/factory/iteration_count.txt)\n           SATISFIED=false\n+          SCORE=0.0\n+          ITER=0\n \n           echo \"Starting factory loop: max=$MAX_ITER, threshold=$THRESHOLD, current_iter=$CURRENT_ITER\"\n \n-          for i in $(seq 1 $MAX_ITER); do\n-            ITER=$((CURRENT_ITER + i))\n+          # \u2500\u2500 Validation-only mode (no Codex API key) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+          # When no OPENAI_API_KEY, run a single validation pass (Gate 1 + Gate 3)\n+          # and exit. No state commits, no convergence loop.\n+          # This is the expected mode when Claude Code orchestrates via browser.\n+          if [ -z \"$OPENAI_API_KEY\" ]; then\n+            echo \"No OPENAI_API_KEY \u2014 running validation-only pass (no convergence loop)\"\n             echo \"\"\n-            echo \"==========================================\"\n-            echo \"FACTORY ITERATION $ITER\"\n-            echo \"==========================================\"\n-\n-            # \u2500\u2500 Layer 1: Deterministic validation \u2500\u2500\n-            echo \"--- Layer 1: lint + typecheck + test ---\"\n-            LAYER1_PASS=true\n-            make lint 2>&1 | tee -a artifacts/factory/ci_output.log || LAYER1_PASS=false\n-            make typecheck 2>&1 | tee -a artifacts/factory/ci_output.log || LAYER1_PASS=false\n-            make test 2>&1 | tee -a artifacts/factory/ci_output.log || LAYER1_PASS=false\n-\n-            if [ \"$LAYER1_PASS\" = false ]; then\n-              echo \"Layer 1 FAILED \u2014 skipping scenarios, compiling feedback\"\n-              echo '{\"total\":0,\"passed\":0,\"failed\":0,\"skipped\":0,\"satisfaction_score\":0.0,\"results\":[],\"timestamp\":\"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'\",\"layer1_failed\":true}' > artifacts/factory/scenario_results.json\n-            else\n-              # \u2500\u2500 Layer 2: Behavioral scenarios \u2500\u2500\n-              echo \"--- Layer 2: Behavioral scenarios ---\"\n-              # Restore scenarios from safe location for evaluation\n-              if [ -d /tmp/factory_scenarios ]; then\n-                cp -r /tmp/factory_scenarios scenarios\n-              fi\n+            echo \"--- Gate 1: lint + typecheck + test ---\"\n+            GATE1_PASS=true\n+            make lint 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n+            make typecheck 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n+            make test 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n+\n+            if [ \"$GATE1_PASS\" = true ] && [ -d scenarios ]; then\n+              echo \"--- Gate 3: Behavioral scenarios ---\"\n               python scripts/run_scenarios.py --timeout 180 2>&1 | tee -a artifacts/factory/ci_output.log || true\n-              # Move scenarios out again before Codex sees them\n-              mv scenarios /tmp/factory_scenarios 2>/dev/null || true\n+              if [ -f artifacts/factory/scenario_results.json ]; then\n+                SCORE=$(python -c \"import json; r=json.load(open('artifacts/factory/scenario_results.json')); print(r.get('satisfaction_score', 0.0))\")\n+                echo \"Satisfaction score: $SCORE\"\n+              fi\n             fi\n \n-            # \u2500\u2500 Check satisfaction \u2500\u2500\n-            if [ -f artifacts/factory/scenario_results.json ]; then\n-              SCORE=$(python -c \"import json; r=json.load(open('artifacts/factory/scenario_results.json')); print(r.get('satisfaction_score', 0.0))\")\n-              echo \"Satisfaction score: $SCORE (threshold: $THRESHOLD)\"\n-\n-              MEETS=$(python -c \"print('true' if $SCORE >= $THRESHOLD else 'false')\")\n-              if [ \"$MEETS\" = \"true\" ]; then\n-                echo \"SATISFIED \u2014 satisfaction threshold met!\"\n-                SATISFIED=true\n-                echo \"$ITER\" > artifacts/factory/iteration_count.txt\n-                break\n+            echo \"Validation-only pass complete. Convergence requires OPENAI_API_KEY or Claude Code orchestration.\"\n+            # ITER stays 0 \u2014 no state commit will be pushed\n+          else\n+            # \u2500\u2500 Full convergence loop (Codex API available) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+            for i in $(seq 1 $MAX_ITER); do\n+              ITER=$((CURRENT_ITER + i))\n+              echo \"\"\n+              echo \"==========================================\"\n+              echo \"FACTORY ITERATION $ITER\"\n+              echo \"==========================================\"\n+\n+              # Clear stale results from previous iteration\n+              rm -f artifacts/factory/scenario_results.json\n+\n+              # \u2500\u2500 Gate 1: Deterministic validation \u2500\u2500\n+              echo \"--- Gate 1: lint + typecheck + test ---\"\n+              GATE1_PASS=true\n+              make lint 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n+              make typecheck 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n+              make test 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n+\n+              if [ \"$GATE1_PASS\" = false ]; then\n+                echo \"Gate 1 FAILED \u2014 skipping scenarios, compiling feedback\"\n+                echo '{\"total\":0,\"passed\":0,\"failed\":0,\"skipped\":0,\"satisfaction_score\":0.0,\"results\":[],\"timestamp\":\"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'\",\"gate1_failed\":true}' > artifacts/factory/scenario_results.json\n+              else\n+                # \u2500\u2500 Gate 3: Behavioral scenarios \u2500\u2500\n+                echo \"--- Gate 3: Behavioral scenarios ---\"\n+                # Restore scenarios from safe location for evaluation\n+                if [ -d /tmp/factory_scenarios ]; then\n+                  cp -r /tmp/factory_scenarios scenarios\n+                else\n+                  echo \"WARNING: /tmp/factory_scenarios not found \u2014 using in-place scenarios\"\n+                fi\n+                python scripts/run_scenarios.py --timeout 180 2>&1 | tee -a artifacts/factory/ci_output.log\n+                # Move scenarios out before Codex step\n+                # NOTE: /tmp/ is readable by Codex \u2014 this is a prompt-level control only.\n+                # True holdout isolation requires a separate evaluation job (see ProjectLeadAsks).\n+                mv scenarios /tmp/factory_scenarios 2>/dev/null || true\n               fi\n-            fi\n \n-            # \u2500\u2500 Compile feedback \u2500\u2500\n-            echo \"--- Compiling feedback ---\"\n-            python scripts/compile_feedback.py --iteration $ITER 2>&1\n+              # \u2500\u2500 Check satisfaction (injection-safe) \u2500\u2500\n+              if [ -f artifacts/factory/scenario_results.json ]; then\n+                SCORE=$(python -c \"import json; r=json.load(open('artifacts/factory/scenario_results.json')); print(r.get('satisfaction_score', 0.0))\")\n+                echo \"Satisfaction score: $SCORE (threshold: $THRESHOLD)\"\n+\n+                MEETS=$(python -c \"import sys; sys.exit(0 if float(sys.argv[1]) >= float(sys.argv[2]) else 1)\" \"$SCORE\" \"$THRESHOLD\" && echo \"true\" || echo \"false\")\n+                if [ \"$MEETS\" = \"true\" ]; then\n+                  echo \"SATISFIED \u2014 satisfaction threshold met!\"\n+                  SATISFIED=true\n+                  echo \"$ITER\" > artifacts/factory/iteration_count.txt\n+                  break\n+                fi\n+              else\n+                echo \"WARNING: No scenario_results.json \u2014 skipping satisfaction check\"\n+                SCORE=0.0\n+              fi\n \n-            # \u2500\u2500 Invoke Codex \u2500\u2500\n-            if [ -z \"$OPENAI_API_KEY\" ]; then\n-              echo \"WARNING: No OPENAI_API_KEY \u2014 cannot invoke Codex. Stopping loop.\"\n-              echo \"$ITER\" > artifacts/factory/iteration_count.txt\n-              break\n-            fi\n+              # \u2500\u2500 Compile feedback \u2500\u2500\n+              echo \"--- Compiling feedback ---\"\n+              python scripts/compile_feedback.py --iteration $ITER 2>&1\n \n-            echo \"--- Invoking Codex (iteration $ITER) ---\"\n-            # Filesystem shuffle: hide scenarios from Codex\n-            mv scenarios /tmp/factory_scenarios 2>/dev/null || true\n+              # \u2500\u2500 Invoke Codex \u2500\u2500\n+              echo \"--- Invoking Codex (iteration $ITER) ---\"\n+              mv scenarios /tmp/factory_scenarios 2>/dev/null || true\n \n-            # Build the Codex prompt with feedback reference\n-            FEEDBACK_FILE=\"artifacts/factory/feedback_iter_${ITER}.md\"\n-            CODEX_PROMPT=\"Read .github/codex/prompts/factory_fix.md for your instructions. Read ${FEEDBACK_FILE} for the current failures to fix. Read the specs in /specs/ for requirements.\"\n+              FEEDBACK_FILE=\"artifacts/factory/feedback_iter_${ITER}.md\"\n+              CODEX_PROMPT=\"Read .github/codex/prompts/factory_fix.md for your instructions. Read ${FEEDBACK_FILE} for the current failures to fix. Read the specs in /specs/ for requirements.\"\n \n-            # Run Codex\n-            npx @openai/codex exec \\\n-              --prompt \"$CODEX_PROMPT\" \\\n-              --sandbox workspace-write \\\n-              2>&1 | tee -a artifacts/factory/ci_output.log || true\n+              npx @openai/codex exec \\\n+                --prompt \"$CODEX_PROMPT\" \\\n+                --sandbox workspace-write \\\n+                2>&1 | tee -a artifacts/factory/ci_output.log || true\n \n-            # Restore scenarios\n-            if [ -d /tmp/factory_scenarios ]; then\n-              cp -r /tmp/factory_scenarios scenarios\n-            fi\n+              # Restore scenarios after Codex runs\n+              if [ -d /tmp/factory_scenarios ]; then\n+                cp -r /tmp/factory_scenarios scenarios\n+              fi\n \n-            # Commit Codex changes\n-            git config user.name \"dark-factory[bot]\"\n-            git config user.email \"dark-factory[bot]@users.noreply.github.com\"\n-            git add -A\n-            git diff --staged --quiet || git commit -m \"factory: iteration $ITER \u2014 codex fix\n+              # Commit Codex changes\n+              git config user.name \"dark-factory[bot]\"\n+              git config user.email \"dark-factory[bot]@users.noreply.github.com\"\n+              git add -A\n+              git diff --staged --quiet || git commit -m \"factory: iteration $ITER \u2014 codex fix\n \n-            Satisfaction: $SCORE\n-            Co-Authored-By: OpenAI Codex <noreply@openai.com>\"\n+              Satisfaction: $SCORE\n+              Co-Authored-By: OpenAI Codex <noreply@openai.com>\"\n \n-            echo \"$ITER\" > artifacts/factory/iteration_count.txt\n-          done\n+              echo \"$ITER\" > artifacts/factory/iteration_count.txt\n+            done\n+          fi\n \n           # \u2500\u2500 Export results \u2500\u2500\n           echo \"satisfied=$SATISFIED\" >> $GITHUB_OUTPUT\n-          echo \"final_score=$SCORE\" >> $GITHUB_OUTPUT\n-          echo \"iterations=$ITER\" >> $GITHUB_OUTPUT\n+          echo \"final_score=${SCORE:-0.0}\" >> $GITHUB_OUTPUT\n+          echo \"iterations=${ITER:-0}\" >> $GITHUB_OUTPUT\n \n       # \u2500\u2500 Post-loop actions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       - name: Commit factory state\n+        if: steps.factory.outputs.iterations != '0'\n         run: |\n           git config user.name \"dark-factory[bot]\"\n           git config user.email \"dark-factory[bot]@users.noreply.github.com\"\n           git add artifacts/factory/feedback_iter_*.md artifacts/factory/iteration_count.txt\n-          git diff --staged --quiet || git commit -m \"factory: update factory state\"\n+          git diff --staged --quiet || git commit -m \"factory: update factory state after ${{ steps.factory.outputs.iterations }} iterations\"\n           git push || true\n \n-      - name: Post PR comment (success)\n-        if: steps.factory.outputs.satisfied == 'true'\n+      # \u2500\u2500 Accept/Merge Gate \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+      # The factory NEVER auto-merges. On convergence, it creates\n+      # (or updates) a PR for the project lead to review and merge.\n+      # This is the human decision point \u2014 the only place where\n+      # a human evaluates whether factory output ships.\n+\n+      - name: Create or update PR (convergence gate)\n+        if: steps.factory.outputs.satisfied == 'true' && steps.factory.outputs.iterations != '0'\n         uses: actions/github-script@v7\n         with:\n           script: |\n             const score = '${{ steps.factory.outputs.final_score }}';\n             const iters = '${{ steps.factory.outputs.iterations }}';\n-            const body = `## Dark Factory \u2014 Converged\n+            const branch = context.ref.replace('refs/heads/', '');\n+            const baseBranch = 'main';\n+\n+            const body = `## Dark Factory \u2014 Converged \u2705\n \n             **Satisfaction score: ${(parseFloat(score) * 100).toFixed(0)}%**\n             Iterations: ${iters}\n \n-            The factory loop has met the satisfaction threshold. Ready for human review of the behavioral results.`;\n+            The factory loop has met the satisfaction threshold.\n \n-            // Find open PRs for this branch\n-            const { data: prs } = await github.rest.pulls.list({\n+            ### Accept/Merge Gate\n+            This PR was produced entirely by the dark factory convergence loop.\n+            Code was never reviewed during production \u2014 correctness was inferred\n+            from behavioral scenario evaluation.\n+\n+            **Before merging, verify:**\n+            - [ ] Satisfaction score meets your quality bar\n+            - [ ] Review \\`artifacts/factory/feedback_iter_${iters}.md\\` for residual warnings\n+            - [ ] Run \\`make factory-local\\` locally if you want additional confidence\n+            - [ ] No unexpected files or dependencies introduced\n+\n+            **To merge:** Approve and merge this PR. The factory branch can then be deleted.\n+            **To reject:** Close this PR and either adjust scenarios/specs or trigger another factory run.`;\n+\n+            // Check for existing PR from this branch\n+            const { data: existingPrs } = await github.rest.pulls.list({\n               owner: context.repo.owner,\n               repo: context.repo.repo,\n-              head: `${context.repo.owner}:${context.ref.replace('refs/heads/', '')}`,\n+              head: `${context.repo.owner}:${branch}`,\n+              base: baseBranch,\n               state: 'open'\n             });\n \n-            for (const pr of prs) {\n+            if (existingPrs.length > 0) {\n+              // Update existing PR\n+              const pr = existingPrs[0];\n+              await github.rest.pulls.update({\n+                owner: context.repo.owner,\n+                repo: context.repo.repo,\n+                pull_number: pr.number,\n+                body: body\n+              });\n               await github.rest.issues.createComment({\n                 owner: context.repo.owner,\n                 repo: context.repo.repo,\n                 issue_number: pr.number,\n+                body: `Factory converged at ${(parseFloat(score) * 100).toFixed(0)}% after ${iters} iterations. Ready for accept/merge review.`\n+              });\n+              await github.rest.issues.addLabels({\n+                owner: context.repo.owner,\n+                repo: context.repo.repo,\n+                issue_number: pr.number,\n+                labels: ['factory-converged', 'accept-merge-gate']\n+              });\n+            } else {\n+              // Create new PR\n+              const { data: pr } = await github.rest.pulls.create({\n+                owner: context.repo.owner,\n+                repo: context.repo.repo,\n+                title: `[Factory] Converged at ${(parseFloat(score) * 100).toFixed(0)}% \u2014 ready for accept/merge`,\n+                head: branch,\n+                base: baseBranch,\n                 body: body\n               });\n+              await github.rest.issues.addLabels({\n+                owner: context.repo.owner,\n+                repo: context.repo.repo,\n+                issue_number: pr.number,\n+                labels: ['factory-converged', 'accept-merge-gate']\n+              });\n             }\n \n       - name: Post PR comment (stalled)\n-        if: steps.factory.outputs.satisfied == 'false'\n+        if: steps.factory.outputs.satisfied == 'false' && steps.factory.outputs.iterations != '0'\n         uses: actions/github-script@v7\n         with:\n           script: |\n@@ -218,7 +308,7 @@ jobs:\n             **Satisfaction score: ${(parseFloat(score) * 100).toFixed(0)}%** (threshold not met)\n             Iterations used: ${iters}\n \n-            The factory loop did not converge. @joeyfezster \u2014 escalation needed.\n+            The factory loop did not converge. Escalation needed \u2014 tag the project lead.\n \n             Check \\`artifacts/factory/feedback_iter_${iters}.md\\` for the latest failure analysis.`;\n \n",
      "raw": "name: Dark Factory\n\non:\n  workflow_dispatch:\n    inputs:\n      max_iterations:\n        description: \"Maximum convergence iterations\"\n        required: false\n        default: \"5\"\n        type: string\n      satisfaction_threshold:\n        description: \"Satisfaction score to stop (0.0-1.0)\"\n        required: false\n        default: \"0.80\"\n        type: string\n      target_branch:\n        description: \"Branch to run factory on (default: triggering branch)\"\n        required: false\n        default: \"\"\n        type: string\n  push:\n    branches:\n      - \"factory/**\"\n      - \"df-crank-**\"\n\npermissions:\n  contents: write\n  pull-requests: write\n  issues: write\n\nconcurrency:\n  group: factory-${{ github.ref }}\n  cancel-in-progress: false\n\nenv:\n  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n\njobs:\n  factory-loop:\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n\n    steps:\n      # \u2500\u2500 Setup \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          ref: ${{ inputs.target_branch || github.ref }}\n          fetch-depth: 0\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt -r requirements-dev.txt\n\n      - name: Set up Node.js (for Codex CLI)\n        uses: actions/setup-node@v4\n        with:\n          node-version: \"20\"\n\n      - name: Initialize factory state\n        run: |\n          mkdir -p artifacts/factory\n          if [ ! -f artifacts/factory/iteration_count.txt ]; then\n            echo \"0\" > artifacts/factory/iteration_count.txt\n          fi\n\n      # \u2500\u2500 Convergence Loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: Run convergence loop\n        id: factory\n        run: |\n          MAX_ITER=${{ inputs.max_iterations || '5' }}\n          THRESHOLD=${{ inputs.satisfaction_threshold || '0.80' }}\n          CURRENT_ITER=$(cat artifacts/factory/iteration_count.txt)\n          SATISFIED=false\n          SCORE=0.0\n          ITER=0\n\n          echo \"Starting factory loop: max=$MAX_ITER, threshold=$THRESHOLD, current_iter=$CURRENT_ITER\"\n\n          # \u2500\u2500 Validation-only mode (no Codex API key) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n          # When no OPENAI_API_KEY, run a single validation pass (Gate 1 + Gate 3)\n          # and exit. No state commits, no convergence loop.\n          # This is the expected mode when Claude Code orchestrates via browser.\n          if [ -z \"$OPENAI_API_KEY\" ]; then\n            echo \"No OPENAI_API_KEY \u2014 running validation-only pass (no convergence loop)\"\n            echo \"\"\n            echo \"--- Gate 1: lint + typecheck + test ---\"\n            GATE1_PASS=true\n            make lint 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n            make typecheck 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n            make test 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n\n            if [ \"$GATE1_PASS\" = true ] && [ -d scenarios ]; then\n              echo \"--- Gate 3: Behavioral scenarios ---\"\n              python scripts/run_scenarios.py --timeout 180 2>&1 | tee -a artifacts/factory/ci_output.log || true\n              if [ -f artifacts/factory/scenario_results.json ]; then\n                SCORE=$(python -c \"import json; r=json.load(open('artifacts/factory/scenario_results.json')); print(r.get('satisfaction_score', 0.0))\")\n                echo \"Satisfaction score: $SCORE\"\n              fi\n            fi\n\n            echo \"Validation-only pass complete. Convergence requires OPENAI_API_KEY or Claude Code orchestration.\"\n            # ITER stays 0 \u2014 no state commit will be pushed\n          else\n            # \u2500\u2500 Full convergence loop (Codex API available) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            for i in $(seq 1 $MAX_ITER); do\n              ITER=$((CURRENT_ITER + i))\n              echo \"\"\n              echo \"==========================================\"\n              echo \"FACTORY ITERATION $ITER\"\n              echo \"==========================================\"\n\n              # Clear stale results from previous iteration\n              rm -f artifacts/factory/scenario_results.json\n\n              # \u2500\u2500 Gate 1: Deterministic validation \u2500\u2500\n              echo \"--- Gate 1: lint + typecheck + test ---\"\n              GATE1_PASS=true\n              make lint 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n              make typecheck 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n              make test 2>&1 | tee -a artifacts/factory/ci_output.log || GATE1_PASS=false\n\n              if [ \"$GATE1_PASS\" = false ]; then\n                echo \"Gate 1 FAILED \u2014 skipping scenarios, compiling feedback\"\n                echo '{\"total\":0,\"passed\":0,\"failed\":0,\"skipped\":0,\"satisfaction_score\":0.0,\"results\":[],\"timestamp\":\"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'\",\"gate1_failed\":true}' > artifacts/factory/scenario_results.json\n              else\n                # \u2500\u2500 Gate 3: Behavioral scenarios \u2500\u2500\n                echo \"--- Gate 3: Behavioral scenarios ---\"\n                # Restore scenarios from safe location for evaluation\n                if [ -d /tmp/factory_scenarios ]; then\n                  cp -r /tmp/factory_scenarios scenarios\n                else\n                  echo \"WARNING: /tmp/factory_scenarios not found \u2014 using in-place scenarios\"\n                fi\n                python scripts/run_scenarios.py --timeout 180 2>&1 | tee -a artifacts/factory/ci_output.log\n                # Move scenarios out before Codex step\n                # NOTE: /tmp/ is readable by Codex \u2014 this is a prompt-level control only.\n                # True holdout isolation requires a separate evaluation job (see ProjectLeadAsks).\n                mv scenarios /tmp/factory_scenarios 2>/dev/null || true\n              fi\n\n              # \u2500\u2500 Check satisfaction (injection-safe) \u2500\u2500\n              if [ -f artifacts/factory/scenario_results.json ]; then\n                SCORE=$(python -c \"import json; r=json.load(open('artifacts/factory/scenario_results.json')); print(r.get('satisfaction_score', 0.0))\")\n                echo \"Satisfaction score: $SCORE (threshold: $THRESHOLD)\"\n\n                MEETS=$(python -c \"import sys; sys.exit(0 if float(sys.argv[1]) >= float(sys.argv[2]) else 1)\" \"$SCORE\" \"$THRESHOLD\" && echo \"true\" || echo \"false\")\n                if [ \"$MEETS\" = \"true\" ]; then\n                  echo \"SATISFIED \u2014 satisfaction threshold met!\"\n                  SATISFIED=true\n                  echo \"$ITER\" > artifacts/factory/iteration_count.txt\n                  break\n                fi\n              else\n                echo \"WARNING: No scenario_results.json \u2014 skipping satisfaction check\"\n                SCORE=0.0\n              fi\n\n              # \u2500\u2500 Compile feedback \u2500\u2500\n              echo \"--- Compiling feedback ---\"\n              python scripts/compile_feedback.py --iteration $ITER 2>&1\n\n              # \u2500\u2500 Invoke Codex \u2500\u2500\n              echo \"--- Invoking Codex (iteration $ITER) ---\"\n              mv scenarios /tmp/factory_scenarios 2>/dev/null || true\n\n              FEEDBACK_FILE=\"artifacts/factory/feedback_iter_${ITER}.md\"\n              CODEX_PROMPT=\"Read .github/codex/prompts/factory_fix.md for your instructions. Read ${FEEDBACK_FILE} for the current failures to fix. Read the specs in /specs/ for requirements.\"\n\n              npx @openai/codex exec \\\n                --prompt \"$CODEX_PROMPT\" \\\n                --sandbox workspace-write \\\n                2>&1 | tee -a artifacts/factory/ci_output.log || true\n\n              # Restore scenarios after Codex runs\n              if [ -d /tmp/factory_scenarios ]; then\n                cp -r /tmp/factory_scenarios scenarios\n              fi\n\n              # Commit Codex changes\n              git config user.name \"dark-factory[bot]\"\n              git config user.email \"dark-factory[bot]@users.noreply.github.com\"\n              git add -A\n              git diff --staged --quiet || git commit -m \"factory: iteration $ITER \u2014 codex fix\n\n              Satisfaction: $SCORE\n              Co-Authored-By: OpenAI Codex <noreply@openai.com>\"\n\n              echo \"$ITER\" > artifacts/factory/iteration_count.txt\n            done\n          fi\n\n          # \u2500\u2500 Export results \u2500\u2500\n          echo \"satisfied=$SATISFIED\" >> $GITHUB_OUTPUT\n          echo \"final_score=${SCORE:-0.0}\" >> $GITHUB_OUTPUT\n          echo \"iterations=${ITER:-0}\" >> $GITHUB_OUTPUT\n\n      # \u2500\u2500 Post-loop actions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: Commit factory state\n        if: steps.factory.outputs.iterations != '0'\n        run: |\n          git config user.name \"dark-factory[bot]\"\n          git config user.email \"dark-factory[bot]@users.noreply.github.com\"\n          git add artifacts/factory/feedback_iter_*.md artifacts/factory/iteration_count.txt\n          git diff --staged --quiet || git commit -m \"factory: update factory state after ${{ steps.factory.outputs.iterations }} iterations\"\n          git push || true\n\n      # \u2500\u2500 Accept/Merge Gate \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      # The factory NEVER auto-merges. On convergence, it creates\n      # (or updates) a PR for the project lead to review and merge.\n      # This is the human decision point \u2014 the only place where\n      # a human evaluates whether factory output ships.\n\n      - name: Create or update PR (convergence gate)\n        if: steps.factory.outputs.satisfied == 'true' && steps.factory.outputs.iterations != '0'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const score = '${{ steps.factory.outputs.final_score }}';\n            const iters = '${{ steps.factory.outputs.iterations }}';\n            const branch = context.ref.replace('refs/heads/', '');\n            const baseBranch = 'main';\n\n            const body = `## Dark Factory \u2014 Converged \u2705\n\n            **Satisfaction score: ${(parseFloat(score) * 100).toFixed(0)}%**\n            Iterations: ${iters}\n\n            The factory loop has met the satisfaction threshold.\n\n            ### Accept/Merge Gate\n            This PR was produced entirely by the dark factory convergence loop.\n            Code was never reviewed during production \u2014 correctness was inferred\n            from behavioral scenario evaluation.\n\n            **Before merging, verify:**\n            - [ ] Satisfaction score meets your quality bar\n            - [ ] Review \\`artifacts/factory/feedback_iter_${iters}.md\\` for residual warnings\n            - [ ] Run \\`make factory-local\\` locally if you want additional confidence\n            - [ ] No unexpected files or dependencies introduced\n\n            **To merge:** Approve and merge this PR. The factory branch can then be deleted.\n            **To reject:** Close this PR and either adjust scenarios/specs or trigger another factory run.`;\n\n            // Check for existing PR from this branch\n            const { data: existingPrs } = await github.rest.pulls.list({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              head: `${context.repo.owner}:${branch}`,\n              base: baseBranch,\n              state: 'open'\n            });\n\n            if (existingPrs.length > 0) {\n              // Update existing PR\n              const pr = existingPrs[0];\n              await github.rest.pulls.update({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                pull_number: pr.number,\n                body: body\n              });\n              await github.rest.issues.createComment({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: pr.number,\n                body: `Factory converged at ${(parseFloat(score) * 100).toFixed(0)}% after ${iters} iterations. Ready for accept/merge review.`\n              });\n              await github.rest.issues.addLabels({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: pr.number,\n                labels: ['factory-converged', 'accept-merge-gate']\n              });\n            } else {\n              // Create new PR\n              const { data: pr } = await github.rest.pulls.create({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                title: `[Factory] Converged at ${(parseFloat(score) * 100).toFixed(0)}% \u2014 ready for accept/merge`,\n                head: branch,\n                base: baseBranch,\n                body: body\n              });\n              await github.rest.issues.addLabels({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: pr.number,\n                labels: ['factory-converged', 'accept-merge-gate']\n              });\n            }\n\n      - name: Post PR comment (stalled)\n        if: steps.factory.outputs.satisfied == 'false' && steps.factory.outputs.iterations != '0'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const score = '${{ steps.factory.outputs.final_score }}';\n            const iters = '${{ steps.factory.outputs.iterations }}';\n            const body = `## Dark Factory \u2014 Stalled\n\n            **Satisfaction score: ${(parseFloat(score) * 100).toFixed(0)}%** (threshold not met)\n            Iterations used: ${iters}\n\n            The factory loop did not converge. Escalation needed \u2014 tag the project lead.\n\n            Check \\`artifacts/factory/feedback_iter_${iters}.md\\` for the latest failure analysis.`;\n\n            const { data: prs } = await github.rest.pulls.list({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              head: `${context.repo.owner}:${context.ref.replace('refs/heads/', '')}`,\n              state: 'open'\n            });\n\n            for (const pr of prs) {\n              await github.rest.issues.createComment({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: pr.number,\n                body: body\n              });\n            }\n\n      - name: Upload factory artifacts\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: factory-artifacts\n          path: artifacts/factory/\n",
      "base": "name: Dark Factory\n\non:\n  workflow_dispatch:\n    inputs:\n      max_iterations:\n        description: \"Maximum convergence iterations\"\n        required: false\n        default: \"5\"\n        type: string\n      satisfaction_threshold:\n        description: \"Satisfaction score to stop (0.0-1.0)\"\n        required: false\n        default: \"0.80\"\n        type: string\n      target_branch:\n        description: \"Branch to run factory on (default: triggering branch)\"\n        required: false\n        default: \"\"\n        type: string\n  push:\n    branches:\n      - \"factory/**\"\n\npermissions:\n  contents: write\n  pull-requests: write\n  issues: write\n\nenv:\n  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n\njobs:\n  factory-loop:\n    runs-on: ubuntu-latest\n    timeout-minutes: 60\n\n    steps:\n      # \u2500\u2500 Setup \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          ref: ${{ inputs.target_branch || github.ref }}\n          fetch-depth: 0\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt -r requirements-dev.txt\n\n      - name: Set up Node.js (for Codex CLI)\n        uses: actions/setup-node@v4\n        with:\n          node-version: \"20\"\n\n      - name: Initialize factory state\n        run: |\n          mkdir -p artifacts/factory\n          if [ ! -f artifacts/factory/iteration_count.txt ]; then\n            echo \"0\" > artifacts/factory/iteration_count.txt\n          fi\n\n      # \u2500\u2500 Convergence Loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: Run convergence loop\n        id: factory\n        run: |\n          MAX_ITER=${{ inputs.max_iterations || '5' }}\n          THRESHOLD=${{ inputs.satisfaction_threshold || '0.80' }}\n          CURRENT_ITER=$(cat artifacts/factory/iteration_count.txt)\n          SATISFIED=false\n\n          echo \"Starting factory loop: max=$MAX_ITER, threshold=$THRESHOLD, current_iter=$CURRENT_ITER\"\n\n          for i in $(seq 1 $MAX_ITER); do\n            ITER=$((CURRENT_ITER + i))\n            echo \"\"\n            echo \"==========================================\"\n            echo \"FACTORY ITERATION $ITER\"\n            echo \"==========================================\"\n\n            # \u2500\u2500 Layer 1: Deterministic validation \u2500\u2500\n            echo \"--- Layer 1: lint + typecheck + test ---\"\n            LAYER1_PASS=true\n            make lint 2>&1 | tee -a artifacts/factory/ci_output.log || LAYER1_PASS=false\n            make typecheck 2>&1 | tee -a artifacts/factory/ci_output.log || LAYER1_PASS=false\n            make test 2>&1 | tee -a artifacts/factory/ci_output.log || LAYER1_PASS=false\n\n            if [ \"$LAYER1_PASS\" = false ]; then\n              echo \"Layer 1 FAILED \u2014 skipping scenarios, compiling feedback\"\n              echo '{\"total\":0,\"passed\":0,\"failed\":0,\"skipped\":0,\"satisfaction_score\":0.0,\"results\":[],\"timestamp\":\"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'\",\"layer1_failed\":true}' > artifacts/factory/scenario_results.json\n            else\n              # \u2500\u2500 Layer 2: Behavioral scenarios \u2500\u2500\n              echo \"--- Layer 2: Behavioral scenarios ---\"\n              # Restore scenarios from safe location for evaluation\n              if [ -d /tmp/factory_scenarios ]; then\n                cp -r /tmp/factory_scenarios scenarios\n              fi\n              python scripts/run_scenarios.py --timeout 180 2>&1 | tee -a artifacts/factory/ci_output.log || true\n              # Move scenarios out again before Codex sees them\n              mv scenarios /tmp/factory_scenarios 2>/dev/null || true\n            fi\n\n            # \u2500\u2500 Check satisfaction \u2500\u2500\n            if [ -f artifacts/factory/scenario_results.json ]; then\n              SCORE=$(python -c \"import json; r=json.load(open('artifacts/factory/scenario_results.json')); print(r.get('satisfaction_score', 0.0))\")\n              echo \"Satisfaction score: $SCORE (threshold: $THRESHOLD)\"\n\n              MEETS=$(python -c \"print('true' if $SCORE >= $THRESHOLD else 'false')\")\n              if [ \"$MEETS\" = \"true\" ]; then\n                echo \"SATISFIED \u2014 satisfaction threshold met!\"\n                SATISFIED=true\n                echo \"$ITER\" > artifacts/factory/iteration_count.txt\n                break\n              fi\n            fi\n\n            # \u2500\u2500 Compile feedback \u2500\u2500\n            echo \"--- Compiling feedback ---\"\n            python scripts/compile_feedback.py --iteration $ITER 2>&1\n\n            # \u2500\u2500 Invoke Codex \u2500\u2500\n            if [ -z \"$OPENAI_API_KEY\" ]; then\n              echo \"WARNING: No OPENAI_API_KEY \u2014 cannot invoke Codex. Stopping loop.\"\n              echo \"$ITER\" > artifacts/factory/iteration_count.txt\n              break\n            fi\n\n            echo \"--- Invoking Codex (iteration $ITER) ---\"\n            # Filesystem shuffle: hide scenarios from Codex\n            mv scenarios /tmp/factory_scenarios 2>/dev/null || true\n\n            # Build the Codex prompt with feedback reference\n            FEEDBACK_FILE=\"artifacts/factory/feedback_iter_${ITER}.md\"\n            CODEX_PROMPT=\"Read .github/codex/prompts/factory_fix.md for your instructions. Read ${FEEDBACK_FILE} for the current failures to fix. Read the specs in /specs/ for requirements.\"\n\n            # Run Codex\n            npx @openai/codex exec \\\n              --prompt \"$CODEX_PROMPT\" \\\n              --sandbox workspace-write \\\n              2>&1 | tee -a artifacts/factory/ci_output.log || true\n\n            # Restore scenarios\n            if [ -d /tmp/factory_scenarios ]; then\n              cp -r /tmp/factory_scenarios scenarios\n            fi\n\n            # Commit Codex changes\n            git config user.name \"dark-factory[bot]\"\n            git config user.email \"dark-factory[bot]@users.noreply.github.com\"\n            git add -A\n            git diff --staged --quiet || git commit -m \"factory: iteration $ITER \u2014 codex fix\n\n            Satisfaction: $SCORE\n            Co-Authored-By: OpenAI Codex <noreply@openai.com>\"\n\n            echo \"$ITER\" > artifacts/factory/iteration_count.txt\n          done\n\n          # \u2500\u2500 Export results \u2500\u2500\n          echo \"satisfied=$SATISFIED\" >> $GITHUB_OUTPUT\n          echo \"final_score=$SCORE\" >> $GITHUB_OUTPUT\n          echo \"iterations=$ITER\" >> $GITHUB_OUTPUT\n\n      # \u2500\u2500 Post-loop actions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n      - name: Commit factory state\n        run: |\n          git config user.name \"dark-factory[bot]\"\n          git config user.email \"dark-factory[bot]@users.noreply.github.com\"\n          git add artifacts/factory/feedback_iter_*.md artifacts/factory/iteration_count.txt\n          git diff --staged --quiet || git commit -m \"factory: update factory state\"\n          git push || true\n\n      - name: Post PR comment (success)\n        if: steps.factory.outputs.satisfied == 'true'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const score = '${{ steps.factory.outputs.final_score }}';\n            const iters = '${{ steps.factory.outputs.iterations }}';\n            const body = `## Dark Factory \u2014 Converged\n\n            **Satisfaction score: ${(parseFloat(score) * 100).toFixed(0)}%**\n            Iterations: ${iters}\n\n            The factory loop has met the satisfaction threshold. Ready for human review of the behavioral results.`;\n\n            // Find open PRs for this branch\n            const { data: prs } = await github.rest.pulls.list({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              head: `${context.repo.owner}:${context.ref.replace('refs/heads/', '')}`,\n              state: 'open'\n            });\n\n            for (const pr of prs) {\n              await github.rest.issues.createComment({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: pr.number,\n                body: body\n              });\n            }\n\n      - name: Post PR comment (stalled)\n        if: steps.factory.outputs.satisfied == 'false'\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const score = '${{ steps.factory.outputs.final_score }}';\n            const iters = '${{ steps.factory.outputs.iterations }}';\n            const body = `## Dark Factory \u2014 Stalled\n\n            **Satisfaction score: ${(parseFloat(score) * 100).toFixed(0)}%** (threshold not met)\n            Iterations used: ${iters}\n\n            The factory loop did not converge. @joeyfezster \u2014 escalation needed.\n\n            Check \\`artifacts/factory/feedback_iter_${iters}.md\\` for the latest failure analysis.`;\n\n            const { data: prs } = await github.rest.pulls.list({\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              head: `${context.repo.owner}:${context.ref.replace('refs/heads/', '')}`,\n              state: 'open'\n            });\n\n            for (const pr of prs) {\n              await github.rest.issues.createComment({\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                issue_number: pr.number,\n                body: body\n              });\n            }\n\n      - name: Upload factory artifacts\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: factory-artifacts\n          path: artifacts/factory/\n"
    },
    ".gitignore": {
      "additions": 3,
      "deletions": 10,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/.gitignore b/.gitignore\nindex ae40f8d..cf8ecd7 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -1,16 +1,9 @@\n __pycache__/\n-*.pyc\n-*.pyo\n-*.pyd\n-*.egg-info/\n-.dist/\n-.build/\n-.venv/\n-.env/\n-.mypy_cache/\n-.ruff_cache/\n .pytest_cache/\n+.mypy_cache/\n+.venv/\n .DS_Store\n+*.pyc\n \n # Training artifacts (large, ephemeral)\n artifacts/*/tensorboard/\n",
      "raw": "__pycache__/\n.pytest_cache/\n.mypy_cache/\n.venv/\n.DS_Store\n*.pyc\n\n# Training artifacts (large, ephemeral)\nartifacts/*/tensorboard/\nartifacts/*/checkpoints/\nartifacts/*/videos/\nartifacts/*/logs.jsonl\n\n# Factory ephemeral artifacts (feedback files ARE committed)\nartifacts/factory/scenario_results.json\nartifacts/factory/ci_output.log\n",
      "base": "__pycache__/\n*.pyc\n*.pyo\n*.pyd\n*.egg-info/\n.dist/\n.build/\n.venv/\n.env/\n.mypy_cache/\n.ruff_cache/\n.pytest_cache/\n.DS_Store\n\n# Training artifacts (large, ephemeral)\nartifacts/*/tensorboard/\nartifacts/*/checkpoints/\nartifacts/*/videos/\nartifacts/*/logs.jsonl\n\n# Factory ephemeral artifacts (feedback files ARE committed)\nartifacts/factory/scenario_results.json\nartifacts/factory/ci_output.log\n"
    },
    ".pre-commit-config.yaml": {
      "additions": 6,
      "deletions": 14,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 0ade28d..355d968 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -1,18 +1,10 @@\n repos:\n-  - repo: local\n+  - repo: https://github.com/astral-sh/ruff-pre-commit\n+    rev: v0.15.1\n     hooks:\n       - id: ruff\n-        name: ruff\n-        entry: ruff check .\n-        language: system\n-        types: [python]\n+      - id: ruff-format\n+  - repo: https://github.com/pre-commit/mirrors-mypy\n+    rev: v1.19.1\n+    hooks:\n       - id: mypy\n-        name: mypy\n-        entry: mypy src\n-        language: system\n-        types: [python]\n-      - id: pytest\n-        name: pytest\n-        entry: pytest -q\n-        language: system\n-        types: [python]\n",
      "raw": "repos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.15.1\n    hooks:\n      - id: ruff\n      - id: ruff-format\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.19.1\n    hooks:\n      - id: mypy\n",
      "base": "repos:\n  - repo: local\n    hooks:\n      - id: ruff\n        name: ruff\n        entry: ruff check .\n        language: system\n        types: [python]\n      - id: mypy\n        name: mypy\n        entry: mypy src\n        language: system\n        types: [python]\n      - id: pytest\n        name: pytest\n        entry: pytest -q\n        language: system\n        types: [python]\n"
    },
    "CLAUDE.md": {
      "additions": 21,
      "deletions": 4,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/CLAUDE.md b/CLAUDE.md\nindex fca884a..5244f85 100644\n--- a/CLAUDE.md\n+++ b/CLAUDE.md\n@@ -24,26 +24,43 @@ The following files are **never touched by the Attractor (Codex)**. They are fac\n - `/.github/workflows/factory.yaml` \u2014 factory orchestrator\n - `/.github/codex/prompts/factory_fix.md` \u2014 Codex prompt template\n - `/specs/` \u2014 component specifications (read-only for Codex)\n+- `/agents/` \u2014 pre-factory agent definitions (reference only)\n+- `/scripts/strip_holdout.py` \u2014 holdout stripping script (isolation gate)\n+- `/scripts/restore_holdout.py` \u2014 holdout restoration script\n+- `/scripts/nfr_checks.py` \u2014 Gate 2 NFR checker\n - `/CLAUDE.md` \u2014 this file\n \n+## Code Quality Standards\n+\n+All code written in this repository \u2014 by Codex, Claude Code, or humans \u2014 must follow the standards in `docs/code_quality_standards.md`. This includes:\n+- Anti-stam test rules (no mocking the system under test, no stub assertions)\n+- Anti-gaming rules (no hardcoded lookup tables, no overfitting)\n+- Implementation honesty (real imports, real configs, real dependencies)\n+- Test hygiene and quality gates\n+\n+These standards are enforced by Gate 0 (adversarial review), Gate 1 (lint/typecheck/test), Gate 2 (NFR checks), and the LLM-as-judge.\n+\n ## Quick Commands\n \n ```bash\n+make install-hooks         # set up git hooks (ruff + mypy on every commit, no virtualenv needed)\n make validate              # lint + typecheck + test + docker-build + docker-smoke + env-smoke\n make run-scenarios         # run holdout scenario evaluation\n make compile-feedback      # compile validation results into feedback markdown\n-make factory-local         # run one factory iteration locally (validate \u2192 scenarios \u2192 feedback)\n+make nfr-check             # run Gate 2 NFR checks (code quality, complexity, dead code, security)\n+make factory-local         # run one factory iteration locally (Gate 1 \u2192 Gate 2 \u2192 Gate 3 \u2192 feedback)\n make factory-status        # show current iteration count and satisfaction score\n ```\n \n ## Human Decision Log\n \n-- `/ProjectLeadAsks.md` \u2014 Open questions and decisions requiring Joey's input. **Check this file at every session start.** Update it when questions are resolved or new ones arise. This file survives context compaction \u2014 it's the canonical list of what's pending.\n+- `/ProjectLeadAsks.md` \u2014 Open questions and decisions requiring the project lead's input. **Check this file at every session start.** Update it when questions are resolved or new ones arise. This file survives context compaction \u2014 it's the canonical list of what's pending.\n \n ## Stack\n \n - Python 3.12, pip-tools for dependency management\n - PyTorch, Gymnasium, NumPy for RL\n - ruff + mypy + pytest for quality\n-- GitHub Actions for CI and factory orchestration\n-- OpenAI Codex (via `codex-action`) as the non-interactive coding agent\n+- GitHub Actions for CI and validation\n+- OpenAI Codex as the non-interactive coding agent (attractor)\n+- Claude Code as factory orchestrator (skill: `/factory-orchestrate`)\n",
      "raw": "# MiniPong RL System \u2014 Dark Factory\n\nEnd-to-end proof that a reinforcement learning agent can learn Pong from pixels, built entirely by AI agents orchestrated through a convergence loop.\n\n## Operating Model\n\nThis repo is built by a **dark factory loop**, not by humans writing code. Code is treated as opaque weights \u2014 correctness is inferred exclusively from externally observable behavior, never from source inspection.\n\nThe loop: **Seed \u2192 Agent \u2192 Validate \u2192 Feedback \u2192 Repeat until satisfied.**\n\n## Source of Truth\n\n- `/specs/` \u2014 Component specifications. This is what the coding agent reads. The specs define what the system should do.\n- `/scenarios/` \u2014 Behavioral holdout evaluation criteria. These are what the system is evaluated against. **Scenarios must NEVER be modified by the coding agent (Codex).** They are the holdout set \u2014 the agent never sees its own evaluation criteria.\n- `/docs/dark_factory.md` \u2014 Full factory documentation: how the loop works, how to trigger it, how to write scenarios, when to escalate.\n\n## Factory-Protected Files\n\nThe following files are **never touched by the Attractor (Codex)**. They are factory infrastructure, not product code:\n\n- `/scenarios/` \u2014 holdout evaluation criteria\n- `/scripts/run_scenarios.py` \u2014 scenario evaluation runner\n- `/scripts/compile_feedback.py` \u2014 feedback compiler\n- `/.github/workflows/factory.yaml` \u2014 factory orchestrator\n- `/.github/codex/prompts/factory_fix.md` \u2014 Codex prompt template\n- `/specs/` \u2014 component specifications (read-only for Codex)\n- `/agents/` \u2014 pre-factory agent definitions (reference only)\n- `/scripts/strip_holdout.py` \u2014 holdout stripping script (isolation gate)\n- `/scripts/restore_holdout.py` \u2014 holdout restoration script\n- `/scripts/nfr_checks.py` \u2014 Gate 2 NFR checker\n- `/CLAUDE.md` \u2014 this file\n\n## Code Quality Standards\n\nAll code written in this repository \u2014 by Codex, Claude Code, or humans \u2014 must follow the standards in `docs/code_quality_standards.md`. This includes:\n- Anti-stam test rules (no mocking the system under test, no stub assertions)\n- Anti-gaming rules (no hardcoded lookup tables, no overfitting)\n- Implementation honesty (real imports, real configs, real dependencies)\n- Test hygiene and quality gates\n\nThese standards are enforced by Gate 0 (adversarial review), Gate 1 (lint/typecheck/test), Gate 2 (NFR checks), and the LLM-as-judge.\n\n## Quick Commands\n\n```bash\nmake install-hooks         # set up git hooks (ruff + mypy on every commit, no virtualenv needed)\nmake validate              # lint + typecheck + test + docker-build + docker-smoke + env-smoke\nmake run-scenarios         # run holdout scenario evaluation\nmake compile-feedback      # compile validation results into feedback markdown\nmake nfr-check             # run Gate 2 NFR checks (code quality, complexity, dead code, security)\nmake factory-local         # run one factory iteration locally (Gate 1 \u2192 Gate 2 \u2192 Gate 3 \u2192 feedback)\nmake factory-status        # show current iteration count and satisfaction score\n```\n\n## Human Decision Log\n\n- `/ProjectLeadAsks.md` \u2014 Open questions and decisions requiring the project lead's input. **Check this file at every session start.** Update it when questions are resolved or new ones arise. This file survives context compaction \u2014 it's the canonical list of what's pending.\n\n## Stack\n\n- Python 3.12, pip-tools for dependency management\n- PyTorch, Gymnasium, NumPy for RL\n- ruff + mypy + pytest for quality\n- GitHub Actions for CI and validation\n- OpenAI Codex as the non-interactive coding agent (attractor)\n- Claude Code as factory orchestrator (skill: `/factory-orchestrate`)\n",
      "base": "# MiniPong RL System \u2014 Dark Factory\n\nEnd-to-end proof that a reinforcement learning agent can learn Pong from pixels, built entirely by AI agents orchestrated through a convergence loop.\n\n## Operating Model\n\nThis repo is built by a **dark factory loop**, not by humans writing code. Code is treated as opaque weights \u2014 correctness is inferred exclusively from externally observable behavior, never from source inspection.\n\nThe loop: **Seed \u2192 Agent \u2192 Validate \u2192 Feedback \u2192 Repeat until satisfied.**\n\n## Source of Truth\n\n- `/specs/` \u2014 Component specifications. This is what the coding agent reads. The specs define what the system should do.\n- `/scenarios/` \u2014 Behavioral holdout evaluation criteria. These are what the system is evaluated against. **Scenarios must NEVER be modified by the coding agent (Codex).** They are the holdout set \u2014 the agent never sees its own evaluation criteria.\n- `/docs/dark_factory.md` \u2014 Full factory documentation: how the loop works, how to trigger it, how to write scenarios, when to escalate.\n\n## Factory-Protected Files\n\nThe following files are **never touched by the Attractor (Codex)**. They are factory infrastructure, not product code:\n\n- `/scenarios/` \u2014 holdout evaluation criteria\n- `/scripts/run_scenarios.py` \u2014 scenario evaluation runner\n- `/scripts/compile_feedback.py` \u2014 feedback compiler\n- `/.github/workflows/factory.yaml` \u2014 factory orchestrator\n- `/.github/codex/prompts/factory_fix.md` \u2014 Codex prompt template\n- `/specs/` \u2014 component specifications (read-only for Codex)\n- `/CLAUDE.md` \u2014 this file\n\n## Quick Commands\n\n```bash\nmake validate              # lint + typecheck + test + docker-build + docker-smoke + env-smoke\nmake run-scenarios         # run holdout scenario evaluation\nmake compile-feedback      # compile validation results into feedback markdown\nmake factory-local         # run one factory iteration locally (validate \u2192 scenarios \u2192 feedback)\nmake factory-status        # show current iteration count and satisfaction score\n```\n\n## Human Decision Log\n\n- `/ProjectLeadAsks.md` \u2014 Open questions and decisions requiring Joey's input. **Check this file at every session start.** Update it when questions are resolved or new ones arise. This file survives context compaction \u2014 it's the canonical list of what's pending.\n\n## Stack\n\n- Python 3.12, pip-tools for dependency management\n- PyTorch, Gymnasium, NumPy for RL\n- ruff + mypy + pytest for quality\n- GitHub Actions for CI and factory orchestration\n- OpenAI Codex (via `codex-action`) as the non-interactive coding agent\n"
    },
    "LICENSE": {
      "additions": 21,
      "deletions": 201,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/LICENSE b/LICENSE\nindex 261eeb9..14fac91 100644\n--- a/LICENSE\n+++ b/LICENSE\n@@ -1,201 +1,21 @@\n-                                 Apache License\n-                           Version 2.0, January 2004\n-                        http://www.apache.org/licenses/\n-\n-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n-\n-   1. Definitions.\n-\n-      \"License\" shall mean the terms and conditions for use, reproduction,\n-      and distribution as defined by Sections 1 through 9 of this document.\n-\n-      \"Licensor\" shall mean the copyright owner or entity authorized by\n-      the copyright owner that is granting the License.\n-\n-      \"Legal Entity\" shall mean the union of the acting entity and all\n-      other entities that control, are controlled by, or are under common\n-      control with that entity. For the purposes of this definition,\n-      \"control\" means (i) the power, direct or indirect, to cause the\n-      direction or management of such entity, whether by contract or\n-      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n-      outstanding shares, or (iii) beneficial ownership of such entity.\n-\n-      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n-      exercising permissions granted by this License.\n-\n-      \"Source\" form shall mean the preferred form for making modifications,\n-      including but not limited to software source code, documentation\n-      source, and configuration files.\n-\n-      \"Object\" form shall mean any form resulting from mechanical\n-      transformation or translation of a Source form, including but\n-      not limited to compiled object code, generated documentation,\n-      and conversions to other media types.\n-\n-      \"Work\" shall mean the work of authorship, whether in Source or\n-      Object form, made available under the License, as indicated by a\n-      copyright notice that is included in or attached to the work\n-      (an example is provided in the Appendix below).\n-\n-      \"Derivative Works\" shall mean any work, whether in Source or Object\n-      form, that is based on (or derived from) the Work and for which the\n-      editorial revisions, annotations, elaborations, or other modifications\n-      represent, as a whole, an original work of authorship. For the purposes\n-      of this License, Derivative Works shall not include works that remain\n-      separable from, or merely link (or bind by name) to the interfaces of,\n-      the Work and Derivative Works thereof.\n-\n-      \"Contribution\" shall mean any work of authorship, including\n-      the original version of the Work and any modifications or additions\n-      to that Work or Derivative Works thereof, that is intentionally\n-      submitted to Licensor for inclusion in the Work by the copyright owner\n-      or by an individual or Legal Entity authorized to submit on behalf of\n-      the copyright owner. For the purposes of this definition, \"submitted\"\n-      means any form of electronic, verbal, or written communication sent\n-      to the Licensor or its representatives, including but not limited to\n-      communication on electronic mailing lists, source code control systems,\n-      and issue tracking systems that are managed by, or on behalf of, the\n-      Licensor for the purpose of discussing and improving the Work, but\n-      excluding communication that is conspicuously marked or otherwise\n-      designated in writing by the copyright owner as \"Not a Contribution.\"\n-\n-      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n-      on behalf of whom a Contribution has been received by Licensor and\n-      subsequently incorporated within the Work.\n-\n-   2. Grant of Copyright License. Subject to the terms and conditions of\n-      this License, each Contributor hereby grants to You a perpetual,\n-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n-      copyright license to reproduce, prepare Derivative Works of,\n-      publicly display, publicly perform, sublicense, and distribute the\n-      Work and such Derivative Works in Source or Object form.\n-\n-   3. Grant of Patent License. Subject to the terms and conditions of\n-      this License, each Contributor hereby grants to You a perpetual,\n-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n-      (except as stated in this section) patent license to make, have made,\n-      use, offer to sell, sell, import, and otherwise transfer the Work,\n-      where such license applies only to those patent claims licensable\n-      by such Contributor that are necessarily infringed by their\n-      Contribution(s) alone or by combination of their Contribution(s)\n-      with the Work to which such Contribution(s) was submitted. If You\n-      institute patent litigation against any entity (including a\n-      cross-claim or counterclaim in a lawsuit) alleging that the Work\n-      or a Contribution incorporated within the Work constitutes direct\n-      or contributory patent infringement, then any patent licenses\n-      granted to You under this License for that Work shall terminate\n-      as of the date such litigation is filed.\n-\n-   4. Redistribution. You may reproduce and distribute copies of the\n-      Work or Derivative Works thereof in any medium, with or without\n-      modifications, and in Source or Object form, provided that You\n-      meet the following conditions:\n-\n-      (a) You must give any other recipients of the Work or\n-          Derivative Works a copy of this License; and\n-\n-      (b) You must cause any modified files to carry prominent notices\n-          stating that You changed the files; and\n-\n-      (c) You must retain, in the Source form of any Derivative Works\n-          that You distribute, all copyright, patent, trademark, and\n-          attribution notices from the Source form of the Work,\n-          excluding those notices that do not pertain to any part of\n-          the Derivative Works; and\n-\n-      (d) If the Work includes a \"NOTICE\" text file as part of its\n-          distribution, then any Derivative Works that You distribute must\n-          include a readable copy of the attribution notices contained\n-          within such NOTICE file, excluding those notices that do not\n-          pertain to any part of the Derivative Works, in at least one\n-          of the following places: within a NOTICE text file distributed\n-          as part of the Derivative Works; within the Source form or\n-          documentation, if provided along with the Derivative Works; or,\n-          within a display generated by the Derivative Works, if and\n-          wherever such third-party notices normally appear. The contents\n-          of the NOTICE file are for informational purposes only and\n-          do not modify the License. You may add Your own attribution\n-          notices within Derivative Works that You distribute, alongside\n-          or as an addendum to the NOTICE text from the Work, provided\n-          that such additional attribution notices cannot be construed\n-          as modifying the License.\n-\n-      You may add Your own copyright statement to Your modifications and\n-      may provide additional or different license terms and conditions\n-      for use, reproduction, or distribution of Your modifications, or\n-      for any such Derivative Works as a whole, provided Your use,\n-      reproduction, and distribution of the Work otherwise complies with\n-      the conditions stated in this License.\n-\n-   5. Submission of Contributions. Unless You explicitly state otherwise,\n-      any Contribution intentionally submitted for inclusion in the Work\n-      by You to the Licensor shall be under the terms and conditions of\n-      this License, without any additional terms or conditions.\n-      Notwithstanding the above, nothing herein shall supersede or modify\n-      the terms of any separate license agreement you may have executed\n-      with Licensor regarding such Contributions.\n-\n-   6. Trademarks. This License does not grant permission to use the trade\n-      names, trademarks, service marks, or product names of the Licensor,\n-      except as required for reasonable and customary use in describing the\n-      origin of the Work and reproducing the content of the NOTICE file.\n-\n-   7. Disclaimer of Warranty. Unless required by applicable law or\n-      agreed to in writing, Licensor provides the Work (and each\n-      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n-      implied, including, without limitation, any warranties or conditions\n-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n-      PARTICULAR PURPOSE. You are solely responsible for determining the\n-      appropriateness of using or redistributing the Work and assume any\n-      risks associated with Your exercise of permissions under this License.\n-\n-   8. Limitation of Liability. In no event and under no legal theory,\n-      whether in tort (including negligence), contract, or otherwise,\n-      unless required by applicable law (such as deliberate and grossly\n-      negligent acts) or agreed to in writing, shall any Contributor be\n-      liable to You for damages, including any direct, indirect, special,\n-      incidental, or consequential damages of any character arising as a\n-      result of this License or out of the use or inability to use the\n-      Work (including but not limited to damages for loss of goodwill,\n-      work stoppage, computer failure or malfunction, or any and all\n-      other commercial damages or losses), even if such Contributor\n-      has been advised of the possibility of such damages.\n-\n-   9. Accepting Warranty or Additional Liability. While redistributing\n-      the Work or Derivative Works thereof, You may choose to offer,\n-      and charge a fee for, acceptance of support, warranty, indemnity,\n-      or other liability obligations and/or rights consistent with this\n-      License. However, in accepting such obligations, You may act only\n-      on Your own behalf and on Your sole responsibility, not on behalf\n-      of any other Contributor, and only if You agree to indemnify,\n-      defend, and hold each Contributor harmless for any liability\n-      incurred by, or claims asserted against, such Contributor by reason\n-      of your accepting any such warranty or additional liability.\n-\n-   END OF TERMS AND CONDITIONS\n-\n-   APPENDIX: How to apply the Apache License to your work.\n-\n-      To apply the Apache License to your work, attach the following\n-      boilerplate notice, with the fields enclosed by brackets \"[]\"\n-      replaced with your own identifying information. (Don't include\n-      the brackets!)  The text should be enclosed in the appropriate\n-      comment syntax for the file format. We also recommend that a\n-      file or class name and description of purpose be included on the\n-      same \"printed page\" as the copyright notice for easier\n-      identification within third-party archives.\n-\n-   Copyright [yyyy] [name of copyright owner]\n-\n-   Licensed under the Apache License, Version 2.0 (the \"License\");\n-   you may not use this file except in compliance with the License.\n-   You may obtain a copy of the License at\n-\n-       http://www.apache.org/licenses/LICENSE-2.0\n-\n-   Unless required by applicable law or agreed to in writing, software\n-   distributed under the License is distributed on an \"AS IS\" BASIS,\n-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-   See the License for the specific language governing permissions and\n-   limitations under the License.\n+MIT License\n+\n+Copyright (c) 2026\n+\n+Permission is hereby granted, free of charge, to any person obtaining a copy\n+of this software and associated documentation files (the \"Software\"), to deal\n+in the Software without restriction, including without limitation the rights\n+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n+copies of the Software, and to permit persons to whom the Software is\n+furnished to do so, subject to the following conditions:\n+\n+The above copyright notice and this permission notice shall be included in all\n+copies or substantial portions of the Software.\n+\n+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n+SOFTWARE.\n",
      "raw": "MIT License\n\nCopyright (c) 2026\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
      "base": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
    },
    "Makefile": {
      "additions": 40,
      "deletions": 20,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/Makefile b/Makefile\nindex bc8b264..57d93ed 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -1,4 +1,13 @@\n-.PHONY: lint test typecheck deps docker-build docker-smoke env-smoke train-smoke eval-smoke verify-learning dashboard validate run-scenarios compile-feedback factory-local factory-status\n+.PHONY: deps install-hooks lint typecheck test docker-build docker-smoke whitepapers-acquire whitepapers-verify env-smoke train-smoke eval-smoke verify-learning dashboard validate run-scenarios compile-feedback nfr-check factory-local factory-status\n+\n+deps:\n+\tpip-compile requirements.in\n+\tpip-compile requirements-dev.in\n+\tpip install -r requirements.txt -r requirements-dev.txt\n+\n+install-hooks: ## Set up git hooks (ruff + mypy on every commit, no virtualenv needed)\n+\tgit config core.hooksPath .githooks\n+\t@echo \"\u2705 Git hooks installed from .githooks/\"\n \n # \u2500\u2500 Quality \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n lint:\n@@ -10,10 +19,21 @@ typecheck:\n test:\n \tpytest -q\n \n-deps:\n-\tpip-compile requirements.in\n-\tpip-compile requirements-dev.in\n-\tpip install -r requirements.txt -r requirements-dev.txt\n+# \u2500\u2500 Docker \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+docker-build:\n+\tdocker build -f infra/docker/Dockerfile.train --build-arg BASE_IMAGE=python:3.12-slim -t minipong-train .\n+\tdocker build -f infra/docker/Dockerfile.demo -t minipong-demo .\n+\n+docker-smoke:\n+\tdocker run --rm minipong-train python -m src.train.train_dqn --help\n+\tdocker run --rm minipong-demo python -m src.train.record_video --help\n+\n+# \u2500\u2500 Whitepapers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+whitepapers-acquire:\n+\tpython scripts/acquire_whitepapers.py\n+\n+whitepapers-verify:\n+\tpython scripts/verify_whitepapers.py\n \n # \u2500\u2500 Environment \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n env-smoke:\n@@ -33,17 +53,8 @@ verify-learning:\n dashboard:\n \tstreamlit run src/dashboard/app.py\n \n-# \u2500\u2500 Docker \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n-docker-build:\n-\tdocker build -f infra/docker/Dockerfile.train --build-arg BASE_IMAGE=python:3.12-slim -t minipong-train .\n-\tdocker build -f infra/docker/Dockerfile.demo -t minipong-demo .\n-\n-docker-smoke:\n-\tdocker run --rm minipong-train python -m src.train.train_dqn --help\n-\tdocker run --rm minipong-demo python -m src.train.record_video --help\n-\n # \u2500\u2500 Validation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n-validate: lint typecheck test docker-build docker-smoke env-smoke\n+validate: lint typecheck test docker-build docker-smoke env-smoke whitepapers-verify\n \n # \u2500\u2500 Dark Factory \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n run-scenarios: ## Run holdout scenario evaluation\n@@ -52,8 +63,14 @@ run-scenarios: ## Run holdout scenario evaluation\n compile-feedback: ## Compile validation results into feedback markdown\n \tpython scripts/compile_feedback.py\n \n-factory-local: ## Run one factory iteration locally (validate \u2192 scenarios \u2192 feedback)\n-\t@echo \"=== Layer 1: lint + typecheck + test ===\"\n+nfr-check: ## Run Gate 2 NFR checks (non-blocking quality analysis)\n+\t@mkdir -p artifacts/factory\n+\tpython scripts/nfr_checks.py --output artifacts/factory/nfr_results.json\n+\tpython scripts/nfr_checks.py\n+\n+factory-local: ## Run one factory iteration locally (Gate 1 \u2192 Gate 2 \u2192 Gate 3 \u2192 feedback)\n+\t@mkdir -p artifacts/factory\n+\t@echo \"=== Gate 1: lint + typecheck + test ===\"\n \t@make lint 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n \tLINT_EXIT=$$?; \\\n \tmake typecheck 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n@@ -61,11 +78,14 @@ factory-local: ## Run one factory iteration locally (validate \u2192 scenarios \u2192\n \tmake test 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n \tTEST_EXIT=$$?; \\\n \tif [ $$LINT_EXIT -ne 0 ] || [ $$TYPE_EXIT -ne 0 ] || [ $$TEST_EXIT -ne 0 ]; then \\\n-\t\techo \"Layer 1 FAILED \u2014 skipping scenarios\"; \\\n-\t\techo '{\"total\":0,\"passed\":0,\"failed\":0,\"skipped\":0,\"satisfaction_score\":0.0,\"results\":[],\"timestamp\":\"N/A\",\"layer1_failed\":true}' > artifacts/factory/scenario_results.json; \\\n+\t\techo \"Gate 1 FAILED \u2014 skipping Gates 2-3\"; \\\n+\t\techo '{\"total\":0,\"passed\":0,\"failed\":0,\"skipped\":0,\"satisfaction_score\":0.0,\"results\":[],\"timestamp\":\"N/A\",\"gate1_failed\":true}' > artifacts/factory/scenario_results.json; \\\n \telse \\\n \t\techo \"\"; \\\n-\t\techo \"=== Layer 2: Behavioral scenarios ===\"; \\\n+\t\techo \"=== Gate 2: NFR checks (non-blocking) ===\"; \\\n+\t\tmake nfr-check 2>&1 | tee -a artifacts/factory/ci_output.log || true; \\\n+\t\techo \"\"; \\\n+\t\techo \"=== Gate 3: Behavioral scenarios ===\"; \\\n \t\tpython scripts/run_scenarios.py --timeout 180 || true; \\\n \tfi\n \t@echo \"\"\n",
      "raw": ".PHONY: deps install-hooks lint typecheck test docker-build docker-smoke whitepapers-acquire whitepapers-verify env-smoke train-smoke eval-smoke verify-learning dashboard validate run-scenarios compile-feedback nfr-check factory-local factory-status\n\ndeps:\n\tpip-compile requirements.in\n\tpip-compile requirements-dev.in\n\tpip install -r requirements.txt -r requirements-dev.txt\n\ninstall-hooks: ## Set up git hooks (ruff + mypy on every commit, no virtualenv needed)\n\tgit config core.hooksPath .githooks\n\t@echo \"\u2705 Git hooks installed from .githooks/\"\n\n# \u2500\u2500 Quality \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nlint:\n\truff check .\n\ntypecheck:\n\tmypy src\n\ntest:\n\tpytest -q\n\n# \u2500\u2500 Docker \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndocker-build:\n\tdocker build -f infra/docker/Dockerfile.train --build-arg BASE_IMAGE=python:3.12-slim -t minipong-train .\n\tdocker build -f infra/docker/Dockerfile.demo -t minipong-demo .\n\ndocker-smoke:\n\tdocker run --rm minipong-train python -m src.train.train_dqn --help\n\tdocker run --rm minipong-demo python -m src.train.record_video --help\n\n# \u2500\u2500 Whitepapers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nwhitepapers-acquire:\n\tpython scripts/acquire_whitepapers.py\n\nwhitepapers-verify:\n\tpython scripts/verify_whitepapers.py\n\n# \u2500\u2500 Environment \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nenv-smoke:\n\tpython -c \"from src.envs.minipong import MiniPongEnv; env=MiniPongEnv(); obs,_=env.reset(seed=0); assert obs.dtype.name=='uint8'; print(obs.shape)\"\n\n# \u2500\u2500 Training \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntrain-smoke:\n\tpython -m src.train.train_dqn --config configs/dqn_minipong.yaml --run-id smoke_run\n\neval-smoke:\n\tpython -m src.train.evaluate --run-id smoke_run --episodes 2 --seeds 1 2\n\nverify-learning:\n\tpython -m src.train.verify_learning --run-id smoke_run --min-return-gain -0.1 --min-hits-gain -0.1\n\n# \u2500\u2500 Dashboard \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndashboard:\n\tstreamlit run src/dashboard/app.py\n\n# \u2500\u2500 Validation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nvalidate: lint typecheck test docker-build docker-smoke env-smoke whitepapers-verify\n\n# \u2500\u2500 Dark Factory \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nrun-scenarios: ## Run holdout scenario evaluation\n\tpython scripts/run_scenarios.py\n\ncompile-feedback: ## Compile validation results into feedback markdown\n\tpython scripts/compile_feedback.py\n\nnfr-check: ## Run Gate 2 NFR checks (non-blocking quality analysis)\n\t@mkdir -p artifacts/factory\n\tpython scripts/nfr_checks.py --output artifacts/factory/nfr_results.json\n\tpython scripts/nfr_checks.py\n\nfactory-local: ## Run one factory iteration locally (Gate 1 \u2192 Gate 2 \u2192 Gate 3 \u2192 feedback)\n\t@mkdir -p artifacts/factory\n\t@echo \"=== Gate 1: lint + typecheck + test ===\"\n\t@make lint 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tLINT_EXIT=$$?; \\\n\tmake typecheck 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tTYPE_EXIT=$$?; \\\n\tmake test 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tTEST_EXIT=$$?; \\\n\tif [ $$LINT_EXIT -ne 0 ] || [ $$TYPE_EXIT -ne 0 ] || [ $$TEST_EXIT -ne 0 ]; then \\\n\t\techo \"Gate 1 FAILED \u2014 skipping Gates 2-3\"; \\\n\t\techo '{\"total\":0,\"passed\":0,\"failed\":0,\"skipped\":0,\"satisfaction_score\":0.0,\"results\":[],\"timestamp\":\"N/A\",\"gate1_failed\":true}' > artifacts/factory/scenario_results.json; \\\n\telse \\\n\t\techo \"\"; \\\n\t\techo \"=== Gate 2: NFR checks (non-blocking) ===\"; \\\n\t\tmake nfr-check 2>&1 | tee -a artifacts/factory/ci_output.log || true; \\\n\t\techo \"\"; \\\n\t\techo \"=== Gate 3: Behavioral scenarios ===\"; \\\n\t\tpython scripts/run_scenarios.py --timeout 180 || true; \\\n\tfi\n\t@echo \"\"\n\t@echo \"=== Compiling feedback ===\"\n\t@python scripts/compile_feedback.py\n\t@echo \"\"\n\t@echo \"=== Factory iteration complete ===\"\n\t@make factory-status\n\nfactory-status: ## Show current iteration count and satisfaction score\n\t@echo \"--- Factory Status ---\"\n\t@if [ -f artifacts/factory/iteration_count.txt ]; then \\\n\t\techo \"Iteration: $$(cat artifacts/factory/iteration_count.txt)\"; \\\n\telse \\\n\t\techo \"Iteration: 0 (not started)\"; \\\n\tfi\n\t@if [ -f artifacts/factory/scenario_results.json ]; then \\\n\t\tpython -c \"import json; r=json.load(open('artifacts/factory/scenario_results.json')); print(f'Satisfaction: {r.get(\\\"satisfaction_score\\\", 0):.0%} ({r.get(\\\"passed\\\", 0)}/{r.get(\\\"total\\\", 0)} scenarios)')\"; \\\n\telse \\\n\t\techo \"Satisfaction: N/A (no results yet)\"; \\\n\tfi\n",
      "base": ".PHONY: lint test typecheck deps docker-build docker-smoke env-smoke train-smoke eval-smoke verify-learning dashboard validate run-scenarios compile-feedback factory-local factory-status\n\n# \u2500\u2500 Quality \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nlint:\n\truff check .\n\ntypecheck:\n\tmypy src\n\ntest:\n\tpytest -q\n\ndeps:\n\tpip-compile requirements.in\n\tpip-compile requirements-dev.in\n\tpip install -r requirements.txt -r requirements-dev.txt\n\n# \u2500\u2500 Environment \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nenv-smoke:\n\tpython -c \"from src.envs.minipong import MiniPongEnv; env=MiniPongEnv(); obs,_=env.reset(seed=0); assert obs.dtype.name=='uint8'; print(obs.shape)\"\n\n# \u2500\u2500 Training \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntrain-smoke:\n\tpython -m src.train.train_dqn --config configs/dqn_minipong.yaml --run-id smoke_run\n\neval-smoke:\n\tpython -m src.train.evaluate --run-id smoke_run --episodes 2 --seeds 1 2\n\nverify-learning:\n\tpython -m src.train.verify_learning --run-id smoke_run --min-return-gain -0.1 --min-hits-gain -0.1\n\n# \u2500\u2500 Dashboard \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndashboard:\n\tstreamlit run src/dashboard/app.py\n\n# \u2500\u2500 Docker \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndocker-build:\n\tdocker build -f infra/docker/Dockerfile.train --build-arg BASE_IMAGE=python:3.12-slim -t minipong-train .\n\tdocker build -f infra/docker/Dockerfile.demo -t minipong-demo .\n\ndocker-smoke:\n\tdocker run --rm minipong-train python -m src.train.train_dqn --help\n\tdocker run --rm minipong-demo python -m src.train.record_video --help\n\n# \u2500\u2500 Validation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nvalidate: lint typecheck test docker-build docker-smoke env-smoke\n\n# \u2500\u2500 Dark Factory \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nrun-scenarios: ## Run holdout scenario evaluation\n\tpython scripts/run_scenarios.py\n\ncompile-feedback: ## Compile validation results into feedback markdown\n\tpython scripts/compile_feedback.py\n\nfactory-local: ## Run one factory iteration locally (validate \u2192 scenarios \u2192 feedback)\n\t@echo \"=== Layer 1: lint + typecheck + test ===\"\n\t@make lint 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tLINT_EXIT=$$?; \\\n\tmake typecheck 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tTYPE_EXIT=$$?; \\\n\tmake test 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tTEST_EXIT=$$?; \\\n\tif [ $$LINT_EXIT -ne 0 ] || [ $$TYPE_EXIT -ne 0 ] || [ $$TEST_EXIT -ne 0 ]; then \\\n\t\techo \"Layer 1 FAILED \u2014 skipping scenarios\"; \\\n\t\techo '{\"total\":0,\"passed\":0,\"failed\":0,\"skipped\":0,\"satisfaction_score\":0.0,\"results\":[],\"timestamp\":\"N/A\",\"layer1_failed\":true}' > artifacts/factory/scenario_results.json; \\\n\telse \\\n\t\techo \"\"; \\\n\t\techo \"=== Layer 2: Behavioral scenarios ===\"; \\\n\t\tpython scripts/run_scenarios.py --timeout 180 || true; \\\n\tfi\n\t@echo \"\"\n\t@echo \"=== Compiling feedback ===\"\n\t@python scripts/compile_feedback.py\n\t@echo \"\"\n\t@echo \"=== Factory iteration complete ===\"\n\t@make factory-status\n\nfactory-status: ## Show current iteration count and satisfaction score\n\t@echo \"--- Factory Status ---\"\n\t@if [ -f artifacts/factory/iteration_count.txt ]; then \\\n\t\techo \"Iteration: $$(cat artifacts/factory/iteration_count.txt)\"; \\\n\telse \\\n\t\techo \"Iteration: 0 (not started)\"; \\\n\tfi\n\t@if [ -f artifacts/factory/scenario_results.json ]; then \\\n\t\tpython -c \"import json; r=json.load(open('artifacts/factory/scenario_results.json')); print(f'Satisfaction: {r.get(\\\"satisfaction_score\\\", 0):.0%} ({r.get(\\\"passed\\\", 0)}/{r.get(\\\"total\\\", 0)} scenarios)')\"; \\\n\telse \\\n\t\techo \"Satisfaction: N/A (no results yet)\"; \\\n\tfi\n"
    },
    "ProjectLeadAsks.md": {
      "additions": 71,
      "deletions": 31,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/ProjectLeadAsks.md b/ProjectLeadAsks.md\nindex 70bfa93..8abbab5 100644\n--- a/ProjectLeadAsks.md\n+++ b/ProjectLeadAsks.md\n@@ -1,41 +1,77 @@\n # Project Lead Asks\n \n-Items requiring Joey's input or decision.\n+Items requiring the project lead's input or decision.\n \n ## Open\n \n-### 1. OpenAI API Key for Codex in GitHub Actions\n-**Status:** Blocking for production factory runs (not blocking for local testing)\n+### 1. Satisfaction Threshold\n+**Status:** Decision needed before first factory crank\n+**Priority:** Pre-first-crank\n \n-The factory workflow needs `OPENAI_API_KEY` as a GitHub Actions secret. ChatGPT Plus ($20/mo) subscription does NOT bridge to the API \u2014 they're separate billing systems.\n-\n-**Action needed:**\n-- Check platform.openai.com for included API credits with your Plus account\n-- If none: add payment method on platform side (pay-as-you-go)\n-- Generate API key at platform.openai.com/api-keys\n-- Store it: `gh secret set OPENAI_API_KEY --repo joeyfezster/building_ai_w_ai`\n-\n-**Cost estimate:** ~$1-5 per factory iteration depending on context size.\n-\n-### 2. Satisfaction Threshold\n-**Default:** 80% (8/10 scenarios passing to declare convergence)\n+**Default:** 80% (10/12 scenarios passing to declare convergence)\n \n Is 80% the right starting point? Can be adjusted per workflow_dispatch run. Lower thresholds converge faster but accept more broken scenarios.\n \n-### 3. Codex Branch Strategy\n-**Current assumption:** Factory runs on `factory/*` branches created from the Codex seed branch.\n-\n-To kick off the first factory run:\n-1. Merge the factory infrastructure into the Codex branch (or create a new branch combining both)\n-2. Push to `factory/v1`\n-3. Trigger the workflow\n+### 2. Token Budget\n+**Status:** Non-blocking (current defaults are reasonable)\n+**Priority:** Post-first-crank optimization\n \n-### 4. Token Budget\n Codex context window determines how much the agent can read per iteration. Long feedback files + all specs could get expensive. Consider:\n - Capping feedback to last 2 iterations\n - Summarizing older feedback more aggressively\n \n-### 5. Factory Extraction \u2014 Phase 2\n+### 3. Factory Control Plane\n+**Status:** Design phase\n+**Priority:** Post-first-crank\n+\n+The factory has multiple dimensions requiring centralized control:\n+\n+| Dimension | Examples | Current State |\n+|-----------|----------|---------------|\n+| Models | Attractor model (codex-mini-latest), LLM-as-judge model | Hardcoded |\n+| Gate configs | Satisfaction threshold, NFR severity levels, timeout limits | Scattered in scripts |\n+| Iteration limits | Max iterations per crank, stall detection threshold | In workflow yaml |\n+| Branch policies | Naming conventions, auto-cleanup rules | In skill docs |\n+| Cost tracking | Token consumption per worker, per iteration | Not implemented |\n+\n+**Desired capabilities:**\n+- Dashboard showing all factory dimensions with current settings\n+- Token consumption per worker, per iteration, per time window (day/week)\n+- Ability for the human operator to swap models per worker (e.g., switch attractor from codex-mini to gpt-5)\n+- Gate configuration tuning without code changes\n+- Cost tracking and budget alerts\n+- Latency metrics per worker per iteration\n+\n+**Implementation options:**\n+1. **Streamlit page** \u2014 extend the existing dashboard with a \"Control Plane\" tab\n+2. **Config file** \u2014 `configs/factory_control.yaml` mapping all dimensions, read by all components\n+3. **Hybrid** \u2014 config file for settings, dashboard for visibility and tuning\n+\n+**Decision needed:** When to build, and how much observability is worth the infrastructure cost.\n+\n+## Roadmap\n+\n+### Pre-First-Crank (do before triggering the factory)\n+1. \u2705 factory/v1 branch created and pushed (factory infra + Codex code merged)\n+2. \u2705 Validation guidelines consolidated into attractor prompt\n+3. \u2705 agents/dev_team collected as factory-side reference assets\n+4. \u2705 Holdout isolation via branch stripping (`scripts/strip_holdout.py`, `scripts/restore_holdout.py`)\n+5. \u2705 Gate 2 NFR framework implemented (`scripts/nfr_checks.py` \u2014 code quality, complexity, dead code, security)\n+6. \u2705 Factory orchestration skill created (`.claude/skills/factory-orchestrate/SKILL.md`)\n+7. \u2705 Code quality standards codified (`docs/code_quality_standards.md`, linked from CLAUDE.md)\n+8. \u2705 Claude Code as orchestrator \u2014 replaces CI-only loop, browser automation for Codex\n+9. \ud83d\udd27 Satisfaction dashboard \u2014 visibility into scenario pass rates and convergence trajectory\n+10. \ud83d\udd27 Accept/merge gate \u2014 human interaction point for promoting factory output\n+11. \u23f3 Satisfaction threshold confirmed (#1 above)\n+\n+### Post-First-Crank (improve after first successful iteration)\n+1. LLM-as-judge \u2014 holistic evaluation beyond satisfaction score (built into orchestration skill)\n+2. Factory extraction to separate repo/package (see Phase 2 below)\n+3. Token budget optimization (#2 above)\n+4. Additional NFR checks \u2014 duplication, import hygiene, coverage, maintainability, reliability\n+5. Factory control plane \u2014 see #3 above\n+\n+### Phase 2: Factory Extraction\n **Status:** Planned after first successful convergence run\n \n The factory infrastructure is designed for extraction into its own repo (and eventually a reusable package). The separation is already native:\n@@ -57,14 +93,18 @@ The factory infrastructure is designed for extraction into its own repo (and eve\n 1. **Fork mode:** Move factory scripts + workflow to `joeyfezster/dark-factory`. Product repo's workflow calls factory repo via [reusable workflows](https://docs.github.com/en/actions/using-workflows/reusing-workflows) or checks it out as a step.\n 2. **Package mode:** Publish factory as a pip-installable CLI (`pip install dark-factory`). Product repo runs `dark-factory run-scenarios` instead of `python scripts/run_scenarios.py`. Workflow becomes a thin shell.\n \n-**What makes extraction easy today:**\n-- Scripts discover paths relative to repo root \u2014 no hardcoded project names\n-- Scenario format is a markdown convention, not code \u2014 any project can write them\n-- Feedback compiler is input-format-driven, not project-aware\n-- The workflow is already parameterized (max_iterations, satisfaction_threshold, target_branch)\n-\n **Decision point:** After the first successful convergence, evaluate whether to extract immediately or after a second project validates the pattern.\n \n ## Resolved\n \n-(none yet)\n+### \u2705 Codex Branch Strategy\n+**Resolved:** factory/v1 branch created from main, merged with Codex branch. Factory runs on `factory/*` and `df-crank-**` branches.\n+\n+### \u2705 Codex Billing / API Key\n+**Resolved:** No separate API key needed. Claude Code orchestrates via browser automation using Joey's existing ChatGPT Plus login (Chrome already authenticated). The billing \"hack\" is isolated in the orchestration skill's Codex invocation step \u2014 all other factory infrastructure (stripping, validation, feedback, PR creation) works identically regardless of how Codex is invoked. If/when an API key becomes available, only that one step changes.\n+\n+### \u2705 Scenario Isolation Architecture\n+**Resolved:** Branch stripping replaces the `/tmp/` filesystem shuffle. Deterministic scripts (`scripts/strip_holdout.py` and `scripts/restore_holdout.py`) remove and restore `/scenarios/` from the branch Codex works on. Scenarios literally don't exist on the attractor's branch \u2014 not hidden, not shuffled, physically absent. The strip script commits with marker `[factory:holdout-stripped]` and verifies no scenario files remain.\n+\n+### \u2705 Validation Guidelines in Attractor\n+**Resolved:** DoD, hard constraints, and quality checklist from agents/dev_team consolidated into `.github/codex/prompts/factory_fix.md`. Reduces iterations by giving Codex upfront quality expectations.\n",
      "raw": "# Project Lead Asks\n\nItems requiring the project lead's input or decision.\n\n## Open\n\n### 1. Satisfaction Threshold\n**Status:** Decision needed before first factory crank\n**Priority:** Pre-first-crank\n\n**Default:** 80% (10/12 scenarios passing to declare convergence)\n\nIs 80% the right starting point? Can be adjusted per workflow_dispatch run. Lower thresholds converge faster but accept more broken scenarios.\n\n### 2. Token Budget\n**Status:** Non-blocking (current defaults are reasonable)\n**Priority:** Post-first-crank optimization\n\nCodex context window determines how much the agent can read per iteration. Long feedback files + all specs could get expensive. Consider:\n- Capping feedback to last 2 iterations\n- Summarizing older feedback more aggressively\n\n### 3. Factory Control Plane\n**Status:** Design phase\n**Priority:** Post-first-crank\n\nThe factory has multiple dimensions requiring centralized control:\n\n| Dimension | Examples | Current State |\n|-----------|----------|---------------|\n| Models | Attractor model (codex-mini-latest), LLM-as-judge model | Hardcoded |\n| Gate configs | Satisfaction threshold, NFR severity levels, timeout limits | Scattered in scripts |\n| Iteration limits | Max iterations per crank, stall detection threshold | In workflow yaml |\n| Branch policies | Naming conventions, auto-cleanup rules | In skill docs |\n| Cost tracking | Token consumption per worker, per iteration | Not implemented |\n\n**Desired capabilities:**\n- Dashboard showing all factory dimensions with current settings\n- Token consumption per worker, per iteration, per time window (day/week)\n- Ability for the human operator to swap models per worker (e.g., switch attractor from codex-mini to gpt-5)\n- Gate configuration tuning without code changes\n- Cost tracking and budget alerts\n- Latency metrics per worker per iteration\n\n**Implementation options:**\n1. **Streamlit page** \u2014 extend the existing dashboard with a \"Control Plane\" tab\n2. **Config file** \u2014 `configs/factory_control.yaml` mapping all dimensions, read by all components\n3. **Hybrid** \u2014 config file for settings, dashboard for visibility and tuning\n\n**Decision needed:** When to build, and how much observability is worth the infrastructure cost.\n\n## Roadmap\n\n### Pre-First-Crank (do before triggering the factory)\n1. \u2705 factory/v1 branch created and pushed (factory infra + Codex code merged)\n2. \u2705 Validation guidelines consolidated into attractor prompt\n3. \u2705 agents/dev_team collected as factory-side reference assets\n4. \u2705 Holdout isolation via branch stripping (`scripts/strip_holdout.py`, `scripts/restore_holdout.py`)\n5. \u2705 Gate 2 NFR framework implemented (`scripts/nfr_checks.py` \u2014 code quality, complexity, dead code, security)\n6. \u2705 Factory orchestration skill created (`.claude/skills/factory-orchestrate/SKILL.md`)\n7. \u2705 Code quality standards codified (`docs/code_quality_standards.md`, linked from CLAUDE.md)\n8. \u2705 Claude Code as orchestrator \u2014 replaces CI-only loop, browser automation for Codex\n9. \ud83d\udd27 Satisfaction dashboard \u2014 visibility into scenario pass rates and convergence trajectory\n10. \ud83d\udd27 Accept/merge gate \u2014 human interaction point for promoting factory output\n11. \u23f3 Satisfaction threshold confirmed (#1 above)\n\n### Post-First-Crank (improve after first successful iteration)\n1. LLM-as-judge \u2014 holistic evaluation beyond satisfaction score (built into orchestration skill)\n2. Factory extraction to separate repo/package (see Phase 2 below)\n3. Token budget optimization (#2 above)\n4. Additional NFR checks \u2014 duplication, import hygiene, coverage, maintainability, reliability\n5. Factory control plane \u2014 see #3 above\n\n### Phase 2: Factory Extraction\n**Status:** Planned after first successful convergence run\n\nThe factory infrastructure is designed for extraction into its own repo (and eventually a reusable package). The separation is already native:\n\n**Factory-generic code** (extractable as-is):\n- `scripts/run_scenarios.py` \u2014 reads any `/scenarios/*.md`, zero project-specific logic\n- `scripts/compile_feedback.py` \u2014 reads any `scenario_results.json`, zero project-specific logic\n- `.github/workflows/factory.yaml` \u2014 parameterized via `workflow_dispatch` inputs\n- `.github/codex/prompts/factory_fix.md` \u2014 generic fix template, refs `/specs/` and `/scenarios/`\n- `docs/dark_factory.md` \u2014 generic operating manual\n\n**Project-specific content** (stays in product repo):\n- `/specs/*.md` \u2014 MiniPong-specific requirements\n- `/scenarios/*.md` \u2014 MiniPong-specific holdout evaluations\n- `CLAUDE.md` \u2014 MiniPong-specific context\n- Makefile factory targets \u2014 thin wrappers calling scripts\n\n**Extraction path (when ready):**\n1. **Fork mode:** Move factory scripts + workflow to `joeyfezster/dark-factory`. Product repo's workflow calls factory repo via [reusable workflows](https://docs.github.com/en/actions/using-workflows/reusing-workflows) or checks it out as a step.\n2. **Package mode:** Publish factory as a pip-installable CLI (`pip install dark-factory`). Product repo runs `dark-factory run-scenarios` instead of `python scripts/run_scenarios.py`. Workflow becomes a thin shell.\n\n**Decision point:** After the first successful convergence, evaluate whether to extract immediately or after a second project validates the pattern.\n\n## Resolved\n\n### \u2705 Codex Branch Strategy\n**Resolved:** factory/v1 branch created from main, merged with Codex branch. Factory runs on `factory/*` and `df-crank-**` branches.\n\n### \u2705 Codex Billing / API Key\n**Resolved:** No separate API key needed. Claude Code orchestrates via browser automation using Joey's existing ChatGPT Plus login (Chrome already authenticated). The billing \"hack\" is isolated in the orchestration skill's Codex invocation step \u2014 all other factory infrastructure (stripping, validation, feedback, PR creation) works identically regardless of how Codex is invoked. If/when an API key becomes available, only that one step changes.\n\n### \u2705 Scenario Isolation Architecture\n**Resolved:** Branch stripping replaces the `/tmp/` filesystem shuffle. Deterministic scripts (`scripts/strip_holdout.py` and `scripts/restore_holdout.py`) remove and restore `/scenarios/` from the branch Codex works on. Scenarios literally don't exist on the attractor's branch \u2014 not hidden, not shuffled, physically absent. The strip script commits with marker `[factory:holdout-stripped]` and verifies no scenario files remain.\n\n### \u2705 Validation Guidelines in Attractor\n**Resolved:** DoD, hard constraints, and quality checklist from agents/dev_team consolidated into `.github/codex/prompts/factory_fix.md`. Reduces iterations by giving Codex upfront quality expectations.\n",
      "base": "# Project Lead Asks\n\nItems requiring Joey's input or decision.\n\n## Open\n\n### 1. OpenAI API Key for Codex in GitHub Actions\n**Status:** Blocking for production factory runs (not blocking for local testing)\n\nThe factory workflow needs `OPENAI_API_KEY` as a GitHub Actions secret. ChatGPT Plus ($20/mo) subscription does NOT bridge to the API \u2014 they're separate billing systems.\n\n**Action needed:**\n- Check platform.openai.com for included API credits with your Plus account\n- If none: add payment method on platform side (pay-as-you-go)\n- Generate API key at platform.openai.com/api-keys\n- Store it: `gh secret set OPENAI_API_KEY --repo joeyfezster/building_ai_w_ai`\n\n**Cost estimate:** ~$1-5 per factory iteration depending on context size.\n\n### 2. Satisfaction Threshold\n**Default:** 80% (8/10 scenarios passing to declare convergence)\n\nIs 80% the right starting point? Can be adjusted per workflow_dispatch run. Lower thresholds converge faster but accept more broken scenarios.\n\n### 3. Codex Branch Strategy\n**Current assumption:** Factory runs on `factory/*` branches created from the Codex seed branch.\n\nTo kick off the first factory run:\n1. Merge the factory infrastructure into the Codex branch (or create a new branch combining both)\n2. Push to `factory/v1`\n3. Trigger the workflow\n\n### 4. Token Budget\nCodex context window determines how much the agent can read per iteration. Long feedback files + all specs could get expensive. Consider:\n- Capping feedback to last 2 iterations\n- Summarizing older feedback more aggressively\n\n### 5. Factory Extraction \u2014 Phase 2\n**Status:** Planned after first successful convergence run\n\nThe factory infrastructure is designed for extraction into its own repo (and eventually a reusable package). The separation is already native:\n\n**Factory-generic code** (extractable as-is):\n- `scripts/run_scenarios.py` \u2014 reads any `/scenarios/*.md`, zero project-specific logic\n- `scripts/compile_feedback.py` \u2014 reads any `scenario_results.json`, zero project-specific logic\n- `.github/workflows/factory.yaml` \u2014 parameterized via `workflow_dispatch` inputs\n- `.github/codex/prompts/factory_fix.md` \u2014 generic fix template, refs `/specs/` and `/scenarios/`\n- `docs/dark_factory.md` \u2014 generic operating manual\n\n**Project-specific content** (stays in product repo):\n- `/specs/*.md` \u2014 MiniPong-specific requirements\n- `/scenarios/*.md` \u2014 MiniPong-specific holdout evaluations\n- `CLAUDE.md` \u2014 MiniPong-specific context\n- Makefile factory targets \u2014 thin wrappers calling scripts\n\n**Extraction path (when ready):**\n1. **Fork mode:** Move factory scripts + workflow to `joeyfezster/dark-factory`. Product repo's workflow calls factory repo via [reusable workflows](https://docs.github.com/en/actions/using-workflows/reusing-workflows) or checks it out as a step.\n2. **Package mode:** Publish factory as a pip-installable CLI (`pip install dark-factory`). Product repo runs `dark-factory run-scenarios` instead of `python scripts/run_scenarios.py`. Workflow becomes a thin shell.\n\n**What makes extraction easy today:**\n- Scripts discover paths relative to repo root \u2014 no hardcoded project names\n- Scenario format is a markdown convention, not code \u2014 any project can write them\n- Feedback compiler is input-format-driven, not project-aware\n- The workflow is already parameterized (max_iterations, satisfaction_threshold, target_branch)\n\n**Decision point:** After the first successful convergence, evaluate whether to extract immediately or after a second project validates the pattern.\n\n## Resolved\n\n(none yet)\n"
    },
    "README.md": {
      "additions": 49,
      "deletions": 6,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/README.md b/README.md\nindex 782a031..08bbc46 100644\n--- a/README.md\n+++ b/README.md\n@@ -1,8 +1,51 @@\n-# Retro RL Milestones\n+# Retro RL Milestones: MiniPong + DQN\n \n-Recreating classic AI milestones by training play-agents on 80s-style games from pixels + controls.\n+This repo provides a complete open-source proof that a reinforcement learning agent can learn a Pong-like game using only pixels and discrete controls.\n \n-## TODO\n-- Document project goals and roadmap.\n-- Add setup instructions.\n-- Link to demo packs and whitepapers.\n+## Quickstart\n+\n+```bash\n+make deps\n+make train-smoke\n+make eval-smoke\n+make verify-learning\n+make dashboard\n+```\n+\n+## Artifacts\n+\n+Training and evaluation artifacts are written to:\n+\n+- `artifacts/<run_id>/logs.jsonl`\n+- `artifacts/<run_id>/tensorboard/`\n+- `artifacts/<run_id>/checkpoints/`\n+- `artifacts/<run_id>/eval/`\n+- `artifacts/<run_id>/videos/`\n+- `artifacts/<run_id>/demo/index.html`\n+\n+## Validation\n+\n+**Intention:** prove that MiniPong learning happens from pixels-only observations and controls, with quantitative and qualitative artifacts.\n+\n+**Sanity checks (no GPU required):**\n+- `make validate`\n+- `make train-smoke`\n+- `make eval-smoke`\n+\n+**Emulator correctness:**\n+- `make env-smoke`\n+- deterministic physics tests in `tests/test_env_minipong_determinism.py`\n+\n+**Whitepapers integrity:**\n+- `make whitepapers-acquire`\n+- `make whitepapers-verify`\n+\n+**Packaging:**\n+- `make docker-build`\n+- `make docker-smoke`\n+\n+**Learning proof:**\n+- `make verify-learning`\n+- progression videos: `artifacts/<run_id>/videos/`\n+- montage/index: `artifacts/<run_id>/demo/index.html`\n+- dashboard data source: `artifacts/<run_id>/logs.jsonl` and `artifacts/<run_id>/eval/*.json`\n",
      "raw": "# Retro RL Milestones: MiniPong + DQN\n\nThis repo provides a complete open-source proof that a reinforcement learning agent can learn a Pong-like game using only pixels and discrete controls.\n\n## Quickstart\n\n```bash\nmake deps\nmake train-smoke\nmake eval-smoke\nmake verify-learning\nmake dashboard\n```\n\n## Artifacts\n\nTraining and evaluation artifacts are written to:\n\n- `artifacts/<run_id>/logs.jsonl`\n- `artifacts/<run_id>/tensorboard/`\n- `artifacts/<run_id>/checkpoints/`\n- `artifacts/<run_id>/eval/`\n- `artifacts/<run_id>/videos/`\n- `artifacts/<run_id>/demo/index.html`\n\n## Validation\n\n**Intention:** prove that MiniPong learning happens from pixels-only observations and controls, with quantitative and qualitative artifacts.\n\n**Sanity checks (no GPU required):**\n- `make validate`\n- `make train-smoke`\n- `make eval-smoke`\n\n**Emulator correctness:**\n- `make env-smoke`\n- deterministic physics tests in `tests/test_env_minipong_determinism.py`\n\n**Whitepapers integrity:**\n- `make whitepapers-acquire`\n- `make whitepapers-verify`\n\n**Packaging:**\n- `make docker-build`\n- `make docker-smoke`\n\n**Learning proof:**\n- `make verify-learning`\n- progression videos: `artifacts/<run_id>/videos/`\n- montage/index: `artifacts/<run_id>/demo/index.html`\n- dashboard data source: `artifacts/<run_id>/logs.jsonl` and `artifacts/<run_id>/eval/*.json`\n",
      "base": "# Retro RL Milestones\n\nRecreating classic AI milestones by training play-agents on 80s-style games from pixels + controls.\n\n## TODO\n- Document project goals and roadmap.\n- Add setup instructions.\n- Link to demo packs and whitepapers.\n"
    },
    "agents/README.md": {
      "additions": 14,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/agents/README.md b/agents/README.md\nindex 01d8297..11784f9 100644\n--- a/agents/README.md\n+++ b/agents/README.md\n@@ -1,6 +1,15 @@\n-# Dev-Team Agents\n+# Agent Definitions (Pre-Factory Reference)\n \n-## TODO\n-- Describe dev-team agent roles.\n-- Explain how to use workflows.\n-- Add onboarding instructions.\n+These agent role definitions were created before the dark factory pattern was adopted. In the factory model, a single **Attractor** (Codex) handles all coding \u2014 there is no multi-agent team.\n+\n+## What's Here\n+- `dev_team/` \u2014 Role specs (architect, builder, reviewer, etc.) from the original multi-agent design\n+- `workflows/` \u2014 Collaboration and PR templates from the team-based approach\n+\n+## How They're Used Now\n+The useful content from these files \u2014 validation guidelines, Definition of Done, hard constraints, PR checklists \u2014 has been consolidated into the Attractor's prompt template at `.github/codex/prompts/factory_fix.md`.\n+\n+These files are kept as reference for the factory's design evolution. They are **not read by Codex** during factory iterations.\n+\n+## Factory-Side Asset\n+This directory is factory infrastructure, not product code. It should NOT be modified by the Attractor.\n",
      "raw": "# Agent Definitions (Pre-Factory Reference)\n\nThese agent role definitions were created before the dark factory pattern was adopted. In the factory model, a single **Attractor** (Codex) handles all coding \u2014 there is no multi-agent team.\n\n## What's Here\n- `dev_team/` \u2014 Role specs (architect, builder, reviewer, etc.) from the original multi-agent design\n- `workflows/` \u2014 Collaboration and PR templates from the team-based approach\n\n## How They're Used Now\nThe useful content from these files \u2014 validation guidelines, Definition of Done, hard constraints, PR checklists \u2014 has been consolidated into the Attractor's prompt template at `.github/codex/prompts/factory_fix.md`.\n\nThese files are kept as reference for the factory's design evolution. They are **not read by Codex** during factory iterations.\n\n## Factory-Side Asset\nThis directory is factory infrastructure, not product code. It should NOT be modified by the Attractor.\n",
      "base": "# Dev-Team Agents\n\n## TODO\n- Describe dev-team agent roles.\n- Explain how to use workflows.\n- Add onboarding instructions.\n"
    },
    "agents/dev_team/architect.md": {
      "additions": 18,
      "deletions": 7,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/agents/dev_team/architect.md b/agents/dev_team/architect.md\nindex b3d3540..ba61d42 100644\n--- a/agents/dev_team/architect.md\n+++ b/agents/dev_team/architect.md\n@@ -1,19 +1,30 @@\n # Agent Instructions\n \n ## Mission\n-- TODO\n+Deliver production-ready contributions for MiniPong RL milestone delivery.\n \n ## Inputs\n-- TODO\n+- Product requirements and milestone docs\n+- Existing source code, tests, and artifacts\n \n ## Outputs\n-- TODO\n+- Code changes with tests\n+- Updated docs and validation evidence\n \n ## Definition of Done (DoD)\n-- TODO\n+- Functional requirements implemented\n+- Architectural consistency maintained\n+- Integration checks pass\n+- Required artifacts generated and linked\n \n-## Constraints\n-- TODO\n+## Hard Constraints\n+- No proprietary ROM dependencies\n+- Policy consumes pixels only\n+- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n \n ## PR Checklist\n-- TODO\n+- [ ] Scope is complete\n+- [ ] Tests added/updated\n+- [ ] `make validate` passed\n+- [ ] `make verify-learning` passed (if training-related)\n+- [ ] Artifact paths documented\n",
      "raw": "# Agent Instructions\n\n## Mission\nDeliver production-ready contributions for MiniPong RL milestone delivery.\n\n## Inputs\n- Product requirements and milestone docs\n- Existing source code, tests, and artifacts\n\n## Outputs\n- Code changes with tests\n- Updated docs and validation evidence\n\n## Definition of Done (DoD)\n- Functional requirements implemented\n- Architectural consistency maintained\n- Integration checks pass\n- Required artifacts generated and linked\n\n## Hard Constraints\n- No proprietary ROM dependencies\n- Policy consumes pixels only\n- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n\n## PR Checklist\n- [ ] Scope is complete\n- [ ] Tests added/updated\n- [ ] `make validate` passed\n- [ ] `make verify-learning` passed (if training-related)\n- [ ] Artifact paths documented\n",
      "base": "# Agent Instructions\n\n## Mission\n- TODO\n\n## Inputs\n- TODO\n\n## Outputs\n- TODO\n\n## Definition of Done (DoD)\n- TODO\n\n## Constraints\n- TODO\n\n## PR Checklist\n- TODO\n"
    },
    "agents/dev_team/builder.md": {
      "additions": 18,
      "deletions": 7,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/agents/dev_team/builder.md b/agents/dev_team/builder.md\nindex b3d3540..ba61d42 100644\n--- a/agents/dev_team/builder.md\n+++ b/agents/dev_team/builder.md\n@@ -1,19 +1,30 @@\n # Agent Instructions\n \n ## Mission\n-- TODO\n+Deliver production-ready contributions for MiniPong RL milestone delivery.\n \n ## Inputs\n-- TODO\n+- Product requirements and milestone docs\n+- Existing source code, tests, and artifacts\n \n ## Outputs\n-- TODO\n+- Code changes with tests\n+- Updated docs and validation evidence\n \n ## Definition of Done (DoD)\n-- TODO\n+- Functional requirements implemented\n+- Architectural consistency maintained\n+- Integration checks pass\n+- Required artifacts generated and linked\n \n-## Constraints\n-- TODO\n+## Hard Constraints\n+- No proprietary ROM dependencies\n+- Policy consumes pixels only\n+- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n \n ## PR Checklist\n-- TODO\n+- [ ] Scope is complete\n+- [ ] Tests added/updated\n+- [ ] `make validate` passed\n+- [ ] `make verify-learning` passed (if training-related)\n+- [ ] Artifact paths documented\n",
      "raw": "# Agent Instructions\n\n## Mission\nDeliver production-ready contributions for MiniPong RL milestone delivery.\n\n## Inputs\n- Product requirements and milestone docs\n- Existing source code, tests, and artifacts\n\n## Outputs\n- Code changes with tests\n- Updated docs and validation evidence\n\n## Definition of Done (DoD)\n- Functional requirements implemented\n- Architectural consistency maintained\n- Integration checks pass\n- Required artifacts generated and linked\n\n## Hard Constraints\n- No proprietary ROM dependencies\n- Policy consumes pixels only\n- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n\n## PR Checklist\n- [ ] Scope is complete\n- [ ] Tests added/updated\n- [ ] `make validate` passed\n- [ ] `make verify-learning` passed (if training-related)\n- [ ] Artifact paths documented\n",
      "base": "# Agent Instructions\n\n## Mission\n- TODO\n\n## Inputs\n- TODO\n\n## Outputs\n- TODO\n\n## Definition of Done (DoD)\n- TODO\n\n## Constraints\n- TODO\n\n## PR Checklist\n- TODO\n"
    },
    "agents/dev_team/demo_producer.md": {
      "additions": 18,
      "deletions": 7,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/agents/dev_team/demo_producer.md b/agents/dev_team/demo_producer.md\nindex b3d3540..ba61d42 100644\n--- a/agents/dev_team/demo_producer.md\n+++ b/agents/dev_team/demo_producer.md\n@@ -1,19 +1,30 @@\n # Agent Instructions\n \n ## Mission\n-- TODO\n+Deliver production-ready contributions for MiniPong RL milestone delivery.\n \n ## Inputs\n-- TODO\n+- Product requirements and milestone docs\n+- Existing source code, tests, and artifacts\n \n ## Outputs\n-- TODO\n+- Code changes with tests\n+- Updated docs and validation evidence\n \n ## Definition of Done (DoD)\n-- TODO\n+- Functional requirements implemented\n+- Architectural consistency maintained\n+- Integration checks pass\n+- Required artifacts generated and linked\n \n-## Constraints\n-- TODO\n+## Hard Constraints\n+- No proprietary ROM dependencies\n+- Policy consumes pixels only\n+- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n \n ## PR Checklist\n-- TODO\n+- [ ] Scope is complete\n+- [ ] Tests added/updated\n+- [ ] `make validate` passed\n+- [ ] `make verify-learning` passed (if training-related)\n+- [ ] Artifact paths documented\n",
      "raw": "# Agent Instructions\n\n## Mission\nDeliver production-ready contributions for MiniPong RL milestone delivery.\n\n## Inputs\n- Product requirements and milestone docs\n- Existing source code, tests, and artifacts\n\n## Outputs\n- Code changes with tests\n- Updated docs and validation evidence\n\n## Definition of Done (DoD)\n- Functional requirements implemented\n- Architectural consistency maintained\n- Integration checks pass\n- Required artifacts generated and linked\n\n## Hard Constraints\n- No proprietary ROM dependencies\n- Policy consumes pixels only\n- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n\n## PR Checklist\n- [ ] Scope is complete\n- [ ] Tests added/updated\n- [ ] `make validate` passed\n- [ ] `make verify-learning` passed (if training-related)\n- [ ] Artifact paths documented\n",
      "base": "# Agent Instructions\n\n## Mission\n- TODO\n\n## Inputs\n- TODO\n\n## Outputs\n- TODO\n\n## Definition of Done (DoD)\n- TODO\n\n## Constraints\n- TODO\n\n## PR Checklist\n- TODO\n"
    },
    "agents/dev_team/devops_ci.md": {
      "additions": 18,
      "deletions": 7,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/agents/dev_team/devops_ci.md b/agents/dev_team/devops_ci.md\nindex b3d3540..ba61d42 100644\n--- a/agents/dev_team/devops_ci.md\n+++ b/agents/dev_team/devops_ci.md\n@@ -1,19 +1,30 @@\n # Agent Instructions\n \n ## Mission\n-- TODO\n+Deliver production-ready contributions for MiniPong RL milestone delivery.\n \n ## Inputs\n-- TODO\n+- Product requirements and milestone docs\n+- Existing source code, tests, and artifacts\n \n ## Outputs\n-- TODO\n+- Code changes with tests\n+- Updated docs and validation evidence\n \n ## Definition of Done (DoD)\n-- TODO\n+- Functional requirements implemented\n+- Architectural consistency maintained\n+- Integration checks pass\n+- Required artifacts generated and linked\n \n-## Constraints\n-- TODO\n+## Hard Constraints\n+- No proprietary ROM dependencies\n+- Policy consumes pixels only\n+- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n \n ## PR Checklist\n-- TODO\n+- [ ] Scope is complete\n+- [ ] Tests added/updated\n+- [ ] `make validate` passed\n+- [ ] `make verify-learning` passed (if training-related)\n+- [ ] Artifact paths documented\n",
      "raw": "# Agent Instructions\n\n## Mission\nDeliver production-ready contributions for MiniPong RL milestone delivery.\n\n## Inputs\n- Product requirements and milestone docs\n- Existing source code, tests, and artifacts\n\n## Outputs\n- Code changes with tests\n- Updated docs and validation evidence\n\n## Definition of Done (DoD)\n- Functional requirements implemented\n- Architectural consistency maintained\n- Integration checks pass\n- Required artifacts generated and linked\n\n## Hard Constraints\n- No proprietary ROM dependencies\n- Policy consumes pixels only\n- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n\n## PR Checklist\n- [ ] Scope is complete\n- [ ] Tests added/updated\n- [ ] `make validate` passed\n- [ ] `make verify-learning` passed (if training-related)\n- [ ] Artifact paths documented\n",
      "base": "# Agent Instructions\n\n## Mission\n- TODO\n\n## Inputs\n- TODO\n\n## Outputs\n- TODO\n\n## Definition of Done (DoD)\n- TODO\n\n## Constraints\n- TODO\n\n## PR Checklist\n- TODO\n"
    },
    "agents/dev_team/pack_writer.md": {
      "additions": 18,
      "deletions": 7,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/agents/dev_team/pack_writer.md b/agents/dev_team/pack_writer.md\nindex b3d3540..ba61d42 100644\n--- a/agents/dev_team/pack_writer.md\n+++ b/agents/dev_team/pack_writer.md\n@@ -1,19 +1,30 @@\n # Agent Instructions\n \n ## Mission\n-- TODO\n+Deliver production-ready contributions for MiniPong RL milestone delivery.\n \n ## Inputs\n-- TODO\n+- Product requirements and milestone docs\n+- Existing source code, tests, and artifacts\n \n ## Outputs\n-- TODO\n+- Code changes with tests\n+- Updated docs and validation evidence\n \n ## Definition of Done (DoD)\n-- TODO\n+- Functional requirements implemented\n+- Architectural consistency maintained\n+- Integration checks pass\n+- Required artifacts generated and linked\n \n-## Constraints\n-- TODO\n+## Hard Constraints\n+- No proprietary ROM dependencies\n+- Policy consumes pixels only\n+- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n \n ## PR Checklist\n-- TODO\n+- [ ] Scope is complete\n+- [ ] Tests added/updated\n+- [ ] `make validate` passed\n+- [ ] `make verify-learning` passed (if training-related)\n+- [ ] Artifact paths documented\n",
      "raw": "# Agent Instructions\n\n## Mission\nDeliver production-ready contributions for MiniPong RL milestone delivery.\n\n## Inputs\n- Product requirements and milestone docs\n- Existing source code, tests, and artifacts\n\n## Outputs\n- Code changes with tests\n- Updated docs and validation evidence\n\n## Definition of Done (DoD)\n- Functional requirements implemented\n- Architectural consistency maintained\n- Integration checks pass\n- Required artifacts generated and linked\n\n## Hard Constraints\n- No proprietary ROM dependencies\n- Policy consumes pixels only\n- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n\n## PR Checklist\n- [ ] Scope is complete\n- [ ] Tests added/updated\n- [ ] `make validate` passed\n- [ ] `make verify-learning` passed (if training-related)\n- [ ] Artifact paths documented\n",
      "base": "# Agent Instructions\n\n## Mission\n- TODO\n\n## Inputs\n- TODO\n\n## Outputs\n- TODO\n\n## Definition of Done (DoD)\n- TODO\n\n## Constraints\n- TODO\n\n## PR Checklist\n- TODO\n"
    },
    "agents/dev_team/reviewer.md": {
      "additions": 18,
      "deletions": 7,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/agents/dev_team/reviewer.md b/agents/dev_team/reviewer.md\nindex b3d3540..ba61d42 100644\n--- a/agents/dev_team/reviewer.md\n+++ b/agents/dev_team/reviewer.md\n@@ -1,19 +1,30 @@\n # Agent Instructions\n \n ## Mission\n-- TODO\n+Deliver production-ready contributions for MiniPong RL milestone delivery.\n \n ## Inputs\n-- TODO\n+- Product requirements and milestone docs\n+- Existing source code, tests, and artifacts\n \n ## Outputs\n-- TODO\n+- Code changes with tests\n+- Updated docs and validation evidence\n \n ## Definition of Done (DoD)\n-- TODO\n+- Functional requirements implemented\n+- Architectural consistency maintained\n+- Integration checks pass\n+- Required artifacts generated and linked\n \n-## Constraints\n-- TODO\n+## Hard Constraints\n+- No proprietary ROM dependencies\n+- Policy consumes pixels only\n+- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n \n ## PR Checklist\n-- TODO\n+- [ ] Scope is complete\n+- [ ] Tests added/updated\n+- [ ] `make validate` passed\n+- [ ] `make verify-learning` passed (if training-related)\n+- [ ] Artifact paths documented\n",
      "raw": "# Agent Instructions\n\n## Mission\nDeliver production-ready contributions for MiniPong RL milestone delivery.\n\n## Inputs\n- Product requirements and milestone docs\n- Existing source code, tests, and artifacts\n\n## Outputs\n- Code changes with tests\n- Updated docs and validation evidence\n\n## Definition of Done (DoD)\n- Functional requirements implemented\n- Architectural consistency maintained\n- Integration checks pass\n- Required artifacts generated and linked\n\n## Hard Constraints\n- No proprietary ROM dependencies\n- Policy consumes pixels only\n- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n\n## PR Checklist\n- [ ] Scope is complete\n- [ ] Tests added/updated\n- [ ] `make validate` passed\n- [ ] `make verify-learning` passed (if training-related)\n- [ ] Artifact paths documented\n",
      "base": "# Agent Instructions\n\n## Mission\n- TODO\n\n## Inputs\n- TODO\n\n## Outputs\n- TODO\n\n## Definition of Done (DoD)\n- TODO\n\n## Constraints\n- TODO\n\n## PR Checklist\n- TODO\n"
    },
    "agents/dev_team/rl_scientist.md": {
      "additions": 18,
      "deletions": 7,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/agents/dev_team/rl_scientist.md b/agents/dev_team/rl_scientist.md\nindex b3d3540..ba61d42 100644\n--- a/agents/dev_team/rl_scientist.md\n+++ b/agents/dev_team/rl_scientist.md\n@@ -1,19 +1,30 @@\n # Agent Instructions\n \n ## Mission\n-- TODO\n+Deliver production-ready contributions for MiniPong RL milestone delivery.\n \n ## Inputs\n-- TODO\n+- Product requirements and milestone docs\n+- Existing source code, tests, and artifacts\n \n ## Outputs\n-- TODO\n+- Code changes with tests\n+- Updated docs and validation evidence\n \n ## Definition of Done (DoD)\n-- TODO\n+- Functional requirements implemented\n+- Architectural consistency maintained\n+- Integration checks pass\n+- Required artifacts generated and linked\n \n-## Constraints\n-- TODO\n+## Hard Constraints\n+- No proprietary ROM dependencies\n+- Policy consumes pixels only\n+- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n \n ## PR Checklist\n-- TODO\n+- [ ] Scope is complete\n+- [ ] Tests added/updated\n+- [ ] `make validate` passed\n+- [ ] `make verify-learning` passed (if training-related)\n+- [ ] Artifact paths documented\n",
      "raw": "# Agent Instructions\n\n## Mission\nDeliver production-ready contributions for MiniPong RL milestone delivery.\n\n## Inputs\n- Product requirements and milestone docs\n- Existing source code, tests, and artifacts\n\n## Outputs\n- Code changes with tests\n- Updated docs and validation evidence\n\n## Definition of Done (DoD)\n- Functional requirements implemented\n- Architectural consistency maintained\n- Integration checks pass\n- Required artifacts generated and linked\n\n## Hard Constraints\n- No proprietary ROM dependencies\n- Policy consumes pixels only\n- No PR is ready unless `make validate` and `make verify-learning` (where applicable) pass and outputs are attached as artifacts.\n\n## PR Checklist\n- [ ] Scope is complete\n- [ ] Tests added/updated\n- [ ] `make validate` passed\n- [ ] `make verify-learning` passed (if training-related)\n- [ ] Artifact paths documented\n",
      "base": "# Agent Instructions\n\n## Mission\n- TODO\n\n## Inputs\n- TODO\n\n## Outputs\n- TODO\n\n## Definition of Done (DoD)\n- TODO\n\n## Constraints\n- TODO\n\n## PR Checklist\n- TODO\n"
    },
    "agents/workflows/collaboration.md": {
      "additions": 5,
      "deletions": 4,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/agents/workflows/collaboration.md b/agents/workflows/collaboration.md\nindex 3663f8a..bbc8c49 100644\n--- a/agents/workflows/collaboration.md\n+++ b/agents/workflows/collaboration.md\n@@ -1,6 +1,7 @@\n # Collaboration Workflow\n \n-## TODO\n-- Define handoffs between agents.\n-- Define sync cadence.\n-- Define escalation path.\n+1. Architect proposes boundaries and acceptance criteria.\n+2. Builder/rl_scientist implement features with tests.\n+3. DevOps agent ensures CI and container workflow.\n+4. Reviewer validates code quality and reproducibility.\n+5. Demo producer packages proof artifacts.\n",
      "raw": "# Collaboration Workflow\n\n1. Architect proposes boundaries and acceptance criteria.\n2. Builder/rl_scientist implement features with tests.\n3. DevOps agent ensures CI and container workflow.\n4. Reviewer validates code quality and reproducibility.\n5. Demo producer packages proof artifacts.\n",
      "base": "# Collaboration Workflow\n\n## TODO\n- Define handoffs between agents.\n- Define sync cadence.\n- Define escalation path.\n"
    },
    "agents/workflows/pr_template.md": {
      "additions": 22,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/agents/workflows/pr_template.md b/agents/workflows/pr_template.md\nindex a852c2a..87464ae 100644\n--- a/agents/workflows/pr_template.md\n+++ b/agents/workflows/pr_template.md\n@@ -1,6 +1,23 @@\n-# PR Template\n+# Pull Request Template\n \n-## TODO\n-- Add summary section.\n-- Add testing checklist.\n-- Add risk/rollout section.\n+## Summary\n+- [ ] Describe what changed and why\n+\n+## Validation Checklist\n+- [ ] `make lint`\n+- [ ] `make typecheck`\n+- [ ] `make test`\n+- [ ] `make validate`\n+- [ ] `make verify-learning` (where applicable)\n+\n+## Artifact Checklist\n+- [ ] Metrics files attached\n+- [ ] Video outputs attached\n+- [ ] Dashboard data validated\n+\n+## Command Transcripts\n+Paste terminal output snippets for each required command.\n+\n+## Risks / Rollout\n+- [ ] Risks identified\n+- [ ] Rollback plan included\n",
      "raw": "# Pull Request Template\n\n## Summary\n- [ ] Describe what changed and why\n\n## Validation Checklist\n- [ ] `make lint`\n- [ ] `make typecheck`\n- [ ] `make test`\n- [ ] `make validate`\n- [ ] `make verify-learning` (where applicable)\n\n## Artifact Checklist\n- [ ] Metrics files attached\n- [ ] Video outputs attached\n- [ ] Dashboard data validated\n\n## Command Transcripts\nPaste terminal output snippets for each required command.\n\n## Risks / Rollout\n- [ ] Risks identified\n- [ ] Rollback plan included\n",
      "base": "# PR Template\n\n## TODO\n- Add summary section.\n- Add testing checklist.\n- Add risk/rollout section.\n"
    },
    "artifacts/factory/feedback_iter_1.md": {
      "additions": 115,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/artifacts/factory/feedback_iter_1.md b/artifacts/factory/feedback_iter_1.md\nnew file mode 100644\nindex 0000000..8444ac1\n--- /dev/null\n+++ b/artifacts/factory/feedback_iter_1.md\n@@ -0,0 +1,115 @@\n+# Factory Feedback \u2014 Iteration 1\n+Generated: 2026-02-23 01:42:47 UTC\n+\n+## Summary\n+- **Satisfaction score: 42%** (5/12 scenarios passed)\n+- Passed: 5 | Failed: 7 | Total: 12\n+\n+## Likely Root Causes\n+1. Import errors in 5 scenario(s): Environment Determinism, Environment Observation Space, Environment Reward Structure, Environment Rendering, Environment Info Dict Completeness. Likely missing module or wrong import path.\n+2. Assertion failures in 2 scenario(s): Training Produces Required Artifacts, Evaluation Produces Videos. Check the specific assertion messages.\n+\n+## Failed Scenarios \u2014 Full Details\n+\n+### Environment Determinism\n+**Category:** environment\n+**Exit code:** 1\n+**Duration:** 0.02s\n+**Error summary:** ModuleNotFoundError: No module named 'src.envs.minipong'\n+\n+**stderr:**\n+```\n+Traceback (most recent call last):\n+  File \"<string>\", line 2, in <module>\n+ModuleNotFoundError: No module named 'src.envs.minipong'\n+```\n+\n+### Environment Observation Space\n+**Category:** environment\n+**Exit code:** 1\n+**Duration:** 0.02s\n+**Error summary:** ModuleNotFoundError: No module named 'src.envs.minipong'\n+\n+**stderr:**\n+```\n+Traceback (most recent call last):\n+  File \"<string>\", line 2, in <module>\n+ModuleNotFoundError: No module named 'src.envs.minipong'\n+```\n+\n+### Environment Reward Structure\n+**Category:** environment\n+**Exit code:** 1\n+**Duration:** 0.02s\n+**Error summary:** ModuleNotFoundError: No module named 'src.envs.minipong'\n+\n+**stderr:**\n+```\n+Traceback (most recent call last):\n+  File \"<string>\", line 2, in <module>\n+ModuleNotFoundError: No module named 'src.envs.minipong'\n+```\n+\n+### Environment Rendering\n+**Category:** environment\n+**Exit code:** 1\n+**Duration:** 0.02s\n+**Error summary:** ModuleNotFoundError: No module named 'src.envs.minipong'\n+\n+**stderr:**\n+```\n+Traceback (most recent call last):\n+  File \"<string>\", line 2, in <module>\n+ModuleNotFoundError: No module named 'src.envs.minipong'\n+```\n+\n+### Environment Info Dict Completeness\n+**Category:** environment\n+**Exit code:** 1\n+**Duration:** 0.02s\n+**Error summary:** ModuleNotFoundError: No module named 'src.envs.minipong'\n+\n+**stderr:**\n+```\n+Traceback (most recent call last):\n+  File \"<string>\", line 2, in <module>\n+ModuleNotFoundError: No module named 'src.envs.minipong'\n+```\n+\n+### Training Produces Required Artifacts\n+**Category:** training\n+**Exit code:** 1\n+**Duration:** 0.03s\n+**Error summary:** AssertionError: Run directory not created: artifacts/scenario_test\n+\n+**stderr:**\n+```\n+Traceback (most recent call last):\n+  File \"<string>\", line 5, in <module>\n+AssertionError: Run directory not created: artifacts/scenario_test\n+```\n+\n+### Evaluation Produces Videos\n+**Category:** training\n+**Exit code:** 1\n+**Duration:** 0.03s\n+**Error summary:** AssertionError: No video files produced in artifacts/scenario_video_test/videos/\n+\n+**stderr:**\n+```\n+Traceback (most recent call last):\n+  File \"<string>\", line 6, in <module>\n+AssertionError: No video files produced in artifacts/scenario_video_test/videos/\n+```\n+\n+## Instructions for Coding Agent\n+\n+Fix the failures above. Priorities:\n+1. Import errors and missing modules first\n+2. File/artifact production issues next\n+3. Behavioral assertion failures last\n+\n+Constraints:\n+- Do NOT modify /scenarios/, /scripts/, or /.github/workflows/factory.yaml\n+- Do NOT modify /specs/ \u2014 read them as requirements\n+- Keep changes minimal \u2014 fix what's broken, don't refactor\n",
      "raw": "# Factory Feedback \u2014 Iteration 1\nGenerated: 2026-02-23 01:42:47 UTC\n\n## Summary\n- **Satisfaction score: 42%** (5/12 scenarios passed)\n- Passed: 5 | Failed: 7 | Total: 12\n\n## Likely Root Causes\n1. Import errors in 5 scenario(s): Environment Determinism, Environment Observation Space, Environment Reward Structure, Environment Rendering, Environment Info Dict Completeness. Likely missing module or wrong import path.\n2. Assertion failures in 2 scenario(s): Training Produces Required Artifacts, Evaluation Produces Videos. Check the specific assertion messages.\n\n## Failed Scenarios \u2014 Full Details\n\n### Environment Determinism\n**Category:** environment\n**Exit code:** 1\n**Duration:** 0.02s\n**Error summary:** ModuleNotFoundError: No module named 'src.envs.minipong'\n\n**stderr:**\n```\nTraceback (most recent call last):\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'src.envs.minipong'\n```\n\n### Environment Observation Space\n**Category:** environment\n**Exit code:** 1\n**Duration:** 0.02s\n**Error summary:** ModuleNotFoundError: No module named 'src.envs.minipong'\n\n**stderr:**\n```\nTraceback (most recent call last):\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'src.envs.minipong'\n```\n\n### Environment Reward Structure\n**Category:** environment\n**Exit code:** 1\n**Duration:** 0.02s\n**Error summary:** ModuleNotFoundError: No module named 'src.envs.minipong'\n\n**stderr:**\n```\nTraceback (most recent call last):\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'src.envs.minipong'\n```\n\n### Environment Rendering\n**Category:** environment\n**Exit code:** 1\n**Duration:** 0.02s\n**Error summary:** ModuleNotFoundError: No module named 'src.envs.minipong'\n\n**stderr:**\n```\nTraceback (most recent call last):\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'src.envs.minipong'\n```\n\n### Environment Info Dict Completeness\n**Category:** environment\n**Exit code:** 1\n**Duration:** 0.02s\n**Error summary:** ModuleNotFoundError: No module named 'src.envs.minipong'\n\n**stderr:**\n```\nTraceback (most recent call last):\n  File \"<string>\", line 2, in <module>\nModuleNotFoundError: No module named 'src.envs.minipong'\n```\n\n### Training Produces Required Artifacts\n**Category:** training\n**Exit code:** 1\n**Duration:** 0.03s\n**Error summary:** AssertionError: Run directory not created: artifacts/scenario_test\n\n**stderr:**\n```\nTraceback (most recent call last):\n  File \"<string>\", line 5, in <module>\nAssertionError: Run directory not created: artifacts/scenario_test\n```\n\n### Evaluation Produces Videos\n**Category:** training\n**Exit code:** 1\n**Duration:** 0.03s\n**Error summary:** AssertionError: No video files produced in artifacts/scenario_video_test/videos/\n\n**stderr:**\n```\nTraceback (most recent call last):\n  File \"<string>\", line 6, in <module>\nAssertionError: No video files produced in artifacts/scenario_video_test/videos/\n```\n\n## Instructions for Coding Agent\n\nFix the failures above. Priorities:\n1. Import errors and missing modules first\n2. File/artifact production issues next\n3. Behavioral assertion failures last\n\nConstraints:\n- Do NOT modify /scenarios/, /scripts/, or /.github/workflows/factory.yaml\n- Do NOT modify /specs/ \u2014 read them as requirements\n- Keep changes minimal \u2014 fix what's broken, don't refactor\n",
      "base": ""
    },
    "artifacts/factory/iteration_count.txt": {
      "additions": 1,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/artifacts/factory/iteration_count.txt b/artifacts/factory/iteration_count.txt\nnew file mode 100644\nindex 0000000..00750ed\n--- /dev/null\n+++ b/artifacts/factory/iteration_count.txt\n@@ -0,0 +1 @@\n+3\n",
      "raw": "3\n",
      "base": ""
    },
    "configs/dqn_minipong.yaml": {
      "additions": 17,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/configs/dqn_minipong.yaml b/configs/dqn_minipong.yaml\nnew file mode 100644\nindex 0000000..c133823\n--- /dev/null\n+++ b/configs/dqn_minipong.yaml\n@@ -0,0 +1,17 @@\n+run_id: demo_local\n+seed: 7\n+frame_stack: 4\n+total_steps: 2000\n+max_episode_steps: 800\n+replay_capacity: 20000\n+replay_warmup_steps: 200\n+batch_size: 32\n+gamma: 0.99\n+lr: 0.001\n+epsilon_start: 1.0\n+epsilon_end: 0.05\n+epsilon_decay_steps: 1500\n+target_update_period: 100\n+eval_every_steps: 500\n+eval_episodes: 4\n+eval_seeds: [11, 22]\n",
      "raw": "run_id: demo_local\nseed: 7\nframe_stack: 4\ntotal_steps: 2000\nmax_episode_steps: 800\nreplay_capacity: 20000\nreplay_warmup_steps: 200\nbatch_size: 32\ngamma: 0.99\nlr: 0.001\nepsilon_start: 1.0\nepsilon_end: 0.05\nepsilon_decay_steps: 1500\ntarget_update_period: 100\neval_every_steps: 500\neval_episodes: 4\neval_seeds: [11, 22]\n",
      "base": ""
    },
    "configs/eval_minipong.yaml": {
      "additions": 5,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/configs/eval_minipong.yaml b/configs/eval_minipong.yaml\nnew file mode 100644\nindex 0000000..a11704c\n--- /dev/null\n+++ b/configs/eval_minipong.yaml\n@@ -0,0 +1,5 @@\n+run_id: demo_local\n+episodes: 4\n+seeds: [11, 22]\n+frame_stack: 4\n+max_steps: 800\n",
      "raw": "run_id: demo_local\nepisodes: 4\nseeds: [11, 22]\nframe_stack: 4\nmax_steps: 800\n",
      "base": ""
    },
    "demo_pack/README.md": {
      "additions": 3,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/demo_pack/README.md b/demo_pack/README.md\nnew file mode 100644\nindex 0000000..0a6c6ec\n--- /dev/null\n+++ b/demo_pack/README.md\n@@ -0,0 +1,3 @@\n+# Demo Pack\n+\n+Contains packaged outputs needed to reproduce and present training evidence.\n",
      "raw": "# Demo Pack\n\nContains packaged outputs needed to reproduce and present training evidence.\n",
      "base": ""
    },
    "docs/code_quality_standards.md": {
      "additions": 87,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/code_quality_standards.md b/docs/code_quality_standards.md\nnew file mode 100644\nindex 0000000..7324d2b\n--- /dev/null\n+++ b/docs/code_quality_standards.md\n@@ -0,0 +1,87 @@\n+# Code Quality Standards\n+\n+Universal standards for all code written in this repository \u2014 whether by Codex (attractor), Claude Code (orchestrator), or humans. These are not guidelines; they are gates.\n+\n+This file is the canonical source. Other files (`factory_fix.md`, `adversarial_review.md`, `factory_validation_strategy.md`) reference this document.\n+\n+## Anti-Stam Test Rules\n+\n+Every test must exercise real behavior through real code paths. Tests that pass by construction prove nothing and waste factory iterations.\n+\n+1. **No mocking the system under test.** Mocks isolate external dependencies (network, filesystem, third-party APIs) \u2014 never the logic you're testing. If `@patch` targets the function the test claims to validate, the test is stam.\n+2. **No stub assertions.** `assert True`, `assert 1`, `assert not False`, and assertions against hardcoded expected values without running real logic are all stam.\n+3. **No tautological tests.** The expected value must not be computed by the same code being tested. `assert compute(x) == compute(x)` proves nothing.\n+4. **No zero-assertion tests.** Every `test_` function must contain at least one meaningful assertion or `pytest.raises` check.\n+5. **No excessive mocking.** If more than 50% of test setup is patches/mocks, you're testing the mocking framework. Redesign the test.\n+6. **The deletion test.** Would the test still pass if you replaced the implementation with `pass`? If yes, the test is stam.\n+\n+## Anti-Gaming Rules\n+\n+Implementations must solve the general problem, not the specific test inputs.\n+\n+1. **No hardcoded lookup tables** matching known test cases. `is_prime(x): return x in {2, 3, 5, 7, 11, 13}` is not a prime checker.\n+2. **No overfitted implementations.** `if input == specific_value: return specific_output` is gaming. Implement general logic.\n+3. **No output-matching shortcuts.** If a function should compute a result, it must actually compute it \u2014 not return a cached or pre-recorded value.\n+4. **No overfitting to error messages.** When a scenario fails, fix the root cause. Making one specific assertion pass while breaking the general case is gaming.\n+5. **No test-detection.** Code must not behave differently when it detects pytest, CI environment variables, or test fixtures. Same code paths, always.\n+6. **No assertion-matching.** Fixing a specific assertion by hardcoding the expected value rather than fixing underlying logic is gaming.\n+\n+## Implementation Honesty\n+\n+Code must do what it claims to do, through real dependencies and real paths.\n+\n+1. **Real imports.** Test files importing from `src/` must exercise the real module, not a local redefinition with the same name.\n+2. **Real configuration.** Config files must reflect actual runtime parameters. Test-only shortcuts that trivialize behavior (1 step, 1 episode) don't prove the system works.\n+3. **Real dependencies.** Docker builds include all packages the code needs at runtime. Skipping dependencies to speed up builds creates hollow artifacts.\n+4. **No import redirection.** Defining a local class/function that shadows a real import to avoid testing the real thing is architectural dishonesty.\n+5. **No dependency skipping.** Catching `ImportError` and silently degrading to a no-op is acceptable only for optional features, never for core functionality.\n+6. **No dead code.** Functions that exist solely to satisfy import checks but are never called in any real code path are dishonest artifacts.\n+\n+## Test Hygiene\n+\n+1. **Use `tmp_path` fixtures** for file operations \u2014 never touch the real filesystem.\n+2. **Real execution** over mocked execution \u2014 `subprocess.run` over mocked calls when testing command execution.\n+3. **Every assertion tests a meaningful property** \u2014 not just \"doesn't crash.\"\n+4. **Tests must be independent** \u2014 no reliance on execution order or shared mutable state.\n+5. **Fixture scope matches test scope** \u2014 session-scoped fixtures only for truly expensive setup.\n+\n+## Quality Gates\n+\n+These must pass before any commit is considered complete:\n+\n+| Gate | Command | What It Checks |\n+|------|---------|----------------|\n+| Lint | `make lint` | ruff check (style, imports, bugs) |\n+| Types | `make typecheck` | mypy (type correctness) |\n+| Tests | `make test` | pytest (full suite including attractor-built tests) |\n+| Docker | `docker build` | Build completes with real dependencies |\n+| Validate | `make validate` | All of the above + env-smoke |\n+\n+## Non-Functional Requirements (Gate 2)\n+\n+The factory supports pluggable NFR checks. Each NFR has a testable mechanism that runs without human code review:\n+\n+| NFR | Mechanism | Status |\n+|-----|-----------|--------|\n+| Code quality | `ruff check` (extended rules) | Active |\n+| Complexity | `radon cc src/ --min C` (cyclomatic complexity) | Planned |\n+| Dead code | `vulture src/` (unused code detection) | Planned |\n+| Duplication | `jscpd src/` or `pylint --enable=duplicate-code` | Planned |\n+| Import hygiene | Custom check: orphan files, circular imports | Planned |\n+| Test coverage | `pytest --cov=src --cov-fail-under=60` | Planned |\n+| Maintainability | Radon maintainability index + LLM-as-judge | Planned |\n+| Security | `bandit -r src/` (vulnerability patterns) | Planned |\n+| Performance | Scenario-level: training smoke <60s | Via Gate 3 |\n+| Reliability | Run scenario N times, check consistency | Planned |\n+\n+NFR findings are non-blocking but tracked. They feed into the feedback loop and the LLM-as-judge's holistic evaluation.\n+\n+## Automated Enforcement\n+\n+These standards are enforced at multiple levels:\n+\n+1. **`scripts/check_test_quality.py`** \u2014 AST-based scanner detecting tautological asserts, zero-assertion tests, excessive mocking, stub implementations, hardcoded returns, lookup tables.\n+2. **Gate 0: Adversarial Review** \u2014 Claude Code reviews every attractor commit for stam tests, gaming, architectural dishonesty, spec violations, and integration gaps before deterministic gates run.\n+3. **Gate 1: Deterministic CI** \u2014 `make lint && make typecheck && make test` must all pass.\n+4. **Gate 2: NFR checks** \u2014 pluggable non-functional requirement validators.\n+5. **LLM-as-Judge** \u2014 Claude Code reasons holistically through all gate outputs, factoring in NFR findings and trajectory.\n",
      "raw": "# Code Quality Standards\n\nUniversal standards for all code written in this repository \u2014 whether by Codex (attractor), Claude Code (orchestrator), or humans. These are not guidelines; they are gates.\n\nThis file is the canonical source. Other files (`factory_fix.md`, `adversarial_review.md`, `factory_validation_strategy.md`) reference this document.\n\n## Anti-Stam Test Rules\n\nEvery test must exercise real behavior through real code paths. Tests that pass by construction prove nothing and waste factory iterations.\n\n1. **No mocking the system under test.** Mocks isolate external dependencies (network, filesystem, third-party APIs) \u2014 never the logic you're testing. If `@patch` targets the function the test claims to validate, the test is stam.\n2. **No stub assertions.** `assert True`, `assert 1`, `assert not False`, and assertions against hardcoded expected values without running real logic are all stam.\n3. **No tautological tests.** The expected value must not be computed by the same code being tested. `assert compute(x) == compute(x)` proves nothing.\n4. **No zero-assertion tests.** Every `test_` function must contain at least one meaningful assertion or `pytest.raises` check.\n5. **No excessive mocking.** If more than 50% of test setup is patches/mocks, you're testing the mocking framework. Redesign the test.\n6. **The deletion test.** Would the test still pass if you replaced the implementation with `pass`? If yes, the test is stam.\n\n## Anti-Gaming Rules\n\nImplementations must solve the general problem, not the specific test inputs.\n\n1. **No hardcoded lookup tables** matching known test cases. `is_prime(x): return x in {2, 3, 5, 7, 11, 13}` is not a prime checker.\n2. **No overfitted implementations.** `if input == specific_value: return specific_output` is gaming. Implement general logic.\n3. **No output-matching shortcuts.** If a function should compute a result, it must actually compute it \u2014 not return a cached or pre-recorded value.\n4. **No overfitting to error messages.** When a scenario fails, fix the root cause. Making one specific assertion pass while breaking the general case is gaming.\n5. **No test-detection.** Code must not behave differently when it detects pytest, CI environment variables, or test fixtures. Same code paths, always.\n6. **No assertion-matching.** Fixing a specific assertion by hardcoding the expected value rather than fixing underlying logic is gaming.\n\n## Implementation Honesty\n\nCode must do what it claims to do, through real dependencies and real paths.\n\n1. **Real imports.** Test files importing from `src/` must exercise the real module, not a local redefinition with the same name.\n2. **Real configuration.** Config files must reflect actual runtime parameters. Test-only shortcuts that trivialize behavior (1 step, 1 episode) don't prove the system works.\n3. **Real dependencies.** Docker builds include all packages the code needs at runtime. Skipping dependencies to speed up builds creates hollow artifacts.\n4. **No import redirection.** Defining a local class/function that shadows a real import to avoid testing the real thing is architectural dishonesty.\n5. **No dependency skipping.** Catching `ImportError` and silently degrading to a no-op is acceptable only for optional features, never for core functionality.\n6. **No dead code.** Functions that exist solely to satisfy import checks but are never called in any real code path are dishonest artifacts.\n\n## Test Hygiene\n\n1. **Use `tmp_path` fixtures** for file operations \u2014 never touch the real filesystem.\n2. **Real execution** over mocked execution \u2014 `subprocess.run` over mocked calls when testing command execution.\n3. **Every assertion tests a meaningful property** \u2014 not just \"doesn't crash.\"\n4. **Tests must be independent** \u2014 no reliance on execution order or shared mutable state.\n5. **Fixture scope matches test scope** \u2014 session-scoped fixtures only for truly expensive setup.\n\n## Quality Gates\n\nThese must pass before any commit is considered complete:\n\n| Gate | Command | What It Checks |\n|------|---------|----------------|\n| Lint | `make lint` | ruff check (style, imports, bugs) |\n| Types | `make typecheck` | mypy (type correctness) |\n| Tests | `make test` | pytest (full suite including attractor-built tests) |\n| Docker | `docker build` | Build completes with real dependencies |\n| Validate | `make validate` | All of the above + env-smoke |\n\n## Non-Functional Requirements (Gate 2)\n\nThe factory supports pluggable NFR checks. Each NFR has a testable mechanism that runs without human code review:\n\n| NFR | Mechanism | Status |\n|-----|-----------|--------|\n| Code quality | `ruff check` (extended rules) | Active |\n| Complexity | `radon cc src/ --min C` (cyclomatic complexity) | Planned |\n| Dead code | `vulture src/` (unused code detection) | Planned |\n| Duplication | `jscpd src/` or `pylint --enable=duplicate-code` | Planned |\n| Import hygiene | Custom check: orphan files, circular imports | Planned |\n| Test coverage | `pytest --cov=src --cov-fail-under=60` | Planned |\n| Maintainability | Radon maintainability index + LLM-as-judge | Planned |\n| Security | `bandit -r src/` (vulnerability patterns) | Planned |\n| Performance | Scenario-level: training smoke <60s | Via Gate 3 |\n| Reliability | Run scenario N times, check consistency | Planned |\n\nNFR findings are non-blocking but tracked. They feed into the feedback loop and the LLM-as-judge's holistic evaluation.\n\n## Automated Enforcement\n\nThese standards are enforced at multiple levels:\n\n1. **`scripts/check_test_quality.py`** \u2014 AST-based scanner detecting tautological asserts, zero-assertion tests, excessive mocking, stub implementations, hardcoded returns, lookup tables.\n2. **Gate 0: Adversarial Review** \u2014 Claude Code reviews every attractor commit for stam tests, gaming, architectural dishonesty, spec violations, and integration gaps before deterministic gates run.\n3. **Gate 1: Deterministic CI** \u2014 `make lint && make typecheck && make test` must all pass.\n4. **Gate 2: NFR checks** \u2014 pluggable non-functional requirement validators.\n5. **LLM-as-Judge** \u2014 Claude Code reasons holistically through all gate outputs, factoring in NFR findings and trajectory.\n",
      "base": ""
    },
    "docs/compute.md": {
      "additions": 12,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/compute.md b/docs/compute.md\nnew file mode 100644\nindex 0000000..615dfb6\n--- /dev/null\n+++ b/docs/compute.md\n@@ -0,0 +1,12 @@\n+# Compute Guide\n+\n+## Local CPU first\n+Use `make train-smoke` and `make eval-smoke` to validate correctness quickly.\n+\n+## Remote with SkyPilot\n+Use `infra/compute/skypilot/train.yaml` and `infra/compute/skypilot/eval.yaml` for larger runs.\n+\n+## Cost controls\n+- start with smoke configs\n+- use periodic checkpointing\n+- evaluate with fixed small episode sets before scaling\n",
      "raw": "# Compute Guide\n\n## Local CPU first\nUse `make train-smoke` and `make eval-smoke` to validate correctness quickly.\n\n## Remote with SkyPilot\nUse `infra/compute/skypilot/train.yaml` and `infra/compute/skypilot/eval.yaml` for larger runs.\n\n## Cost controls\n- start with smoke configs\n- use periodic checkpointing\n- evaluate with fixed small episode sets before scaling\n",
      "base": ""
    },
    "docs/dark_factory.md": {
      "additions": 131,
      "deletions": 51,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/docs/dark_factory.md b/docs/dark_factory.md\nindex 6913d26..de2db33 100644\n--- a/docs/dark_factory.md\n+++ b/docs/dark_factory.md\n@@ -8,86 +8,146 @@ The pattern: **Seed \u2192 Agent \u2192 Validate \u2192 Feedback \u2192 Repeat until satisfi\n \n ## Architecture\n \n+**Claude Code is the factory orchestrator.** It drives the convergence loop, invokes Codex via browser, runs adversarial review, and makes holistic judgment calls. CI provides background validation on every push \u2014 it validates, it doesn't orchestrate.\n+\n+### Claude Code as Orchestrator (Primary)\n+\n+Claude Code runs the convergence loop via the `/factory-orchestrate` skill, using browser automation to invoke Codex through the ChatGPT Plus UI.\n+\n ```\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n-\u2502                  HUMAN (Joey)                    \u2502\n+\u2502               HUMAN (Project Lead)               \u2502\n \u2502  Authors specs (/specs/) and scenarios           \u2502\n-\u2502  Sets satisfaction threshold                     \u2502\n-\u2502  Triggers factory via workflow_dispatch           \u2502\n-\u2502  Escalates when factory stalls                   \u2502\n-\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n-             \u2502 specs            \u2502 scenarios (holdout)\n-             \u25bc                  \u25bc\n-\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n-\u2502            FACTORY ORCHESTRATOR                 \u2502\n-\u2502        .github/workflows/factory.yaml           \u2502\n-\u2502                                                 \u2502\n-\u2502  for each iteration:                            \u2502\n-\u2502    1. Layer 1: make lint, typecheck, test        \u2502\n-\u2502    2. Layer 2: run_scenarios.py                  \u2502\n-\u2502    3. Check satisfaction threshold               \u2502\n-\u2502    4. compile_feedback.py                        \u2502\n-\u2502    5. Invoke Codex with specs + feedback         \u2502\n-\u2502    6. Codex commits fixes                        \u2502\n-\u2502    7. Loop                                       \u2502\n-\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n+\u2502  Invokes /factory-orchestrate skill              \u2502\n+\u2502  Reviews PR at accept/merge gate                 \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n+             \u2502\n+             \u25bc\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502         CLAUDE CODE (Orchestrator)               \u2502\n+\u2502         .claude/skills/factory-orchestrate        \u2502\n+\u2502                                                   \u2502\n+\u2502  for each iteration:                              \u2502\n+\u2502    1. Create df-crank-vXX branch                  \u2502\n+\u2502    2. strip_holdout.py \u2192 remove /scenarios/       \u2502\n+\u2502    3. Push stripped branch to origin               \u2502\n+\u2502    4. Invoke Codex via browser (Codex UI)          \u2502\n+\u2502       \u2192 Codex creates its own codex-... branch     \u2502\n+\u2502    5. Gate 0: Adversarial code review              \u2502\n+\u2502    6. Merge Codex changes onto factory branch      \u2502\n+\u2502    7. restore_holdout.py \u2192 restore /scenarios/     \u2502\n+\u2502    8. Gate 1: make lint && typecheck && test        \u2502\n+\u2502    9. Gate 2: NFR checks (non-blocking)            \u2502\n+\u2502   10. Gate 3: Behavioral scenarios                 \u2502\n+\u2502   11. LLM-as-judge: holistic evaluation            \u2502\n+\u2502   12. If satisfied \u2192 PR. If not \u2192 feedback \u2192 loop  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502                  \u25b2\n-             \u2502 code changes     \u2502 feedback\n+             \u2502 stripped branch   \u2502 codex-... branch\n              \u25bc                  \u2502\n-\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n-\u2502              CODEX (Attractor)                  \u2502\n-\u2502  Reads: /specs/, feedback_iter_N.md             \u2502\n-\u2502  Writes: src/, tests/, configs/, Makefile       \u2502\n-\u2502  NEVER sees: /scenarios/                        \u2502\n-\u2502  NEVER touches: factory infrastructure          \u2502\n-\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502              CODEX (Attractor)                    \u2502\n+\u2502  Reads: /specs/, feedback_iter_N.md               \u2502\n+\u2502  Writes: src/, tests/, configs/, Makefile         \u2502\n+\u2502  NEVER sees: /scenarios/ (stripped from branch)    \u2502\n+\u2502  NEVER touches: factory infrastructure             \u2502\n+\u2502  Creates own branch: codex-...                     \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n+```\n+\n+### CI Validation on Push (Background)\n+\n+CI runs validation-only on every push to `factory/**` or `df-crank-**` branches: Gate 1 (lint/typecheck/test), Gate 2 (NFR checks), and Gate 3 (behavioral scenarios). Claude Code reads these CI results as input to its orchestration decisions. If an `OPENAI_API_KEY` secret is available, CI can also run a fallback convergence loop with Codex API \u2014 but browser orchestration is the primary path.\n+\n+```\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502            CI VALIDATION                          \u2502\n+\u2502        .github/workflows/factory.yaml             \u2502\n+\u2502                                                   \u2502\n+\u2502  On push to factory/** or df-crank-** branches:   \u2502\n+\u2502    1. Gate 1: lint + typecheck + test              \u2502\n+\u2502    2. Gate 2: NFR checks                           \u2502\n+\u2502    3. Gate 3: Behavioral scenarios                 \u2502\n+\u2502    4. Compile feedback                             \u2502\n+\u2502  Claude Code reads CI results and decides next     \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n ```\n \n-## Validation Layers\n+## Validation Gates\n \n-### Layer 1: Deterministic CI\n+### Gate 0: Adversarial Code Review (Claude Code orchestrated)\n+- Claude Code reviews Codex's changes before merging to the factory branch\n+- Checks for: stam tests, gaming, architectural dishonesty, stub implementations\n+- Standards defined in `docs/code_quality_standards.md`\n+- CRITICAL findings \u2192 send back to Codex with feedback (no point running gates)\n+- Clean or WARNING-only \u2192 proceed to Gate 1\n+\n+### Gate 1: Deterministic CI\n - `make lint` \u2014 ruff check\n - `make typecheck` \u2014 mypy\n-- `make test` \u2014 pytest\n+- `make test` \u2014 full pytest suite (including tests the attractor wrote, already reviewed by Gate 0)\n+\n+If any fail, Gates 2-3 are skipped. The agent gets the CI errors directly.\n \n-If any fail, Layer 2 is skipped. The agent gets the CI errors directly.\n+### Gate 2: Non-Functional Requirements (NFRs)\n+- `make nfr-check` \u2014 extensible framework (`scripts/nfr_checks.py`)\n+- **Active checks:** code quality (ruff extended), complexity (radon), dead code (vulture), security (bandit)\n+- **Planned checks:** duplication, import hygiene, coverage, maintainability, reliability\n+- Non-blocking \u2014 findings feed into feedback and LLM-as-judge evaluation\n+- Adding a new check: write a function, register in `NFR_CHECKS` dict\n \n-### Layer 2: Behavioral Scenarios\n+### Gate 3: Behavioral Scenarios\n - `scripts/run_scenarios.py` executes holdout scenarios from `/scenarios/`\n - Each scenario runs an evaluation command and checks pass criteria\n - Results produce a satisfaction score: `passed / total`\n \n-### Layer 3: LLM-as-Judge (optional, not implemented yet)\n-- For subjective scenarios (video quality, dashboard usability)\n-- Pluggable via `--llm-judge` flag on run_scenarios.py\n-- Not a hard dependency \u2014 factory works without it\n+### LLM-as-Judge (Claude Code orchestrated)\n+- Claude Code reasons holistically through ALL gate outputs\n+- Not just \"score >= threshold\" \u2014 considers trajectory, systemic issues, Gate 2 warnings\n+- Factors in: Are fixes real or is Codex gaming? Do patterns make sense?\n+- Holistic go/no-go decision for creating the PR\n \n-## Scenario Isolation (Filesystem Shuffle)\n+## Scenario Isolation (Branch Stripping)\n \n-The factory hides `/scenarios/` from Codex using a filesystem shuffle:\n+The factory hides `/scenarios/` from Codex using **branch stripping** \u2014 scenarios are physically removed from the branch Codex works on.\n \n-1. Before running scenarios: restore from `/tmp/factory_scenarios`\n-2. After running scenarios: move to `/tmp/factory_scenarios`\n-3. Codex runs with scenarios physically absent from the workspace\n-4. After Codex: restore scenarios for the next iteration\n+### How it works\n+1. `scripts/strip_holdout.py` removes `/scenarios/` and comments out Makefile scenario targets\n+2. Commits with marker `[factory:holdout-stripped]`\n+3. Verifies no scenario files remain on the branch\n+4. Stripped branch is pushed to origin \u2014 Codex only ever sees this branch\n+5. After Codex finishes, `scripts/restore_holdout.py` restores from `origin/main`\n \n-This is a hard gate \u2014 Codex literally cannot read files that don't exist in the filesystem.\n+### Why branch stripping (not filesystem shuffle)\n+The previous approach (`mv scenarios /tmp/`) was security theater \u2014 Codex runs in the same job and can read `/tmp/`. Branch stripping is a real gate: scenarios literally don't exist on the branch Codex sees. There's nothing to read, no path to guess, no hidden directory to discover.\n \n-Why not `.codexignore`? It doesn't exist as a shipped feature (as of Feb 2026, despite being the #1 community request).\n+### Scripts\n+- `scripts/strip_holdout.py` \u2014 deterministic removal, supports `--dry-run` and `--no-commit`\n+- `scripts/restore_holdout.py` \u2014 deterministic restoration from a git ref, supports `--ref` and `--dry-run`\n+- Both are factory-protected files (never modified by Codex)\n \n ## How to Trigger the Factory\n \n-### GitHub Actions (production)\n+### Claude Code Orchestrated (primary)\n+```\n+# Invoke the factory orchestration skill:\n+/factory-orchestrate\n+```\n+Claude Code handles the full loop: branch creation, holdout stripping, Codex invocation (via browser), adversarial review, validation, LLM judgment, and PR creation.\n+\n+### GitHub Actions (CI validation on push)\n ```\n-# Via GitHub UI: Actions \u2192 Dark Factory \u2192 Run workflow\n-# Or via CLI:\n+# Automatic: push to factory/** or df-crank-** branches triggers CI validation\n+# Manual: Actions \u2192 Dark Factory \u2192 Run workflow\n gh workflow run factory.yaml -f max_iterations=5 -f satisfaction_threshold=0.80\n ```\n+CI runs Gates 1-3 + feedback compilation. Claude Code reads the results.\n \n ### Local (testing the plumbing)\n ```bash\n-make factory-local    # One iteration: validate \u2192 scenarios \u2192 feedback (no Codex)\n+make factory-local    # One iteration: Gate 1 \u2192 Gate 2 \u2192 Gate 3 \u2192 feedback\n make factory-status   # Show current iteration and satisfaction score\n+make nfr-check        # Just run Gate 2 NFR checks\n ```\n \n ### Individual components\n@@ -135,12 +195,27 @@ Feedback files are at `artifacts/factory/feedback_iter_N.md`. Each contains:\n - **Full error details** \u2014 every failed scenario with complete stdout/stderr\n - **Instructions** \u2014 prioritized fix guidance for the coding agent\n \n+## Accept/Merge Gate\n+\n+The factory **never auto-merges** to main. When the convergence loop meets the satisfaction threshold, it creates (or updates) a PR with the `factory-converged` and `accept-merge-gate` labels. This is the single human decision point in the entire loop.\n+\n+**What the project lead reviews:**\n+- Satisfaction score \u2014 does it meet your quality bar?\n+- Residual warnings in the latest feedback file\n+- Unexpected files or dependencies introduced by the attractor\n+- Optionally: run `make factory-local` locally for additional confidence\n+\n+**To accept:** Approve and merge the PR. The factory branch can be deleted.\n+**To reject:** Close the PR and either adjust scenarios/specs or trigger another factory run.\n+\n+The accept/merge gate exists because code produced by the factory was never reviewed by humans during production. The satisfaction score provides probabilistic confidence, but the merge decision is always human.\n+\n ## When to Escalate\n \n Escalate to interactive debugging (Claude Code) when:\n - The factory has stalled for 3+ iterations with no score improvement\n - The same scenario keeps failing with the same error pattern\n-- Layer 1 failures persist (the code doesn't even pass lint/typecheck)\n+- Gate 1 failures persist (the code doesn't even pass lint/typecheck)\n - A scenario requires architectural changes the agent can't figure out from error messages alone\n \n ## Factory State\n@@ -161,7 +236,12 @@ artifacts/factory/\n | `/scenarios/*.md` | Human | How to evaluate (holdout) |\n | `/scripts/run_scenarios.py` | Factory | Scenario evaluation engine |\n | `/scripts/compile_feedback.py` | Factory | Feedback generation |\n-| `/.github/workflows/factory.yaml` | Factory | Orchestrator |\n+| `/scripts/strip_holdout.py` | Factory | Holdout stripping (isolation gate) |\n+| `/scripts/restore_holdout.py` | Factory | Holdout restoration |\n+| `/scripts/nfr_checks.py` | Factory | Gate 2 NFR checker |\n+| `/.github/workflows/factory.yaml` | Factory | CI validation on push |\n | `/.github/codex/prompts/factory_fix.md` | Factory | Codex instruction template |\n+| `/.claude/skills/factory-orchestrate/` | Factory | Claude Code orchestration skill |\n+| `/docs/code_quality_standards.md` | Factory | Universal code quality standards |\n | `/CLAUDE.md` | Factory | Repo-level context for Claude Code |\n | `/artifacts/factory/feedback_iter_*.md` | Factory | Iteration feedback (Codex reads) |\n",
      "raw": "# Dark Factory \u2014 Operating Manual\n\n## What Is This\n\nThe dark factory is a convergence loop that turns a one-shot AI code generation into working software through automated validation and feedback. Code is never reviewed by humans \u2014 correctness is inferred from externally observable behavior.\n\nThe pattern: **Seed \u2192 Agent \u2192 Validate \u2192 Feedback \u2192 Repeat until satisfied.**\n\n## Architecture\n\n**Claude Code is the factory orchestrator.** It drives the convergence loop, invokes Codex via browser, runs adversarial review, and makes holistic judgment calls. CI provides background validation on every push \u2014 it validates, it doesn't orchestrate.\n\n### Claude Code as Orchestrator (Primary)\n\nClaude Code runs the convergence loop via the `/factory-orchestrate` skill, using browser automation to invoke Codex through the ChatGPT Plus UI.\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               HUMAN (Project Lead)               \u2502\n\u2502  Authors specs (/specs/) and scenarios           \u2502\n\u2502  Invokes /factory-orchestrate skill              \u2502\n\u2502  Reviews PR at accept/merge gate                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         CLAUDE CODE (Orchestrator)               \u2502\n\u2502         .claude/skills/factory-orchestrate        \u2502\n\u2502                                                   \u2502\n\u2502  for each iteration:                              \u2502\n\u2502    1. Create df-crank-vXX branch                  \u2502\n\u2502    2. strip_holdout.py \u2192 remove /scenarios/       \u2502\n\u2502    3. Push stripped branch to origin               \u2502\n\u2502    4. Invoke Codex via browser (Codex UI)          \u2502\n\u2502       \u2192 Codex creates its own codex-... branch     \u2502\n\u2502    5. Gate 0: Adversarial code review              \u2502\n\u2502    6. Merge Codex changes onto factory branch      \u2502\n\u2502    7. restore_holdout.py \u2192 restore /scenarios/     \u2502\n\u2502    8. Gate 1: make lint && typecheck && test        \u2502\n\u2502    9. Gate 2: NFR checks (non-blocking)            \u2502\n\u2502   10. Gate 3: Behavioral scenarios                 \u2502\n\u2502   11. LLM-as-judge: holistic evaluation            \u2502\n\u2502   12. If satisfied \u2192 PR. If not \u2192 feedback \u2192 loop  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502                  \u25b2\n             \u2502 stripped branch   \u2502 codex-... branch\n             \u25bc                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              CODEX (Attractor)                    \u2502\n\u2502  Reads: /specs/, feedback_iter_N.md               \u2502\n\u2502  Writes: src/, tests/, configs/, Makefile         \u2502\n\u2502  NEVER sees: /scenarios/ (stripped from branch)    \u2502\n\u2502  NEVER touches: factory infrastructure             \u2502\n\u2502  Creates own branch: codex-...                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### CI Validation on Push (Background)\n\nCI runs validation-only on every push to `factory/**` or `df-crank-**` branches: Gate 1 (lint/typecheck/test), Gate 2 (NFR checks), and Gate 3 (behavioral scenarios). Claude Code reads these CI results as input to its orchestration decisions. If an `OPENAI_API_KEY` secret is available, CI can also run a fallback convergence loop with Codex API \u2014 but browser orchestration is the primary path.\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            CI VALIDATION                          \u2502\n\u2502        .github/workflows/factory.yaml             \u2502\n\u2502                                                   \u2502\n\u2502  On push to factory/** or df-crank-** branches:   \u2502\n\u2502    1. Gate 1: lint + typecheck + test              \u2502\n\u2502    2. Gate 2: NFR checks                           \u2502\n\u2502    3. Gate 3: Behavioral scenarios                 \u2502\n\u2502    4. Compile feedback                             \u2502\n\u2502  Claude Code reads CI results and decides next     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Validation Gates\n\n### Gate 0: Adversarial Code Review (Claude Code orchestrated)\n- Claude Code reviews Codex's changes before merging to the factory branch\n- Checks for: stam tests, gaming, architectural dishonesty, stub implementations\n- Standards defined in `docs/code_quality_standards.md`\n- CRITICAL findings \u2192 send back to Codex with feedback (no point running gates)\n- Clean or WARNING-only \u2192 proceed to Gate 1\n\n### Gate 1: Deterministic CI\n- `make lint` \u2014 ruff check\n- `make typecheck` \u2014 mypy\n- `make test` \u2014 full pytest suite (including tests the attractor wrote, already reviewed by Gate 0)\n\nIf any fail, Gates 2-3 are skipped. The agent gets the CI errors directly.\n\n### Gate 2: Non-Functional Requirements (NFRs)\n- `make nfr-check` \u2014 extensible framework (`scripts/nfr_checks.py`)\n- **Active checks:** code quality (ruff extended), complexity (radon), dead code (vulture), security (bandit)\n- **Planned checks:** duplication, import hygiene, coverage, maintainability, reliability\n- Non-blocking \u2014 findings feed into feedback and LLM-as-judge evaluation\n- Adding a new check: write a function, register in `NFR_CHECKS` dict\n\n### Gate 3: Behavioral Scenarios\n- `scripts/run_scenarios.py` executes holdout scenarios from `/scenarios/`\n- Each scenario runs an evaluation command and checks pass criteria\n- Results produce a satisfaction score: `passed / total`\n\n### LLM-as-Judge (Claude Code orchestrated)\n- Claude Code reasons holistically through ALL gate outputs\n- Not just \"score >= threshold\" \u2014 considers trajectory, systemic issues, Gate 2 warnings\n- Factors in: Are fixes real or is Codex gaming? Do patterns make sense?\n- Holistic go/no-go decision for creating the PR\n\n## Scenario Isolation (Branch Stripping)\n\nThe factory hides `/scenarios/` from Codex using **branch stripping** \u2014 scenarios are physically removed from the branch Codex works on.\n\n### How it works\n1. `scripts/strip_holdout.py` removes `/scenarios/` and comments out Makefile scenario targets\n2. Commits with marker `[factory:holdout-stripped]`\n3. Verifies no scenario files remain on the branch\n4. Stripped branch is pushed to origin \u2014 Codex only ever sees this branch\n5. After Codex finishes, `scripts/restore_holdout.py` restores from `origin/main`\n\n### Why branch stripping (not filesystem shuffle)\nThe previous approach (`mv scenarios /tmp/`) was security theater \u2014 Codex runs in the same job and can read `/tmp/`. Branch stripping is a real gate: scenarios literally don't exist on the branch Codex sees. There's nothing to read, no path to guess, no hidden directory to discover.\n\n### Scripts\n- `scripts/strip_holdout.py` \u2014 deterministic removal, supports `--dry-run` and `--no-commit`\n- `scripts/restore_holdout.py` \u2014 deterministic restoration from a git ref, supports `--ref` and `--dry-run`\n- Both are factory-protected files (never modified by Codex)\n\n## How to Trigger the Factory\n\n### Claude Code Orchestrated (primary)\n```\n# Invoke the factory orchestration skill:\n/factory-orchestrate\n```\nClaude Code handles the full loop: branch creation, holdout stripping, Codex invocation (via browser), adversarial review, validation, LLM judgment, and PR creation.\n\n### GitHub Actions (CI validation on push)\n```\n# Automatic: push to factory/** or df-crank-** branches triggers CI validation\n# Manual: Actions \u2192 Dark Factory \u2192 Run workflow\ngh workflow run factory.yaml -f max_iterations=5 -f satisfaction_threshold=0.80\n```\nCI runs Gates 1-3 + feedback compilation. Claude Code reads the results.\n\n### Local (testing the plumbing)\n```bash\nmake factory-local    # One iteration: Gate 1 \u2192 Gate 2 \u2192 Gate 3 \u2192 feedback\nmake factory-status   # Show current iteration and satisfaction score\nmake nfr-check        # Just run Gate 2 NFR checks\n```\n\n### Individual components\n```bash\nmake run-scenarios        # Just run scenarios\nmake compile-feedback     # Just compile feedback from latest results\n```\n\n## How to Write a New Scenario\n\nCreate a markdown file in `/scenarios/` with this structure:\n\n```markdown\n# Scenario: [descriptive name]\n\n## Category\n[environment | training | pipeline | dashboard | integration]\n\n## Preconditions\n- [what must be true before evaluation]\n\n## Behavioral Expectation\n[what the system should do, from observer perspective]\n\n## Evaluation Method\n\\```bash\n[command that tests the behavior]\n\\```\n\n## Pass Criteria\n[specific condition for passing]\n\n## Evidence Required\n- [what to capture as proof]\n```\n\nThe evaluation method is a bash command that exits 0 on pass, non-zero on fail. Keep commands self-contained \u2014 they run in the repo root with PYTHONPATH set.\n\n## How to Read Feedback\n\nFeedback files are at `artifacts/factory/feedback_iter_N.md`. Each contains:\n- **Summary** \u2014 satisfaction score and pass/fail counts\n- **Convergence trajectory** \u2014 how scores changed across iterations\n- **Likely root causes** \u2014 pattern-matched from error types\n- **Full error details** \u2014 every failed scenario with complete stdout/stderr\n- **Instructions** \u2014 prioritized fix guidance for the coding agent\n\n## Accept/Merge Gate\n\nThe factory **never auto-merges** to main. When the convergence loop meets the satisfaction threshold, it creates (or updates) a PR with the `factory-converged` and `accept-merge-gate` labels. This is the single human decision point in the entire loop.\n\n**What the project lead reviews:**\n- Satisfaction score \u2014 does it meet your quality bar?\n- Residual warnings in the latest feedback file\n- Unexpected files or dependencies introduced by the attractor\n- Optionally: run `make factory-local` locally for additional confidence\n\n**To accept:** Approve and merge the PR. The factory branch can be deleted.\n**To reject:** Close the PR and either adjust scenarios/specs or trigger another factory run.\n\nThe accept/merge gate exists because code produced by the factory was never reviewed by humans during production. The satisfaction score provides probabilistic confidence, but the merge decision is always human.\n\n## When to Escalate\n\nEscalate to interactive debugging (Claude Code) when:\n- The factory has stalled for 3+ iterations with no score improvement\n- The same scenario keeps failing with the same error pattern\n- Gate 1 failures persist (the code doesn't even pass lint/typecheck)\n- A scenario requires architectural changes the agent can't figure out from error messages alone\n\n## Factory State\n\n```\nartifacts/factory/\n\u251c\u2500\u2500 scenario_results.json    # Latest run results (gitignored)\n\u251c\u2500\u2500 ci_output.log            # Latest CI output (gitignored)\n\u251c\u2500\u2500 iteration_count.txt      # Current iteration number (committed)\n\u2514\u2500\u2500 feedback_iter_*.md       # All feedback files (committed \u2014 Codex reads these)\n```\n\n## Key Files\n\n| File | Owner | Purpose |\n|------|-------|---------|\n| `/specs/*.md` | Human | What the system should do |\n| `/scenarios/*.md` | Human | How to evaluate (holdout) |\n| `/scripts/run_scenarios.py` | Factory | Scenario evaluation engine |\n| `/scripts/compile_feedback.py` | Factory | Feedback generation |\n| `/scripts/strip_holdout.py` | Factory | Holdout stripping (isolation gate) |\n| `/scripts/restore_holdout.py` | Factory | Holdout restoration |\n| `/scripts/nfr_checks.py` | Factory | Gate 2 NFR checker |\n| `/.github/workflows/factory.yaml` | Factory | CI validation on push |\n| `/.github/codex/prompts/factory_fix.md` | Factory | Codex instruction template |\n| `/.claude/skills/factory-orchestrate/` | Factory | Claude Code orchestration skill |\n| `/docs/code_quality_standards.md` | Factory | Universal code quality standards |\n| `/CLAUDE.md` | Factory | Repo-level context for Claude Code |\n| `/artifacts/factory/feedback_iter_*.md` | Factory | Iteration feedback (Codex reads) |\n",
      "base": "# Dark Factory \u2014 Operating Manual\n\n## What Is This\n\nThe dark factory is a convergence loop that turns a one-shot AI code generation into working software through automated validation and feedback. Code is never reviewed by humans \u2014 correctness is inferred from externally observable behavior.\n\nThe pattern: **Seed \u2192 Agent \u2192 Validate \u2192 Feedback \u2192 Repeat until satisfied.**\n\n## Architecture\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  HUMAN (Joey)                    \u2502\n\u2502  Authors specs (/specs/) and scenarios           \u2502\n\u2502  Sets satisfaction threshold                     \u2502\n\u2502  Triggers factory via workflow_dispatch           \u2502\n\u2502  Escalates when factory stalls                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502 specs            \u2502 scenarios (holdout)\n             \u25bc                  \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            FACTORY ORCHESTRATOR                 \u2502\n\u2502        .github/workflows/factory.yaml           \u2502\n\u2502                                                 \u2502\n\u2502  for each iteration:                            \u2502\n\u2502    1. Layer 1: make lint, typecheck, test        \u2502\n\u2502    2. Layer 2: run_scenarios.py                  \u2502\n\u2502    3. Check satisfaction threshold               \u2502\n\u2502    4. compile_feedback.py                        \u2502\n\u2502    5. Invoke Codex with specs + feedback         \u2502\n\u2502    6. Codex commits fixes                        \u2502\n\u2502    7. Loop                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502                  \u25b2\n             \u2502 code changes     \u2502 feedback\n             \u25bc                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              CODEX (Attractor)                  \u2502\n\u2502  Reads: /specs/, feedback_iter_N.md             \u2502\n\u2502  Writes: src/, tests/, configs/, Makefile       \u2502\n\u2502  NEVER sees: /scenarios/                        \u2502\n\u2502  NEVER touches: factory infrastructure          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Validation Layers\n\n### Layer 1: Deterministic CI\n- `make lint` \u2014 ruff check\n- `make typecheck` \u2014 mypy\n- `make test` \u2014 pytest\n\nIf any fail, Layer 2 is skipped. The agent gets the CI errors directly.\n\n### Layer 2: Behavioral Scenarios\n- `scripts/run_scenarios.py` executes holdout scenarios from `/scenarios/`\n- Each scenario runs an evaluation command and checks pass criteria\n- Results produce a satisfaction score: `passed / total`\n\n### Layer 3: LLM-as-Judge (optional, not implemented yet)\n- For subjective scenarios (video quality, dashboard usability)\n- Pluggable via `--llm-judge` flag on run_scenarios.py\n- Not a hard dependency \u2014 factory works without it\n\n## Scenario Isolation (Filesystem Shuffle)\n\nThe factory hides `/scenarios/` from Codex using a filesystem shuffle:\n\n1. Before running scenarios: restore from `/tmp/factory_scenarios`\n2. After running scenarios: move to `/tmp/factory_scenarios`\n3. Codex runs with scenarios physically absent from the workspace\n4. After Codex: restore scenarios for the next iteration\n\nThis is a hard gate \u2014 Codex literally cannot read files that don't exist in the filesystem.\n\nWhy not `.codexignore`? It doesn't exist as a shipped feature (as of Feb 2026, despite being the #1 community request).\n\n## How to Trigger the Factory\n\n### GitHub Actions (production)\n```\n# Via GitHub UI: Actions \u2192 Dark Factory \u2192 Run workflow\n# Or via CLI:\ngh workflow run factory.yaml -f max_iterations=5 -f satisfaction_threshold=0.80\n```\n\n### Local (testing the plumbing)\n```bash\nmake factory-local    # One iteration: validate \u2192 scenarios \u2192 feedback (no Codex)\nmake factory-status   # Show current iteration and satisfaction score\n```\n\n### Individual components\n```bash\nmake run-scenarios        # Just run scenarios\nmake compile-feedback     # Just compile feedback from latest results\n```\n\n## How to Write a New Scenario\n\nCreate a markdown file in `/scenarios/` with this structure:\n\n```markdown\n# Scenario: [descriptive name]\n\n## Category\n[environment | training | pipeline | dashboard | integration]\n\n## Preconditions\n- [what must be true before evaluation]\n\n## Behavioral Expectation\n[what the system should do, from observer perspective]\n\n## Evaluation Method\n\\```bash\n[command that tests the behavior]\n\\```\n\n## Pass Criteria\n[specific condition for passing]\n\n## Evidence Required\n- [what to capture as proof]\n```\n\nThe evaluation method is a bash command that exits 0 on pass, non-zero on fail. Keep commands self-contained \u2014 they run in the repo root with PYTHONPATH set.\n\n## How to Read Feedback\n\nFeedback files are at `artifacts/factory/feedback_iter_N.md`. Each contains:\n- **Summary** \u2014 satisfaction score and pass/fail counts\n- **Convergence trajectory** \u2014 how scores changed across iterations\n- **Likely root causes** \u2014 pattern-matched from error types\n- **Full error details** \u2014 every failed scenario with complete stdout/stderr\n- **Instructions** \u2014 prioritized fix guidance for the coding agent\n\n## When to Escalate\n\nEscalate to interactive debugging (Claude Code) when:\n- The factory has stalled for 3+ iterations with no score improvement\n- The same scenario keeps failing with the same error pattern\n- Layer 1 failures persist (the code doesn't even pass lint/typecheck)\n- A scenario requires architectural changes the agent can't figure out from error messages alone\n\n## Factory State\n\n```\nartifacts/factory/\n\u251c\u2500\u2500 scenario_results.json    # Latest run results (gitignored)\n\u251c\u2500\u2500 ci_output.log            # Latest CI output (gitignored)\n\u251c\u2500\u2500 iteration_count.txt      # Current iteration number (committed)\n\u2514\u2500\u2500 feedback_iter_*.md       # All feedback files (committed \u2014 Codex reads these)\n```\n\n## Key Files\n\n| File | Owner | Purpose |\n|------|-------|---------|\n| `/specs/*.md` | Human | What the system should do |\n| `/scenarios/*.md` | Human | How to evaluate (holdout) |\n| `/scripts/run_scenarios.py` | Factory | Scenario evaluation engine |\n| `/scripts/compile_feedback.py` | Factory | Feedback generation |\n| `/.github/workflows/factory.yaml` | Factory | Orchestrator |\n| `/.github/codex/prompts/factory_fix.md` | Factory | Codex instruction template |\n| `/CLAUDE.md` | Factory | Repo-level context for Claude Code |\n| `/artifacts/factory/feedback_iter_*.md` | Factory | Iteration feedback (Codex reads) |\n"
    },
    "docs/demo.md": {
      "additions": 13,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/demo.md b/docs/demo.md\nnew file mode 100644\nindex 0000000..f4a0791\n--- /dev/null\n+++ b/docs/demo.md\n@@ -0,0 +1,13 @@\n+# Demo Reproduction\n+\n+```bash\n+make deps\n+make train-smoke\n+python -m src.train.record_video --output artifacts/smoke_run/videos/random.mp4\n+python -m src.train.record_video --checkpoint artifacts/smoke_run/checkpoints/step_500.pt --output artifacts/smoke_run/videos/early.mp4\n+python -m src.train.record_video --checkpoint artifacts/smoke_run/checkpoints/step_1000.pt --output artifacts/smoke_run/videos/mid.mp4\n+python -m src.train.record_video --checkpoint artifacts/smoke_run/checkpoints/step_2000.pt --output artifacts/smoke_run/videos/late.mp4\n+python -m src.train.make_montage --run_id smoke_run\n+make verify-learning\n+make dashboard\n+```\n",
      "raw": "# Demo Reproduction\n\n```bash\nmake deps\nmake train-smoke\npython -m src.train.record_video --output artifacts/smoke_run/videos/random.mp4\npython -m src.train.record_video --checkpoint artifacts/smoke_run/checkpoints/step_500.pt --output artifacts/smoke_run/videos/early.mp4\npython -m src.train.record_video --checkpoint artifacts/smoke_run/checkpoints/step_1000.pt --output artifacts/smoke_run/videos/mid.mp4\npython -m src.train.record_video --checkpoint artifacts/smoke_run/checkpoints/step_2000.pt --output artifacts/smoke_run/videos/late.mp4\npython -m src.train.make_montage --run_id smoke_run\nmake verify-learning\nmake dashboard\n```\n",
      "base": ""
    },
    "docs/dev_agents.md": {
      "additions": 3,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/dev_agents.md b/docs/dev_agents.md\nnew file mode 100644\nindex 0000000..71524b0\n--- /dev/null\n+++ b/docs/dev_agents.md\n@@ -0,0 +1,3 @@\n+# Dev Agents\n+\n+See `agents/dev_team/*.md` for role-specific instructions and `agents/workflows/` for collaboration and PR process.\n",
      "raw": "# Dev Agents\n\nSee `agents/dev_team/*.md` for role-specific instructions and `agents/workflows/` for collaboration and PR process.\n",
      "base": ""
    },
    "docs/factory_architecture.html": {
      "additions": 120,
      "deletions": 8,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/docs/factory_architecture.html b/docs/factory_architecture.html\nindex 81434de..c15971b 100644\n--- a/docs/factory_architecture.html\n+++ b/docs/factory_architecture.html\n@@ -18,6 +18,7 @@\n     --feedback: #bc8cff;\n     --pass: #3fb950;\n     --fail: #f85149;\n+    --claude: #d2a8ff;\n   }\n   * { margin: 0; padding: 0; box-sizing: border-box; }\n   body { background: var(--bg); color: var(--text); font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; line-height: 1.6; }\n@@ -49,6 +50,9 @@\n   .arch-node.holdout h3 { color: var(--holdout); }\n   .arch-node.feedback { border-color: var(--feedback); }\n   .arch-node.feedback h3 { color: var(--feedback); }\n+  .arch-node.claude { border-color: var(--claude); }\n+  .arch-node.claude h3 { color: var(--claude); }\n+  .badge.claude { background: rgba(210,168,255,0.15); color: var(--claude); }\n \n   .arrow { width: 2px; height: 40px; background: var(--border); position: relative; }\n   .arrow::after { content: ''; position: absolute; bottom: -6px; left: -5px; width: 0; height: 0; border-left: 6px solid transparent; border-right: 6px solid transparent; border-top: 8px solid var(--border); }\n@@ -62,6 +66,7 @@\n   .sub-step { background: rgba(255,255,255,0.03); border-radius: 8px; padding: 12px; font-size: 13px; }\n   .sub-step .label { font-weight: 600; font-size: 12px; text-transform: uppercase; letter-spacing: 0.5px; margin-bottom: 4px; }\n   .sub-step.l1 .label { color: var(--factory); }\n+  .sub-step.l15 .label { color: #d2a8ff; }\n   .sub-step.l2 .label { color: var(--holdout); }\n \n   /* Wiring Table */\n@@ -113,11 +118,12 @@\n </div>\n \n <div class=\"legend\">\n-  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--human)\"></div> Human (Joey)</div>\n+  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--human)\"></div> Human (Project Lead)</div>\n   <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--factory)\"></div> Factory Orchestrator</div>\n   <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--codex)\"></div> Codex (Attractor)</div>\n   <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--holdout)\"></div> Holdout Scenarios</div>\n   <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--feedback)\"></div> Feedback Loop</div>\n+  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--claude)\"></div> Claude Code (Orchestrator)</div>\n </div>\n \n <div class=\"tabs\">\n@@ -125,6 +131,7 @@\n   <div class=\"tab\" onclick=\"showView('wiring')\">Component Wiring</div>\n   <div class=\"tab\" onclick=\"showView('human')\">Human Interaction</div>\n   <div class=\"tab\" onclick=\"showView('validation')\">Validation Proof</div>\n+  <div class=\"tab\" onclick=\"showView('orchestrator')\">Claude Code as Orchestrator</div>\n </div>\n \n <!-- \u2500\u2500\u2500 ARCHITECTURE VIEW \u2500\u2500\u2500 -->\n@@ -132,7 +139,7 @@\n   <div class=\"arch-flow\">\n \n     <div class=\"arch-node human\">\n-      <h3>HUMAN \u2014 Joey</h3>\n+      <h3>HUMAN \u2014 Project Lead</h3>\n       <p>Authors specs (/specs/*.md) and scenarios (/scenarios/*.md). Sets satisfaction threshold. Triggers factory. Escalates when loop stalls.</p>\n       <div class=\"sub-steps\">\n         <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--human)\">Writes</div>/specs/ \u2014 what the system should do</div>\n@@ -147,9 +154,10 @@\n       <div class=\"arch-node factory\" style=\"margin-bottom:0\">\n         <h3>FACTORY ORCHESTRATOR</h3>\n         <p>.github/workflows/factory.yaml \u2014 runs the loop, controls iteration</p>\n-        <div class=\"sub-steps\">\n-          <div class=\"sub-step l1\"><div class=\"label\">Layer 1: Deterministic CI</div>make lint + typecheck + test<br><em>If fail \u2192 skip Layer 2, go to feedback</em></div>\n-          <div class=\"sub-step l2\"><div class=\"label\">Layer 2: Behavioral Scenarios</div>scripts/run_scenarios.py<br><em>12 holdout scenarios, satisfaction scoring</em></div>\n+        <div class=\"sub-steps\" style=\"grid-template-columns:1fr 1fr 1fr\">\n+          <div class=\"sub-step l1\"><div class=\"label\">Gate 1: Deterministic CI</div>make lint + typecheck + test<br><em>If fail \u2192 skip remaining, go to feedback</em></div>\n+          <div class=\"sub-step l15\"><div class=\"label\">Gate 2: Structural Quality</div>vulture (dead code), radon (complexity), coverage, bandit (security)<br><em>Non-blocking but tracked</em></div>\n+          <div class=\"sub-step l2\"><div class=\"label\">Gate 3: Behavioral Scenarios</div>scripts/run_scenarios.py<br><em>12 holdout scenarios, satisfaction scoring</em></div>\n         </div>\n       </div>\n       <div class=\"arrow\" style=\"margin: 0 auto\"></div>\n@@ -179,7 +187,7 @@\n     <div class=\"arrow colored\"></div>\n     <div class=\"arch-node factory\" style=\"border-style:dashed\">\n       <h3>EXIT CONDITIONS</h3>\n-      <p>Satisfaction threshold met (default 80%) \u2192 post PR comment, done.<br>Max iterations reached \u2192 tag @joeyfezster for escalation.</p>\n+      <p>Satisfaction threshold met (default 80%) \u2192 post PR comment, await project lead's accept/merge decision.<br>Max iterations reached \u2192 tag project lead for escalation.</p>\n     </div>\n \n     <div class=\"fs-shuffle\">\n@@ -278,7 +286,7 @@ Why: .codexignore doesn't exist. This is a filesystem-level gate, not an LLM ins\n <!-- \u2500\u2500\u2500 HUMAN INTERACTION VIEW \u2500\u2500\u2500 -->\n <div class=\"view\" id=\"view-human\">\n   <h2 class=\"section-title\">Human Interaction Points</h2>\n-  <p style=\"color:var(--muted);margin-bottom:24px\">The factory is designed to run autonomously. Joey's involvement is limited to five distinct touchpoints:</p>\n+  <p style=\"color:var(--muted);margin-bottom:24px\">The factory is designed to run autonomously. The project lead's involvement is limited to six distinct touchpoints:</p>\n \n   <div class=\"interaction-card\">\n     <h3>1. Author Specs</h3>\n@@ -328,8 +336,18 @@ Why: .codexignore doesn't exist. This is a filesystem-level gate, not an LLM ins\n     </div>\n   </div>\n \n+  <div class=\"interaction-card\">\n+    <h3>6. Accept / Promote / Ship</h3>\n+    <div class=\"when\">After the factory converges and the satisfaction report looks right</div>\n+    <div class=\"action\">\n+      Merge the factory branch into main. This is the irreversible human gate \u2014 the decision that the product is ready.\n+      Review the satisfaction dashboard (scenario pass/fail matrix, convergence trajectory).\n+      Do NOT review code. The satisfaction score is your signal.\n+    </div>\n+  </div>\n+\n   <div style=\"background:var(--surface);border:1px solid var(--border);border-radius:12px;padding:24px;margin-top:24px\">\n-    <h3 style=\"color:var(--factory);margin-bottom:12px\">What Joey Does NOT Do</h3>\n+    <h3 style=\"color:var(--factory);margin-bottom:12px\">What the Project Lead Does NOT Do</h3>\n     <ul style=\"color:var(--muted);font-size:14px;padding-left:20px\">\n       <li>Read diffs or code changes</li>\n       <li>Review PRs for code quality</li>\n@@ -478,6 +496,100 @@ Why: .codexignore doesn't exist. This is a filesystem-level gate, not an LLM ins\n   </div>\n </div>\n \n+<!-- \u2500\u2500\u2500 CLAUDE CODE AS ORCHESTRATOR VIEW \u2500\u2500\u2500 -->\n+<div class=\"view\" id=\"view-orchestrator\">\n+  <h2 class=\"section-title\">Claude Code as Orchestrator \u2014 Hybrid Architecture</h2>\n+  <p style=\"color:var(--muted);margin-bottom:24px\">Claude Code runs the convergence loop, invoking Codex via browser. Holdout isolation via branch stripping (not filesystem shuffle). CI handles deterministic validation.</p>\n+\n+  <div class=\"arch-flow\">\n+\n+    <div class=\"arch-node human\">\n+      <h3>HUMAN \u2014 Project Lead</h3>\n+      <p>Authors /specs/ and /scenarios/. Says \"run a factory crank\" to Claude Code. Reviews the PR when the factory converges.</p>\n+    </div>\n+    <div class=\"arrow\"></div>\n+\n+    <div class=\"arch-node claude\">\n+      <h3>CLAUDE CODE \u2014 Orchestrator</h3>\n+      <p>Skill: <code>/factory-orchestrate</code> \u2014 runs the full convergence loop</p>\n+      <div class=\"sub-steps\">\n+        <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--claude)\">Creates</div>Branch: df-crank-vXX-{descriptor}</div>\n+        <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--claude)\">Strips</div>python scripts/strip_holdout.py</div>\n+      </div>\n+    </div>\n+    <div class=\"arrow\"></div>\n+\n+    <div class=\"loop-bracket\">\n+      <div class=\"loop-label\">CONVERGENCE LOOP (up to N iterations)</div>\n+\n+      <div class=\"arch-node codex\">\n+        <h3>CODEX (Attractor) \u2014 via Browser</h3>\n+        <p>Codex UI (ChatGPT Plus). Given: stripped branch (no /scenarios/). Creates its own codex-... branch. Reads /specs/ + feedback.</p>\n+        <div class=\"sub-steps\">\n+          <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--codex)\">Base Branch</div>df-crank-vXX (stripped \u2014 no scenarios)</div>\n+          <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--codex)\">Creates</div>codex-... branch with fixes</div>\n+        </div>\n+      </div>\n+      <div class=\"arrow\" style=\"margin: 0 auto\"></div>\n+\n+      <div class=\"arch-node claude\" style=\"border-color:var(--holdout)\">\n+        <h3 style=\"color:var(--holdout)\">GATE 0: Adversarial Review</h3>\n+        <p>Claude Code reviews Codex diff adversarially. Stam tests, gaming, architectural dishonesty, spec violations. CRITICAL \u2192 back to Codex. Clean \u2192 merge.</p>\n+      </div>\n+      <div class=\"arrow\" style=\"margin: 0 auto\"></div>\n+\n+      <div class=\"arch-node factory\" style=\"margin-bottom:0\">\n+        <h3>VALIDATION GATES (after merge + holdout restore)</h3>\n+        <div class=\"sub-steps\" style=\"grid-template-columns:1fr 1fr 1fr\">\n+          <div class=\"sub-step l1\"><div class=\"label\">Gate 1: Deterministic CI</div>make lint + typecheck + test<br><em>(full suite incl. attractor tests)</em><br><em>If fail \u2192 feedback \u2192 Codex</em></div>\n+          <div class=\"sub-step l15\"><div class=\"label\">Gate 2: NFRs ('ilities')</div>ruff extended, radon, vulture, bandit<br><em>Non-blocking \u2014 feeds LLM judge</em><br><em>Extensible: add checks as needed</em></div>\n+          <div class=\"sub-step l2\"><div class=\"label\">Gate 3: Behavioral Scenarios</div>scripts/run_scenarios.py<br><em>12 holdout scenarios</em><br><em>Produces satisfaction score</em></div>\n+        </div>\n+      </div>\n+      <div class=\"arrow colored\" style=\"margin: 0 auto\"></div>\n+\n+      <div class=\"arch-node claude\">\n+        <h3>LLM-as-JUDGE \u2014 Holistic Evaluation</h3>\n+        <p>Claude Code reasons through ALL outputs: satisfaction trajectory, failure patterns, fix quality, Gate 2 NFR findings, systemic issues. Not just \"score &ge; threshold.\"</p>\n+      </div>\n+      <div class=\"arrow colored\" style=\"margin: 0 auto\"></div>\n+\n+      <div class=\"arch-node feedback\" style=\"flex:1\">\n+        <h3>FEEDBACK COMPILER</h3>\n+        <p>scripts/compile_feedback.py + Claude Code's holistic assessment \u2192 enriched feedback for next iteration</p>\n+      </div>\n+    </div>\n+\n+    <div class=\"arrow colored\"></div>\n+    <div class=\"arch-node factory\" style=\"border-style:dashed\">\n+      <h3>EXIT \u2192 ACCEPT/MERGE GATE</h3>\n+      <p>LLM judge satisfied \u2192 <code>gh pr create</code> with factory-converged label. Human reviews satisfaction report, NFR status, and merges.</p>\n+    </div>\n+\n+    <div style=\"background:rgba(63,185,80,0.1);border:1px dashed var(--codex);border-radius:8px;padding:16px;margin-top:16px;max-width:840px;width:100%\">\n+      <h4 style=\"color:var(--codex);font-size:14px;margin-bottom:8px\">Holdout Isolation \u2014 Branch Stripping (replaces filesystem shuffle)</h4>\n+      <pre style=\"font-size:12px;color:var(--muted);white-space:pre-wrap\">Before Codex:   python scripts/strip_holdout.py     (scenarios removed from branch entirely)\n+Codex works:    codex-... branch has NO /scenarios/  (hard gate \u2014 files don't exist)\n+After Codex:    python scripts/restore_holdout.py    (scenarios restored from origin/main)\n+\n+Why: Filesystem shuffle (/tmp/) was security theater \u2014 Codex could read /tmp/.\n+Branch stripping is a real gate. Scenarios are not on the branch Codex sees.</pre>\n+    </div>\n+\n+    <div style=\"background:rgba(210,168,255,0.1);border:1px dashed var(--claude);border-radius:8px;padding:16px;margin-top:16px;max-width:840px;width:100%\">\n+      <h4 style=\"color:var(--claude);font-size:14px;margin-bottom:8px\">Branch Flow</h4>\n+      <pre style=\"font-size:12px;color:var(--muted);white-space:pre-wrap\">factory/v1 (base)\n+  \u2514\u2500 df-crank-v01-minipong (factory branch, created by Claude Code)\n+       \u251c\u2500 [stripped] scenarios removed, pushed\n+       \u251c\u2500 codex-abc123 (Codex creates, works here)\n+       \u251c\u2500 [merged] Codex changes merged after Gate 0\n+       \u251c\u2500 [restored] scenarios restored from origin/main\n+       \u251c\u2500 [validated] Gates 1-3 + LLM judge\n+       \u2514\u2500 PR \u2192 main (accept/merge gate)</pre>\n+    </div>\n+  </div>\n+</div>\n+\n <script>\n function showView(id) {\n   document.querySelectorAll('.view').forEach(v => v.classList.remove('active'));\n",
      "raw": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n<title>Dark Factory Architecture \u2014 MiniPong</title>\n<style>\n  :root {\n    --bg: #0d1117;\n    --surface: #161b22;\n    --border: #30363d;\n    --text: #e6edf3;\n    --muted: #8b949e;\n    --human: #f0883e;\n    --factory: #58a6ff;\n    --codex: #3fb950;\n    --holdout: #f85149;\n    --feedback: #bc8cff;\n    --pass: #3fb950;\n    --fail: #f85149;\n    --claude: #d2a8ff;\n  }\n  * { margin: 0; padding: 0; box-sizing: border-box; }\n  body { background: var(--bg); color: var(--text); font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; line-height: 1.6; }\n\n  .header { text-align: center; padding: 40px 20px 20px; }\n  .header h1 { font-size: 28px; font-weight: 600; }\n  .header p { color: var(--muted); margin-top: 8px; font-size: 15px; }\n\n  .tabs { display: flex; justify-content: center; gap: 8px; padding: 20px; flex-wrap: wrap; }\n  .tab { padding: 10px 24px; border-radius: 8px; border: 1px solid var(--border); background: var(--surface); color: var(--muted); cursor: pointer; font-size: 14px; transition: all 0.2s; }\n  .tab:hover { border-color: var(--factory); color: var(--text); }\n  .tab.active { background: var(--factory); color: #fff; border-color: var(--factory); }\n\n  .view { max-width: 1100px; margin: 0 auto; padding: 20px; display: none; }\n  .view.active { display: block; }\n\n  /* Architecture Diagram */\n  .arch-flow { display: flex; flex-direction: column; gap: 0; align-items: center; }\n  .arch-node { background: var(--surface); border: 2px solid var(--border); border-radius: 12px; padding: 24px; width: 100%; max-width: 800px; position: relative; }\n  .arch-node h3 { font-size: 16px; margin-bottom: 8px; display: flex; align-items: center; gap: 8px; }\n  .arch-node p { color: var(--muted); font-size: 14px; }\n  .arch-node.human { border-color: var(--human); }\n  .arch-node.human h3 { color: var(--human); }\n  .arch-node.factory { border-color: var(--factory); }\n  .arch-node.factory h3 { color: var(--factory); }\n  .arch-node.codex { border-color: var(--codex); }\n  .arch-node.codex h3 { color: var(--codex); }\n  .arch-node.holdout { border-color: var(--holdout); }\n  .arch-node.holdout h3 { color: var(--holdout); }\n  .arch-node.feedback { border-color: var(--feedback); }\n  .arch-node.feedback h3 { color: var(--feedback); }\n  .arch-node.claude { border-color: var(--claude); }\n  .arch-node.claude h3 { color: var(--claude); }\n  .badge.claude { background: rgba(210,168,255,0.15); color: var(--claude); }\n\n  .arrow { width: 2px; height: 40px; background: var(--border); position: relative; }\n  .arrow::after { content: ''; position: absolute; bottom: -6px; left: -5px; width: 0; height: 0; border-left: 6px solid transparent; border-right: 6px solid transparent; border-top: 8px solid var(--border); }\n  .arrow.colored { background: var(--feedback); }\n  .arrow.colored::after { border-top-color: var(--feedback); }\n\n  .loop-bracket { border: 2px dashed var(--feedback); border-radius: 16px; padding: 16px; margin: -8px 0; width: 100%; max-width: 840px; position: relative; }\n  .loop-label { position: absolute; top: -12px; left: 24px; background: var(--bg); padding: 0 8px; color: var(--feedback); font-size: 13px; font-weight: 600; }\n\n  .sub-steps { display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-top: 16px; }\n  .sub-step { background: rgba(255,255,255,0.03); border-radius: 8px; padding: 12px; font-size: 13px; }\n  .sub-step .label { font-weight: 600; font-size: 12px; text-transform: uppercase; letter-spacing: 0.5px; margin-bottom: 4px; }\n  .sub-step.l1 .label { color: var(--factory); }\n  .sub-step.l15 .label { color: #d2a8ff; }\n  .sub-step.l2 .label { color: var(--holdout); }\n\n  /* Wiring Table */\n  .wiring-table { width: 100%; border-collapse: collapse; margin-top: 20px; }\n  .wiring-table th { text-align: left; padding: 12px 16px; background: var(--surface); border-bottom: 2px solid var(--border); font-size: 13px; color: var(--muted); text-transform: uppercase; letter-spacing: 0.5px; }\n  .wiring-table td { padding: 12px 16px; border-bottom: 1px solid var(--border); font-size: 14px; vertical-align: top; }\n  .wiring-table tr:hover td { background: rgba(255,255,255,0.02); }\n\n  .badge { display: inline-block; padding: 2px 8px; border-radius: 12px; font-size: 11px; font-weight: 600; }\n  .badge.human { background: rgba(240,136,62,0.15); color: var(--human); }\n  .badge.factory { background: rgba(88,166,255,0.15); color: var(--factory); }\n  .badge.codex { background: rgba(63,185,80,0.15); color: var(--codex); }\n  .badge.holdout { background: rgba(248,81,73,0.15); color: var(--holdout); }\n\n  /* Human Interaction Points */\n  .interaction-card { background: var(--surface); border: 1px solid var(--border); border-radius: 12px; padding: 24px; margin-bottom: 16px; }\n  .interaction-card h3 { color: var(--human); font-size: 16px; margin-bottom: 8px; }\n  .interaction-card .when { color: var(--muted); font-size: 13px; margin-bottom: 12px; font-style: italic; }\n  .interaction-card .action { font-size: 14px; }\n  .interaction-card code { background: rgba(255,255,255,0.05); padding: 2px 6px; border-radius: 4px; font-size: 13px; }\n\n  /* Validation Checklist */\n  .check-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 12px; }\n  .check-item { background: var(--surface); border: 1px solid var(--border); border-radius: 8px; padding: 16px; }\n  .check-item.pass { border-color: var(--pass); }\n  .check-item.fail { border-color: var(--fail); }\n  .check-item h4 { font-size: 14px; margin-bottom: 4px; }\n  .check-item .status { font-size: 13px; font-weight: 600; }\n  .check-item .status.pass { color: var(--pass); }\n  .check-item .status.fail { color: var(--fail); }\n  .check-item p { color: var(--muted); font-size: 13px; margin-top: 4px; }\n\n  .section-title { font-size: 20px; font-weight: 600; margin: 32px 0 16px; padding-bottom: 8px; border-bottom: 1px solid var(--border); }\n\n  .legend { display: flex; gap: 24px; justify-content: center; padding: 16px; flex-wrap: wrap; }\n  .legend-item { display: flex; align-items: center; gap: 6px; font-size: 13px; color: var(--muted); }\n  .legend-dot { width: 12px; height: 12px; border-radius: 50%; }\n\n  .fs-shuffle { background: rgba(248,81,73,0.1); border: 1px dashed var(--holdout); border-radius: 8px; padding: 16px; margin-top: 16px; }\n  .fs-shuffle h4 { color: var(--holdout); font-size: 14px; margin-bottom: 8px; }\n  .fs-shuffle pre { font-size: 12px; color: var(--muted); white-space: pre-wrap; }\n</style>\n</head>\n<body>\n\n<div class=\"header\">\n  <h1>Dark Factory Architecture</h1>\n  <p>MiniPong RL System \u2014 Convergence Loop Infrastructure</p>\n</div>\n\n<div class=\"legend\">\n  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--human)\"></div> Human (Project Lead)</div>\n  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--factory)\"></div> Factory Orchestrator</div>\n  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--codex)\"></div> Codex (Attractor)</div>\n  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--holdout)\"></div> Holdout Scenarios</div>\n  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--feedback)\"></div> Feedback Loop</div>\n  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--claude)\"></div> Claude Code (Orchestrator)</div>\n</div>\n\n<div class=\"tabs\">\n  <div class=\"tab active\" onclick=\"showView('architecture')\">Architecture</div>\n  <div class=\"tab\" onclick=\"showView('wiring')\">Component Wiring</div>\n  <div class=\"tab\" onclick=\"showView('human')\">Human Interaction</div>\n  <div class=\"tab\" onclick=\"showView('validation')\">Validation Proof</div>\n  <div class=\"tab\" onclick=\"showView('orchestrator')\">Claude Code as Orchestrator</div>\n</div>\n\n<!-- \u2500\u2500\u2500 ARCHITECTURE VIEW \u2500\u2500\u2500 -->\n<div class=\"view active\" id=\"view-architecture\">\n  <div class=\"arch-flow\">\n\n    <div class=\"arch-node human\">\n      <h3>HUMAN \u2014 Project Lead</h3>\n      <p>Authors specs (/specs/*.md) and scenarios (/scenarios/*.md). Sets satisfaction threshold. Triggers factory. Escalates when loop stalls.</p>\n      <div class=\"sub-steps\">\n        <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--human)\">Writes</div>/specs/ \u2014 what the system should do</div>\n        <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--human)\">Writes</div>/scenarios/ \u2014 holdout evaluation criteria</div>\n      </div>\n    </div>\n    <div class=\"arrow\"></div>\n\n    <div class=\"loop-bracket\">\n      <div class=\"loop-label\">CONVERGENCE LOOP (up to N iterations)</div>\n\n      <div class=\"arch-node factory\" style=\"margin-bottom:0\">\n        <h3>FACTORY ORCHESTRATOR</h3>\n        <p>.github/workflows/factory.yaml \u2014 runs the loop, controls iteration</p>\n        <div class=\"sub-steps\" style=\"grid-template-columns:1fr 1fr 1fr\">\n          <div class=\"sub-step l1\"><div class=\"label\">Gate 1: Deterministic CI</div>make lint + typecheck + test<br><em>If fail \u2192 skip remaining, go to feedback</em></div>\n          <div class=\"sub-step l15\"><div class=\"label\">Gate 2: Structural Quality</div>vulture (dead code), radon (complexity), coverage, bandit (security)<br><em>Non-blocking but tracked</em></div>\n          <div class=\"sub-step l2\"><div class=\"label\">Gate 3: Behavioral Scenarios</div>scripts/run_scenarios.py<br><em>12 holdout scenarios, satisfaction scoring</em></div>\n        </div>\n      </div>\n      <div class=\"arrow\" style=\"margin: 0 auto\"></div>\n\n      <div style=\"display:flex;gap:16px;width:100%;max-width:800px;margin:0 auto\">\n        <div class=\"arch-node holdout\" style=\"flex:1\">\n          <h3>HOLDOUT SCENARIOS</h3>\n          <p>/scenarios/*.md \u2014 Codex NEVER sees these. Filesystem shuffle hides them during Codex execution.</p>\n        </div>\n        <div class=\"arch-node feedback\" style=\"flex:1\">\n          <h3>FEEDBACK COMPILER</h3>\n          <p>scripts/compile_feedback.py \u2192 artifacts/factory/feedback_iter_N.md</p>\n        </div>\n      </div>\n      <div class=\"arrow colored\" style=\"margin: 0 auto\"></div>\n\n      <div class=\"arch-node codex\">\n        <h3>CODEX (Attractor)</h3>\n        <p>openai/codex-action@v1 \u2014 reads /specs/ + feedback, writes code fixes. Non-interactive. Commits directly.</p>\n        <div class=\"sub-steps\">\n          <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--codex)\">Reads</div>/specs/*.md + feedback_iter_N.md</div>\n          <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--codex)\">Writes</div>src/, tests/, configs/, Makefile</div>\n        </div>\n      </div>\n    </div>\n\n    <div class=\"arrow colored\"></div>\n    <div class=\"arch-node factory\" style=\"border-style:dashed\">\n      <h3>EXIT CONDITIONS</h3>\n      <p>Satisfaction threshold met (default 80%) \u2192 post PR comment, await project lead's accept/merge decision.<br>Max iterations reached \u2192 tag project lead for escalation.</p>\n    </div>\n\n    <div class=\"fs-shuffle\">\n      <h4>Filesystem Shuffle \u2014 Scenario Isolation</h4>\n      <pre>Before Codex runs:  mv /scenarios/ /tmp/factory_scenarios    (scenarios disappear)\nCodex executes:     workspace has NO /scenarios/ directory    (hard gate)\nAfter Codex:        cp -r /tmp/factory_scenarios /scenarios/  (scenarios restored)\n\nWhy: .codexignore doesn't exist. This is a filesystem-level gate, not an LLM instruction.</pre>\n    </div>\n  </div>\n</div>\n\n<!-- \u2500\u2500\u2500 WIRING VIEW \u2500\u2500\u2500 -->\n<div class=\"view\" id=\"view-wiring\">\n  <h2 class=\"section-title\">Component Wiring \u2014 How Each File Maps to the Factory</h2>\n  <table class=\"wiring-table\">\n    <thead>\n      <tr><th>File</th><th>Owner</th><th>Dark Factory Role</th><th>Reads From</th><th>Writes To</th></tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td><code>/specs/*.md</code></td>\n        <td><span class=\"badge human\">Human</span></td>\n        <td>Source of truth \u2014 what the system should do. Codex reads these as requirements.</td>\n        <td>\u2014</td>\n        <td>Codex prompt context</td>\n      </tr>\n      <tr>\n        <td><code>/scenarios/*.md</code></td>\n        <td><span class=\"badge holdout\">Holdout</span></td>\n        <td>Behavioral holdout set \u2014 ML-style validation split. Agent never sees evaluation criteria.</td>\n        <td>\u2014</td>\n        <td>run_scenarios.py</td>\n      </tr>\n      <tr>\n        <td><code>scripts/run_scenarios.py</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Scenario evaluation engine. Parses scenario markdown, executes eval commands, produces JSON report.</td>\n        <td>/scenarios/*.md</td>\n        <td>artifacts/factory/scenario_results.json</td>\n      </tr>\n      <tr>\n        <td><code>scripts/compile_feedback.py</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Feedback generation. Reads scenario results + CI logs, infers root causes, produces structured feedback for Codex.</td>\n        <td>scenario_results.json, ci_output.log, previous feedback</td>\n        <td>artifacts/factory/feedback_iter_N.md</td>\n      </tr>\n      <tr>\n        <td><code>.github/workflows/factory.yaml</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Orchestrator. Implements the convergence loop: validate \u2192 scenarios \u2192 threshold check \u2192 feedback \u2192 Codex \u2192 repeat.</td>\n        <td>All of the above</td>\n        <td>PR comments, iteration state</td>\n      </tr>\n      <tr>\n        <td><code>.github/codex/prompts/factory_fix.md</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Codex prompt template. Instructions + constraints for each iteration. Lists protected files, fix priorities.</td>\n        <td>\u2014</td>\n        <td>Codex context</td>\n      </tr>\n      <tr>\n        <td><code>artifacts/factory/feedback_iter_N.md</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Iteration feedback. Full error output, inferred causes, convergence trajectory. This is what Codex reads to fix things.</td>\n        <td>compile_feedback.py</td>\n        <td>Codex context</td>\n      </tr>\n      <tr>\n        <td><code>CLAUDE.md</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Repo-level context for Claude Code (escalation path). Documents the factory model, protected files, quick commands.</td>\n        <td>\u2014</td>\n        <td>Claude Code context</td>\n      </tr>\n      <tr>\n        <td><code>src/**</code></td>\n        <td><span class=\"badge codex\">Codex</span></td>\n        <td>Product code. The \"opaque weights\" \u2014 correctness inferred from behavior, not inspection.</td>\n        <td>specs</td>\n        <td>Evaluated by scenarios</td>\n      </tr>\n      <tr>\n        <td><code>Makefile</code></td>\n        <td><span class=\"badge codex\">Codex</span> / <span class=\"badge factory\">Factory</span></td>\n        <td>Build interface. Factory targets (run-scenarios, compile-feedback, factory-local) + product targets (lint, test, train-smoke).</td>\n        <td>\u2014</td>\n        <td>\u2014</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n\n<!-- \u2500\u2500\u2500 HUMAN INTERACTION VIEW \u2500\u2500\u2500 -->\n<div class=\"view\" id=\"view-human\">\n  <h2 class=\"section-title\">Human Interaction Points</h2>\n  <p style=\"color:var(--muted);margin-bottom:24px\">The factory is designed to run autonomously. The project lead's involvement is limited to six distinct touchpoints:</p>\n\n  <div class=\"interaction-card\">\n    <h3>1. Author Specs</h3>\n    <div class=\"when\">Before the factory runs \u2014 defines intent</div>\n    <div class=\"action\">\n      Write or update files in <code>/specs/</code>. These are the requirements Codex reads. The specs define <em>what</em> the system should do.\n      No code. No implementation details. Just behavioral contracts.\n    </div>\n  </div>\n\n  <div class=\"interaction-card\">\n    <h3>2. Author Scenarios</h3>\n    <div class=\"when\">Before the factory runs \u2014 defines success criteria</div>\n    <div class=\"action\">\n      Write or update files in <code>/scenarios/</code>. Each scenario is a behavioral expectation with a runnable evaluation command.\n      These are the holdout set \u2014 Codex never sees them. This is how you define \"done\" without reviewing code.\n    </div>\n  </div>\n\n  <div class=\"interaction-card\">\n    <h3>3. Trigger the Factory</h3>\n    <div class=\"when\">When specs and scenarios are ready</div>\n    <div class=\"action\">\n      GitHub Actions UI: <code>Actions \u2192 Dark Factory \u2192 Run workflow</code><br>\n      Or CLI: <code>gh workflow run factory.yaml -f max_iterations=5 -f satisfaction_threshold=0.80</code><br>\n      Or: push to a <code>factory/*</code> branch.\n    </div>\n  </div>\n\n  <div class=\"interaction-card\">\n    <h3>4. Review Satisfaction Report</h3>\n    <div class=\"when\">After the factory converges or stalls</div>\n    <div class=\"action\">\n      The factory posts a PR comment with the satisfaction score and iteration count.\n      If converged: review the score, decide if it's good enough. You don't read the code.\n      If stalled: read <code>artifacts/factory/feedback_iter_N.md</code> to understand what's stuck.\n    </div>\n  </div>\n\n  <div class=\"interaction-card\">\n    <h3>5. Escalate to Interactive Debug</h3>\n    <div class=\"when\">When the factory stalls for 3+ iterations with no improvement</div>\n    <div class=\"action\">\n      Open the repo in Claude Code (desktop). It reads <code>CLAUDE.md</code> automatically.\n      Point it at the latest feedback file. Claude Code does interactive debugging \u2014 the thing the factory loop can't do.\n      Fix the issue, push, let the factory resume.\n    </div>\n  </div>\n\n  <div class=\"interaction-card\">\n    <h3>6. Accept / Promote / Ship</h3>\n    <div class=\"when\">After the factory converges and the satisfaction report looks right</div>\n    <div class=\"action\">\n      Merge the factory branch into main. This is the irreversible human gate \u2014 the decision that the product is ready.\n      Review the satisfaction dashboard (scenario pass/fail matrix, convergence trajectory).\n      Do NOT review code. The satisfaction score is your signal.\n    </div>\n  </div>\n\n  <div style=\"background:var(--surface);border:1px solid var(--border);border-radius:12px;padding:24px;margin-top:24px\">\n    <h3 style=\"color:var(--factory);margin-bottom:12px\">What the Project Lead Does NOT Do</h3>\n    <ul style=\"color:var(--muted);font-size:14px;padding-left:20px\">\n      <li>Read diffs or code changes</li>\n      <li>Review PRs for code quality</li>\n      <li>Debug test failures manually</li>\n      <li>Write any production code</li>\n      <li>Approve individual Codex commits</li>\n    </ul>\n    <p style=\"margin-top:12px;font-size:14px\">Code is opaque weights. Correctness is inferred from behavior. This is the validation constraint.</p>\n  </div>\n</div>\n\n<!-- \u2500\u2500\u2500 VALIDATION VIEW \u2500\u2500\u2500 -->\n<div class=\"view\" id=\"view-validation\">\n  <h2 class=\"section-title\">Factory Validation \u2014 What's Built and Proven</h2>\n  <p style=\"color:var(--muted);margin-bottom:24px\">Current status: factory infrastructure complete. Running against main branch (no MiniPong code yet).</p>\n\n  <div class=\"check-grid\">\n    <div class=\"check-item pass\">\n      <h4>CLAUDE.md</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>Repo-level context with factory operating model, protected files, quick commands</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>/specs/ (6 files)</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>system.md, env.md, rl.md, training.md, dashboard.md, proof.md</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>/scenarios/ (12 files)</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>5 environment, 2 training, 1 pipeline, 1 dashboard, 3 integration</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>run_scenarios.py</h4>\n      <div class=\"status pass\">COMPLETE \u2014 passes ruff + mypy</div>\n      <p>Parses scenarios, executes eval commands, produces JSON report with satisfaction score</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>compile_feedback.py</h4>\n      <div class=\"status pass\">COMPLETE \u2014 passes ruff + mypy</div>\n      <p>Reads results + CI logs, infers root causes, produces structured feedback markdown</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>factory.yaml workflow</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>Convergence loop with FS shuffle, Codex invocation, PR comments, stall detection</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>factory_fix.md prompt</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>Codex instructions: read specs, read feedback, fix priorities, protected file list</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>Makefile targets</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>run-scenarios, compile-feedback, factory-local, factory-status</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>docs/dark_factory.md</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>Full operating manual: architecture, scenario writing, feedback reading, escalation</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>ProjectLeadAsks.md</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>OpenAI API key, satisfaction threshold, branch strategy, token budget</p>\n    </div>\n  </div>\n\n  <h2 class=\"section-title\">Smoke Test Results \u2014 Iteration 1 (main branch)</h2>\n  <p style=\"color:var(--muted);margin-bottom:16px\">Expected: most scenarios fail because the MiniPong code is on the Codex branch, not main. This proves the factory detects missing functionality correctly.</p>\n\n  <div class=\"check-grid\">\n    <div class=\"check-item fail\">\n      <h4>Environment Determinism</h4>\n      <div class=\"status fail\">FAIL \u2014 ModuleNotFoundError</div>\n      <p>src.envs.minipong doesn't exist on main (expected)</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Observation Space</h4>\n      <div class=\"status fail\">FAIL \u2014 ModuleNotFoundError</div>\n      <p>Same root cause \u2014 no MiniPong env on main</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Reward Structure</h4>\n      <div class=\"status fail\">FAIL \u2014 ModuleNotFoundError</div>\n      <p>Same root cause</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Rendering</h4>\n      <div class=\"status fail\">FAIL \u2014 ModuleNotFoundError</div>\n      <p>Same root cause</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Info Dict</h4>\n      <div class=\"status fail\">FAIL \u2014 ModuleNotFoundError</div>\n      <p>Same root cause</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Training Artifacts</h4>\n      <div class=\"status fail\">FAIL \u2014 No artifacts</div>\n      <p>Training pipeline not on main</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Eval Videos</h4>\n      <div class=\"status fail\">FAIL \u2014 No videos</div>\n      <p>Training pipeline not on main</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>verify-learning cmd</h4>\n      <div class=\"status pass\">PASS</div>\n      <p>Command exists and runs (with relaxed thresholds)</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>Dashboard Loads</h4>\n      <div class=\"status pass\">PASS</div>\n      <p>Module is importable</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>env-smoke</h4>\n      <div class=\"status pass\">PASS</div>\n      <p>Makefile target works (existing atari env)</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>Lint + Typecheck</h4>\n      <div class=\"status pass\">PASS</div>\n      <p>Codebase passes ruff and mypy</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>Full CPU Pipeline</h4>\n      <div class=\"status pass\">PASS</div>\n      <p>Pipeline commands exist (with relaxed thresholds)</p>\n    </div>\n  </div>\n\n  <div style=\"background:var(--surface);border:1px solid var(--border);border-radius:12px;padding:24px;margin-top:24px\">\n    <h3 style=\"margin-bottom:12px\">Satisfaction: 42% (5/12) \u2014 Expected for main branch</h3>\n    <p style=\"color:var(--muted);font-size:14px\">\n      The 7 failures are all because the MiniPong code from PR #4 hasn't been merged yet.\n      When the factory runs against the Codex branch, these should start passing \u2014 and the factory loop will iterate until 80%+ are satisfied.\n    </p>\n    <p style=\"color:var(--muted);font-size:14px;margin-top:8px\">\n      <strong>Feedback compiler correctly identified:</strong> 5 import errors (missing module), 2 assertion failures (missing artifacts).\n      Root cause inference is working.\n    </p>\n  </div>\n</div>\n\n<!-- \u2500\u2500\u2500 CLAUDE CODE AS ORCHESTRATOR VIEW \u2500\u2500\u2500 -->\n<div class=\"view\" id=\"view-orchestrator\">\n  <h2 class=\"section-title\">Claude Code as Orchestrator \u2014 Hybrid Architecture</h2>\n  <p style=\"color:var(--muted);margin-bottom:24px\">Claude Code runs the convergence loop, invoking Codex via browser. Holdout isolation via branch stripping (not filesystem shuffle). CI handles deterministic validation.</p>\n\n  <div class=\"arch-flow\">\n\n    <div class=\"arch-node human\">\n      <h3>HUMAN \u2014 Project Lead</h3>\n      <p>Authors /specs/ and /scenarios/. Says \"run a factory crank\" to Claude Code. Reviews the PR when the factory converges.</p>\n    </div>\n    <div class=\"arrow\"></div>\n\n    <div class=\"arch-node claude\">\n      <h3>CLAUDE CODE \u2014 Orchestrator</h3>\n      <p>Skill: <code>/factory-orchestrate</code> \u2014 runs the full convergence loop</p>\n      <div class=\"sub-steps\">\n        <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--claude)\">Creates</div>Branch: df-crank-vXX-{descriptor}</div>\n        <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--claude)\">Strips</div>python scripts/strip_holdout.py</div>\n      </div>\n    </div>\n    <div class=\"arrow\"></div>\n\n    <div class=\"loop-bracket\">\n      <div class=\"loop-label\">CONVERGENCE LOOP (up to N iterations)</div>\n\n      <div class=\"arch-node codex\">\n        <h3>CODEX (Attractor) \u2014 via Browser</h3>\n        <p>Codex UI (ChatGPT Plus). Given: stripped branch (no /scenarios/). Creates its own codex-... branch. Reads /specs/ + feedback.</p>\n        <div class=\"sub-steps\">\n          <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--codex)\">Base Branch</div>df-crank-vXX (stripped \u2014 no scenarios)</div>\n          <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--codex)\">Creates</div>codex-... branch with fixes</div>\n        </div>\n      </div>\n      <div class=\"arrow\" style=\"margin: 0 auto\"></div>\n\n      <div class=\"arch-node claude\" style=\"border-color:var(--holdout)\">\n        <h3 style=\"color:var(--holdout)\">GATE 0: Adversarial Review</h3>\n        <p>Claude Code reviews Codex diff adversarially. Stam tests, gaming, architectural dishonesty, spec violations. CRITICAL \u2192 back to Codex. Clean \u2192 merge.</p>\n      </div>\n      <div class=\"arrow\" style=\"margin: 0 auto\"></div>\n\n      <div class=\"arch-node factory\" style=\"margin-bottom:0\">\n        <h3>VALIDATION GATES (after merge + holdout restore)</h3>\n        <div class=\"sub-steps\" style=\"grid-template-columns:1fr 1fr 1fr\">\n          <div class=\"sub-step l1\"><div class=\"label\">Gate 1: Deterministic CI</div>make lint + typecheck + test<br><em>(full suite incl. attractor tests)</em><br><em>If fail \u2192 feedback \u2192 Codex</em></div>\n          <div class=\"sub-step l15\"><div class=\"label\">Gate 2: NFRs ('ilities')</div>ruff extended, radon, vulture, bandit<br><em>Non-blocking \u2014 feeds LLM judge</em><br><em>Extensible: add checks as needed</em></div>\n          <div class=\"sub-step l2\"><div class=\"label\">Gate 3: Behavioral Scenarios</div>scripts/run_scenarios.py<br><em>12 holdout scenarios</em><br><em>Produces satisfaction score</em></div>\n        </div>\n      </div>\n      <div class=\"arrow colored\" style=\"margin: 0 auto\"></div>\n\n      <div class=\"arch-node claude\">\n        <h3>LLM-as-JUDGE \u2014 Holistic Evaluation</h3>\n        <p>Claude Code reasons through ALL outputs: satisfaction trajectory, failure patterns, fix quality, Gate 2 NFR findings, systemic issues. Not just \"score &ge; threshold.\"</p>\n      </div>\n      <div class=\"arrow colored\" style=\"margin: 0 auto\"></div>\n\n      <div class=\"arch-node feedback\" style=\"flex:1\">\n        <h3>FEEDBACK COMPILER</h3>\n        <p>scripts/compile_feedback.py + Claude Code's holistic assessment \u2192 enriched feedback for next iteration</p>\n      </div>\n    </div>\n\n    <div class=\"arrow colored\"></div>\n    <div class=\"arch-node factory\" style=\"border-style:dashed\">\n      <h3>EXIT \u2192 ACCEPT/MERGE GATE</h3>\n      <p>LLM judge satisfied \u2192 <code>gh pr create</code> with factory-converged label. Human reviews satisfaction report, NFR status, and merges.</p>\n    </div>\n\n    <div style=\"background:rgba(63,185,80,0.1);border:1px dashed var(--codex);border-radius:8px;padding:16px;margin-top:16px;max-width:840px;width:100%\">\n      <h4 style=\"color:var(--codex);font-size:14px;margin-bottom:8px\">Holdout Isolation \u2014 Branch Stripping (replaces filesystem shuffle)</h4>\n      <pre style=\"font-size:12px;color:var(--muted);white-space:pre-wrap\">Before Codex:   python scripts/strip_holdout.py     (scenarios removed from branch entirely)\nCodex works:    codex-... branch has NO /scenarios/  (hard gate \u2014 files don't exist)\nAfter Codex:    python scripts/restore_holdout.py    (scenarios restored from origin/main)\n\nWhy: Filesystem shuffle (/tmp/) was security theater \u2014 Codex could read /tmp/.\nBranch stripping is a real gate. Scenarios are not on the branch Codex sees.</pre>\n    </div>\n\n    <div style=\"background:rgba(210,168,255,0.1);border:1px dashed var(--claude);border-radius:8px;padding:16px;margin-top:16px;max-width:840px;width:100%\">\n      <h4 style=\"color:var(--claude);font-size:14px;margin-bottom:8px\">Branch Flow</h4>\n      <pre style=\"font-size:12px;color:var(--muted);white-space:pre-wrap\">factory/v1 (base)\n  \u2514\u2500 df-crank-v01-minipong (factory branch, created by Claude Code)\n       \u251c\u2500 [stripped] scenarios removed, pushed\n       \u251c\u2500 codex-abc123 (Codex creates, works here)\n       \u251c\u2500 [merged] Codex changes merged after Gate 0\n       \u251c\u2500 [restored] scenarios restored from origin/main\n       \u251c\u2500 [validated] Gates 1-3 + LLM judge\n       \u2514\u2500 PR \u2192 main (accept/merge gate)</pre>\n    </div>\n  </div>\n</div>\n\n<script>\nfunction showView(id) {\n  document.querySelectorAll('.view').forEach(v => v.classList.remove('active'));\n  document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));\n  document.getElementById('view-' + id).classList.add('active');\n  event.target.classList.add('active');\n}\n</script>\n</body>\n</html>\n",
      "base": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n<title>Dark Factory Architecture \u2014 MiniPong</title>\n<style>\n  :root {\n    --bg: #0d1117;\n    --surface: #161b22;\n    --border: #30363d;\n    --text: #e6edf3;\n    --muted: #8b949e;\n    --human: #f0883e;\n    --factory: #58a6ff;\n    --codex: #3fb950;\n    --holdout: #f85149;\n    --feedback: #bc8cff;\n    --pass: #3fb950;\n    --fail: #f85149;\n  }\n  * { margin: 0; padding: 0; box-sizing: border-box; }\n  body { background: var(--bg); color: var(--text); font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; line-height: 1.6; }\n\n  .header { text-align: center; padding: 40px 20px 20px; }\n  .header h1 { font-size: 28px; font-weight: 600; }\n  .header p { color: var(--muted); margin-top: 8px; font-size: 15px; }\n\n  .tabs { display: flex; justify-content: center; gap: 8px; padding: 20px; flex-wrap: wrap; }\n  .tab { padding: 10px 24px; border-radius: 8px; border: 1px solid var(--border); background: var(--surface); color: var(--muted); cursor: pointer; font-size: 14px; transition: all 0.2s; }\n  .tab:hover { border-color: var(--factory); color: var(--text); }\n  .tab.active { background: var(--factory); color: #fff; border-color: var(--factory); }\n\n  .view { max-width: 1100px; margin: 0 auto; padding: 20px; display: none; }\n  .view.active { display: block; }\n\n  /* Architecture Diagram */\n  .arch-flow { display: flex; flex-direction: column; gap: 0; align-items: center; }\n  .arch-node { background: var(--surface); border: 2px solid var(--border); border-radius: 12px; padding: 24px; width: 100%; max-width: 800px; position: relative; }\n  .arch-node h3 { font-size: 16px; margin-bottom: 8px; display: flex; align-items: center; gap: 8px; }\n  .arch-node p { color: var(--muted); font-size: 14px; }\n  .arch-node.human { border-color: var(--human); }\n  .arch-node.human h3 { color: var(--human); }\n  .arch-node.factory { border-color: var(--factory); }\n  .arch-node.factory h3 { color: var(--factory); }\n  .arch-node.codex { border-color: var(--codex); }\n  .arch-node.codex h3 { color: var(--codex); }\n  .arch-node.holdout { border-color: var(--holdout); }\n  .arch-node.holdout h3 { color: var(--holdout); }\n  .arch-node.feedback { border-color: var(--feedback); }\n  .arch-node.feedback h3 { color: var(--feedback); }\n\n  .arrow { width: 2px; height: 40px; background: var(--border); position: relative; }\n  .arrow::after { content: ''; position: absolute; bottom: -6px; left: -5px; width: 0; height: 0; border-left: 6px solid transparent; border-right: 6px solid transparent; border-top: 8px solid var(--border); }\n  .arrow.colored { background: var(--feedback); }\n  .arrow.colored::after { border-top-color: var(--feedback); }\n\n  .loop-bracket { border: 2px dashed var(--feedback); border-radius: 16px; padding: 16px; margin: -8px 0; width: 100%; max-width: 840px; position: relative; }\n  .loop-label { position: absolute; top: -12px; left: 24px; background: var(--bg); padding: 0 8px; color: var(--feedback); font-size: 13px; font-weight: 600; }\n\n  .sub-steps { display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-top: 16px; }\n  .sub-step { background: rgba(255,255,255,0.03); border-radius: 8px; padding: 12px; font-size: 13px; }\n  .sub-step .label { font-weight: 600; font-size: 12px; text-transform: uppercase; letter-spacing: 0.5px; margin-bottom: 4px; }\n  .sub-step.l1 .label { color: var(--factory); }\n  .sub-step.l2 .label { color: var(--holdout); }\n\n  /* Wiring Table */\n  .wiring-table { width: 100%; border-collapse: collapse; margin-top: 20px; }\n  .wiring-table th { text-align: left; padding: 12px 16px; background: var(--surface); border-bottom: 2px solid var(--border); font-size: 13px; color: var(--muted); text-transform: uppercase; letter-spacing: 0.5px; }\n  .wiring-table td { padding: 12px 16px; border-bottom: 1px solid var(--border); font-size: 14px; vertical-align: top; }\n  .wiring-table tr:hover td { background: rgba(255,255,255,0.02); }\n\n  .badge { display: inline-block; padding: 2px 8px; border-radius: 12px; font-size: 11px; font-weight: 600; }\n  .badge.human { background: rgba(240,136,62,0.15); color: var(--human); }\n  .badge.factory { background: rgba(88,166,255,0.15); color: var(--factory); }\n  .badge.codex { background: rgba(63,185,80,0.15); color: var(--codex); }\n  .badge.holdout { background: rgba(248,81,73,0.15); color: var(--holdout); }\n\n  /* Human Interaction Points */\n  .interaction-card { background: var(--surface); border: 1px solid var(--border); border-radius: 12px; padding: 24px; margin-bottom: 16px; }\n  .interaction-card h3 { color: var(--human); font-size: 16px; margin-bottom: 8px; }\n  .interaction-card .when { color: var(--muted); font-size: 13px; margin-bottom: 12px; font-style: italic; }\n  .interaction-card .action { font-size: 14px; }\n  .interaction-card code { background: rgba(255,255,255,0.05); padding: 2px 6px; border-radius: 4px; font-size: 13px; }\n\n  /* Validation Checklist */\n  .check-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 12px; }\n  .check-item { background: var(--surface); border: 1px solid var(--border); border-radius: 8px; padding: 16px; }\n  .check-item.pass { border-color: var(--pass); }\n  .check-item.fail { border-color: var(--fail); }\n  .check-item h4 { font-size: 14px; margin-bottom: 4px; }\n  .check-item .status { font-size: 13px; font-weight: 600; }\n  .check-item .status.pass { color: var(--pass); }\n  .check-item .status.fail { color: var(--fail); }\n  .check-item p { color: var(--muted); font-size: 13px; margin-top: 4px; }\n\n  .section-title { font-size: 20px; font-weight: 600; margin: 32px 0 16px; padding-bottom: 8px; border-bottom: 1px solid var(--border); }\n\n  .legend { display: flex; gap: 24px; justify-content: center; padding: 16px; flex-wrap: wrap; }\n  .legend-item { display: flex; align-items: center; gap: 6px; font-size: 13px; color: var(--muted); }\n  .legend-dot { width: 12px; height: 12px; border-radius: 50%; }\n\n  .fs-shuffle { background: rgba(248,81,73,0.1); border: 1px dashed var(--holdout); border-radius: 8px; padding: 16px; margin-top: 16px; }\n  .fs-shuffle h4 { color: var(--holdout); font-size: 14px; margin-bottom: 8px; }\n  .fs-shuffle pre { font-size: 12px; color: var(--muted); white-space: pre-wrap; }\n</style>\n</head>\n<body>\n\n<div class=\"header\">\n  <h1>Dark Factory Architecture</h1>\n  <p>MiniPong RL System \u2014 Convergence Loop Infrastructure</p>\n</div>\n\n<div class=\"legend\">\n  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--human)\"></div> Human (Joey)</div>\n  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--factory)\"></div> Factory Orchestrator</div>\n  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--codex)\"></div> Codex (Attractor)</div>\n  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--holdout)\"></div> Holdout Scenarios</div>\n  <div class=\"legend-item\"><div class=\"legend-dot\" style=\"background:var(--feedback)\"></div> Feedback Loop</div>\n</div>\n\n<div class=\"tabs\">\n  <div class=\"tab active\" onclick=\"showView('architecture')\">Architecture</div>\n  <div class=\"tab\" onclick=\"showView('wiring')\">Component Wiring</div>\n  <div class=\"tab\" onclick=\"showView('human')\">Human Interaction</div>\n  <div class=\"tab\" onclick=\"showView('validation')\">Validation Proof</div>\n</div>\n\n<!-- \u2500\u2500\u2500 ARCHITECTURE VIEW \u2500\u2500\u2500 -->\n<div class=\"view active\" id=\"view-architecture\">\n  <div class=\"arch-flow\">\n\n    <div class=\"arch-node human\">\n      <h3>HUMAN \u2014 Joey</h3>\n      <p>Authors specs (/specs/*.md) and scenarios (/scenarios/*.md). Sets satisfaction threshold. Triggers factory. Escalates when loop stalls.</p>\n      <div class=\"sub-steps\">\n        <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--human)\">Writes</div>/specs/ \u2014 what the system should do</div>\n        <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--human)\">Writes</div>/scenarios/ \u2014 holdout evaluation criteria</div>\n      </div>\n    </div>\n    <div class=\"arrow\"></div>\n\n    <div class=\"loop-bracket\">\n      <div class=\"loop-label\">CONVERGENCE LOOP (up to N iterations)</div>\n\n      <div class=\"arch-node factory\" style=\"margin-bottom:0\">\n        <h3>FACTORY ORCHESTRATOR</h3>\n        <p>.github/workflows/factory.yaml \u2014 runs the loop, controls iteration</p>\n        <div class=\"sub-steps\">\n          <div class=\"sub-step l1\"><div class=\"label\">Layer 1: Deterministic CI</div>make lint + typecheck + test<br><em>If fail \u2192 skip Layer 2, go to feedback</em></div>\n          <div class=\"sub-step l2\"><div class=\"label\">Layer 2: Behavioral Scenarios</div>scripts/run_scenarios.py<br><em>12 holdout scenarios, satisfaction scoring</em></div>\n        </div>\n      </div>\n      <div class=\"arrow\" style=\"margin: 0 auto\"></div>\n\n      <div style=\"display:flex;gap:16px;width:100%;max-width:800px;margin:0 auto\">\n        <div class=\"arch-node holdout\" style=\"flex:1\">\n          <h3>HOLDOUT SCENARIOS</h3>\n          <p>/scenarios/*.md \u2014 Codex NEVER sees these. Filesystem shuffle hides them during Codex execution.</p>\n        </div>\n        <div class=\"arch-node feedback\" style=\"flex:1\">\n          <h3>FEEDBACK COMPILER</h3>\n          <p>scripts/compile_feedback.py \u2192 artifacts/factory/feedback_iter_N.md</p>\n        </div>\n      </div>\n      <div class=\"arrow colored\" style=\"margin: 0 auto\"></div>\n\n      <div class=\"arch-node codex\">\n        <h3>CODEX (Attractor)</h3>\n        <p>openai/codex-action@v1 \u2014 reads /specs/ + feedback, writes code fixes. Non-interactive. Commits directly.</p>\n        <div class=\"sub-steps\">\n          <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--codex)\">Reads</div>/specs/*.md + feedback_iter_N.md</div>\n          <div class=\"sub-step\"><div class=\"label\" style=\"color:var(--codex)\">Writes</div>src/, tests/, configs/, Makefile</div>\n        </div>\n      </div>\n    </div>\n\n    <div class=\"arrow colored\"></div>\n    <div class=\"arch-node factory\" style=\"border-style:dashed\">\n      <h3>EXIT CONDITIONS</h3>\n      <p>Satisfaction threshold met (default 80%) \u2192 post PR comment, done.<br>Max iterations reached \u2192 tag @joeyfezster for escalation.</p>\n    </div>\n\n    <div class=\"fs-shuffle\">\n      <h4>Filesystem Shuffle \u2014 Scenario Isolation</h4>\n      <pre>Before Codex runs:  mv /scenarios/ /tmp/factory_scenarios    (scenarios disappear)\nCodex executes:     workspace has NO /scenarios/ directory    (hard gate)\nAfter Codex:        cp -r /tmp/factory_scenarios /scenarios/  (scenarios restored)\n\nWhy: .codexignore doesn't exist. This is a filesystem-level gate, not an LLM instruction.</pre>\n    </div>\n  </div>\n</div>\n\n<!-- \u2500\u2500\u2500 WIRING VIEW \u2500\u2500\u2500 -->\n<div class=\"view\" id=\"view-wiring\">\n  <h2 class=\"section-title\">Component Wiring \u2014 How Each File Maps to the Factory</h2>\n  <table class=\"wiring-table\">\n    <thead>\n      <tr><th>File</th><th>Owner</th><th>Dark Factory Role</th><th>Reads From</th><th>Writes To</th></tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td><code>/specs/*.md</code></td>\n        <td><span class=\"badge human\">Human</span></td>\n        <td>Source of truth \u2014 what the system should do. Codex reads these as requirements.</td>\n        <td>\u2014</td>\n        <td>Codex prompt context</td>\n      </tr>\n      <tr>\n        <td><code>/scenarios/*.md</code></td>\n        <td><span class=\"badge holdout\">Holdout</span></td>\n        <td>Behavioral holdout set \u2014 ML-style validation split. Agent never sees evaluation criteria.</td>\n        <td>\u2014</td>\n        <td>run_scenarios.py</td>\n      </tr>\n      <tr>\n        <td><code>scripts/run_scenarios.py</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Scenario evaluation engine. Parses scenario markdown, executes eval commands, produces JSON report.</td>\n        <td>/scenarios/*.md</td>\n        <td>artifacts/factory/scenario_results.json</td>\n      </tr>\n      <tr>\n        <td><code>scripts/compile_feedback.py</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Feedback generation. Reads scenario results + CI logs, infers root causes, produces structured feedback for Codex.</td>\n        <td>scenario_results.json, ci_output.log, previous feedback</td>\n        <td>artifacts/factory/feedback_iter_N.md</td>\n      </tr>\n      <tr>\n        <td><code>.github/workflows/factory.yaml</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Orchestrator. Implements the convergence loop: validate \u2192 scenarios \u2192 threshold check \u2192 feedback \u2192 Codex \u2192 repeat.</td>\n        <td>All of the above</td>\n        <td>PR comments, iteration state</td>\n      </tr>\n      <tr>\n        <td><code>.github/codex/prompts/factory_fix.md</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Codex prompt template. Instructions + constraints for each iteration. Lists protected files, fix priorities.</td>\n        <td>\u2014</td>\n        <td>Codex context</td>\n      </tr>\n      <tr>\n        <td><code>artifacts/factory/feedback_iter_N.md</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Iteration feedback. Full error output, inferred causes, convergence trajectory. This is what Codex reads to fix things.</td>\n        <td>compile_feedback.py</td>\n        <td>Codex context</td>\n      </tr>\n      <tr>\n        <td><code>CLAUDE.md</code></td>\n        <td><span class=\"badge factory\">Factory</span></td>\n        <td>Repo-level context for Claude Code (escalation path). Documents the factory model, protected files, quick commands.</td>\n        <td>\u2014</td>\n        <td>Claude Code context</td>\n      </tr>\n      <tr>\n        <td><code>src/**</code></td>\n        <td><span class=\"badge codex\">Codex</span></td>\n        <td>Product code. The \"opaque weights\" \u2014 correctness inferred from behavior, not inspection.</td>\n        <td>specs</td>\n        <td>Evaluated by scenarios</td>\n      </tr>\n      <tr>\n        <td><code>Makefile</code></td>\n        <td><span class=\"badge codex\">Codex</span> / <span class=\"badge factory\">Factory</span></td>\n        <td>Build interface. Factory targets (run-scenarios, compile-feedback, factory-local) + product targets (lint, test, train-smoke).</td>\n        <td>\u2014</td>\n        <td>\u2014</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n\n<!-- \u2500\u2500\u2500 HUMAN INTERACTION VIEW \u2500\u2500\u2500 -->\n<div class=\"view\" id=\"view-human\">\n  <h2 class=\"section-title\">Human Interaction Points</h2>\n  <p style=\"color:var(--muted);margin-bottom:24px\">The factory is designed to run autonomously. Joey's involvement is limited to five distinct touchpoints:</p>\n\n  <div class=\"interaction-card\">\n    <h3>1. Author Specs</h3>\n    <div class=\"when\">Before the factory runs \u2014 defines intent</div>\n    <div class=\"action\">\n      Write or update files in <code>/specs/</code>. These are the requirements Codex reads. The specs define <em>what</em> the system should do.\n      No code. No implementation details. Just behavioral contracts.\n    </div>\n  </div>\n\n  <div class=\"interaction-card\">\n    <h3>2. Author Scenarios</h3>\n    <div class=\"when\">Before the factory runs \u2014 defines success criteria</div>\n    <div class=\"action\">\n      Write or update files in <code>/scenarios/</code>. Each scenario is a behavioral expectation with a runnable evaluation command.\n      These are the holdout set \u2014 Codex never sees them. This is how you define \"done\" without reviewing code.\n    </div>\n  </div>\n\n  <div class=\"interaction-card\">\n    <h3>3. Trigger the Factory</h3>\n    <div class=\"when\">When specs and scenarios are ready</div>\n    <div class=\"action\">\n      GitHub Actions UI: <code>Actions \u2192 Dark Factory \u2192 Run workflow</code><br>\n      Or CLI: <code>gh workflow run factory.yaml -f max_iterations=5 -f satisfaction_threshold=0.80</code><br>\n      Or: push to a <code>factory/*</code> branch.\n    </div>\n  </div>\n\n  <div class=\"interaction-card\">\n    <h3>4. Review Satisfaction Report</h3>\n    <div class=\"when\">After the factory converges or stalls</div>\n    <div class=\"action\">\n      The factory posts a PR comment with the satisfaction score and iteration count.\n      If converged: review the score, decide if it's good enough. You don't read the code.\n      If stalled: read <code>artifacts/factory/feedback_iter_N.md</code> to understand what's stuck.\n    </div>\n  </div>\n\n  <div class=\"interaction-card\">\n    <h3>5. Escalate to Interactive Debug</h3>\n    <div class=\"when\">When the factory stalls for 3+ iterations with no improvement</div>\n    <div class=\"action\">\n      Open the repo in Claude Code (desktop). It reads <code>CLAUDE.md</code> automatically.\n      Point it at the latest feedback file. Claude Code does interactive debugging \u2014 the thing the factory loop can't do.\n      Fix the issue, push, let the factory resume.\n    </div>\n  </div>\n\n  <div style=\"background:var(--surface);border:1px solid var(--border);border-radius:12px;padding:24px;margin-top:24px\">\n    <h3 style=\"color:var(--factory);margin-bottom:12px\">What Joey Does NOT Do</h3>\n    <ul style=\"color:var(--muted);font-size:14px;padding-left:20px\">\n      <li>Read diffs or code changes</li>\n      <li>Review PRs for code quality</li>\n      <li>Debug test failures manually</li>\n      <li>Write any production code</li>\n      <li>Approve individual Codex commits</li>\n    </ul>\n    <p style=\"margin-top:12px;font-size:14px\">Code is opaque weights. Correctness is inferred from behavior. This is the validation constraint.</p>\n  </div>\n</div>\n\n<!-- \u2500\u2500\u2500 VALIDATION VIEW \u2500\u2500\u2500 -->\n<div class=\"view\" id=\"view-validation\">\n  <h2 class=\"section-title\">Factory Validation \u2014 What's Built and Proven</h2>\n  <p style=\"color:var(--muted);margin-bottom:24px\">Current status: factory infrastructure complete. Running against main branch (no MiniPong code yet).</p>\n\n  <div class=\"check-grid\">\n    <div class=\"check-item pass\">\n      <h4>CLAUDE.md</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>Repo-level context with factory operating model, protected files, quick commands</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>/specs/ (6 files)</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>system.md, env.md, rl.md, training.md, dashboard.md, proof.md</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>/scenarios/ (12 files)</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>5 environment, 2 training, 1 pipeline, 1 dashboard, 3 integration</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>run_scenarios.py</h4>\n      <div class=\"status pass\">COMPLETE \u2014 passes ruff + mypy</div>\n      <p>Parses scenarios, executes eval commands, produces JSON report with satisfaction score</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>compile_feedback.py</h4>\n      <div class=\"status pass\">COMPLETE \u2014 passes ruff + mypy</div>\n      <p>Reads results + CI logs, infers root causes, produces structured feedback markdown</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>factory.yaml workflow</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>Convergence loop with FS shuffle, Codex invocation, PR comments, stall detection</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>factory_fix.md prompt</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>Codex instructions: read specs, read feedback, fix priorities, protected file list</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>Makefile targets</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>run-scenarios, compile-feedback, factory-local, factory-status</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>docs/dark_factory.md</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>Full operating manual: architecture, scenario writing, feedback reading, escalation</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>ProjectLeadAsks.md</h4>\n      <div class=\"status pass\">COMPLETE</div>\n      <p>OpenAI API key, satisfaction threshold, branch strategy, token budget</p>\n    </div>\n  </div>\n\n  <h2 class=\"section-title\">Smoke Test Results \u2014 Iteration 1 (main branch)</h2>\n  <p style=\"color:var(--muted);margin-bottom:16px\">Expected: most scenarios fail because the MiniPong code is on the Codex branch, not main. This proves the factory detects missing functionality correctly.</p>\n\n  <div class=\"check-grid\">\n    <div class=\"check-item fail\">\n      <h4>Environment Determinism</h4>\n      <div class=\"status fail\">FAIL \u2014 ModuleNotFoundError</div>\n      <p>src.envs.minipong doesn't exist on main (expected)</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Observation Space</h4>\n      <div class=\"status fail\">FAIL \u2014 ModuleNotFoundError</div>\n      <p>Same root cause \u2014 no MiniPong env on main</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Reward Structure</h4>\n      <div class=\"status fail\">FAIL \u2014 ModuleNotFoundError</div>\n      <p>Same root cause</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Rendering</h4>\n      <div class=\"status fail\">FAIL \u2014 ModuleNotFoundError</div>\n      <p>Same root cause</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Info Dict</h4>\n      <div class=\"status fail\">FAIL \u2014 ModuleNotFoundError</div>\n      <p>Same root cause</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Training Artifacts</h4>\n      <div class=\"status fail\">FAIL \u2014 No artifacts</div>\n      <p>Training pipeline not on main</p>\n    </div>\n    <div class=\"check-item fail\">\n      <h4>Eval Videos</h4>\n      <div class=\"status fail\">FAIL \u2014 No videos</div>\n      <p>Training pipeline not on main</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>verify-learning cmd</h4>\n      <div class=\"status pass\">PASS</div>\n      <p>Command exists and runs (with relaxed thresholds)</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>Dashboard Loads</h4>\n      <div class=\"status pass\">PASS</div>\n      <p>Module is importable</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>env-smoke</h4>\n      <div class=\"status pass\">PASS</div>\n      <p>Makefile target works (existing atari env)</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>Lint + Typecheck</h4>\n      <div class=\"status pass\">PASS</div>\n      <p>Codebase passes ruff and mypy</p>\n    </div>\n    <div class=\"check-item pass\">\n      <h4>Full CPU Pipeline</h4>\n      <div class=\"status pass\">PASS</div>\n      <p>Pipeline commands exist (with relaxed thresholds)</p>\n    </div>\n  </div>\n\n  <div style=\"background:var(--surface);border:1px solid var(--border);border-radius:12px;padding:24px;margin-top:24px\">\n    <h3 style=\"margin-bottom:12px\">Satisfaction: 42% (5/12) \u2014 Expected for main branch</h3>\n    <p style=\"color:var(--muted);font-size:14px\">\n      The 7 failures are all because the MiniPong code from PR #4 hasn't been merged yet.\n      When the factory runs against the Codex branch, these should start passing \u2014 and the factory loop will iterate until 80%+ are satisfied.\n    </p>\n    <p style=\"color:var(--muted);font-size:14px;margin-top:8px\">\n      <strong>Feedback compiler correctly identified:</strong> 5 import errors (missing module), 2 assertion failures (missing artifacts).\n      Root cause inference is working.\n    </p>\n  </div>\n</div>\n\n<script>\nfunction showView(id) {\n  document.querySelectorAll('.view').forEach(v => v.classList.remove('active'));\n  document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));\n  document.getElementById('view-' + id).classList.add('active');\n  event.target.classList.add('active');\n}\n</script>\n</body>\n</html>\n"
    },
    "docs/factory_validation_strategy.md": {
      "additions": 89,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/factory_validation_strategy.md b/docs/factory_validation_strategy.md\nnew file mode 100644\nindex 0000000..3d1e9bc\n--- /dev/null\n+++ b/docs/factory_validation_strategy.md\n@@ -0,0 +1,89 @@\n+# Factory Validation Strategy\n+\n+How we validate the factory's own components and their integration.\n+\n+## Component Inventory\n+\n+| Component | Type | Location | Validated By |\n+|-----------|------|----------|-------------|\n+| Scenario runner | Script | `scripts/run_scenarios.py` | `tests/test_factory_run_scenarios.py` + lint + typecheck |\n+| Feedback compiler | Script | `scripts/compile_feedback.py` | `tests/test_factory_compile_feedback.py` + lint + typecheck |\n+| Factory orchestrator | Workflow | `.github/workflows/factory.yaml` | CI dry-run + manual trigger |\n+| Attractor prompt | Markdown | `.github/codex/prompts/factory_fix.md` | Human review + adversarial review |\n+| Adversarial review | Markdown | `.github/codex/prompts/adversarial_review.md` | Human review |\n+| Satisfaction dashboard | Streamlit | `src/dashboard/pages/factory.py` | lint + manual verification |\n+| Scenario files | Markdown | `scenarios/*.md` | Format validator in CI |\n+| Factory CI | Workflow | `.github/workflows/ci.yaml` (factory-self-test job) | Self-referential (runs on itself) |\n+\n+## Validation Layers\n+\n+### Layer 1: Static Analysis (Automated, Every Push)\n+- **ruff check** on all factory Python scripts\n+- **mypy** on factory scripts (with `--ignore-missing-imports`)\n+- **Scenario format validator** \u2014 ensures all `.md` files in `/scenarios/` have required sections\n+- **Factory integrity check** \u2014 verifies protected files exist and are referenced in CLAUDE.md\n+\n+### Layer 2: Unit Tests (Automated, Every Push)\n+- `test_factory_run_scenarios.py` \u2014 19 tests covering:\n+  - Scenario markdown parsing (7 tests)\n+  - Scenario execution with real bash commands (11 tests)\n+  - Dataclass serialization (1 test)\n+- `test_factory_compile_feedback.py` \u2014 29 tests covering:\n+  - Results loading and missing file handling (3 tests)\n+  - CI log loading and truncation (3 tests)\n+  - Iteration counting (3 tests)\n+  - Feedback history loading (3 tests)\n+  - Root cause inference from error patterns (7 tests)\n+  - Feedback compilation output structure (9 tests)\n+  - End-to-end integration (1 test)\n+\n+### Layer 3: Adversarial Review (Manual, Per PR)\n+- Review guidelines at `.github/codex/prompts/adversarial_review.md`\n+- Checks for: stam tests, gaming, architectural dishonesty, spec violations\n+- Applied to attractor output (product code), not to factory infrastructure\n+\n+### Layer 4: Integration Testing (Manual, Pre-Factory-Crank)\n+- `make factory-local` \u2014 runs one full iteration without Codex\n+- Verifies: scenario discovery \u2192 execution \u2192 result JSON \u2192 feedback compilation\n+- This is the integration test for the factory pipeline\n+\n+## What's NOT Tested (Known Gaps)\n+\n+| Gap | Risk | Mitigation | When to Fix |\n+|-----|------|-----------|-------------|\n+| factory.yaml workflow logic | Can't unit test GH Actions | Manual trigger + review | Post-first-crank |\n+| Codex invocation | Browser automation via ChatGPT Plus | Test browser path first; API key is fallback | First crank |\n+| Branch stripping | strip/restore scripts tested; regex edge cases possible | Adversarial review + tests | Post-first-crank |\n+| Dashboard data display | Needs Streamlit runtime | Manual `make dashboard` | Post-first-crank |\n+| Adversarial review effectiveness | New, unproven | Track false negative rate | Ongoing |\n+| Factory workflow concurrency | Multiple factory runs on same branch | Lock via GH concurrency groups | Post-first-crank |\n+\n+## Test Hygiene Rules\n+\n+### For Factory Tests\n+1. **No mocking factory internals.** Tests import and exercise real functions.\n+2. **Use `tmp_path` fixtures** for file operations \u2014 never touch the real filesystem.\n+3. **Real bash execution** in scenario runner tests \u2014 subprocess.run, not mocked.\n+4. **Every assertion tests a meaningful property** \u2014 not just \"doesn't crash.\"\n+\n+### For Product Tests (Enforced by Anti-Gaming Rules)\n+1. No mocking the system under test.\n+2. No stub implementations (`return True`, `pass`, hardcoded lookup tables).\n+3. No patching away tested logic.\n+4. No test-only config shortcuts that trivialize behavior.\n+\n+## How to Run Factory Validation Locally\n+\n+```bash\n+# Full factory self-test\n+ruff check scripts/run_scenarios.py scripts/compile_feedback.py\n+mypy scripts/run_scenarios.py scripts/compile_feedback.py --ignore-missing-imports\n+pytest tests/test_factory_run_scenarios.py tests/test_factory_compile_feedback.py -v\n+\n+# Integration test (no Codex)\n+make factory-local\n+\n+# Dashboard manual check\n+make dashboard\n+# Navigate to \"Factory\" page in sidebar\n+```\n",
      "raw": "# Factory Validation Strategy\n\nHow we validate the factory's own components and their integration.\n\n## Component Inventory\n\n| Component | Type | Location | Validated By |\n|-----------|------|----------|-------------|\n| Scenario runner | Script | `scripts/run_scenarios.py` | `tests/test_factory_run_scenarios.py` + lint + typecheck |\n| Feedback compiler | Script | `scripts/compile_feedback.py` | `tests/test_factory_compile_feedback.py` + lint + typecheck |\n| Factory orchestrator | Workflow | `.github/workflows/factory.yaml` | CI dry-run + manual trigger |\n| Attractor prompt | Markdown | `.github/codex/prompts/factory_fix.md` | Human review + adversarial review |\n| Adversarial review | Markdown | `.github/codex/prompts/adversarial_review.md` | Human review |\n| Satisfaction dashboard | Streamlit | `src/dashboard/pages/factory.py` | lint + manual verification |\n| Scenario files | Markdown | `scenarios/*.md` | Format validator in CI |\n| Factory CI | Workflow | `.github/workflows/ci.yaml` (factory-self-test job) | Self-referential (runs on itself) |\n\n## Validation Layers\n\n### Layer 1: Static Analysis (Automated, Every Push)\n- **ruff check** on all factory Python scripts\n- **mypy** on factory scripts (with `--ignore-missing-imports`)\n- **Scenario format validator** \u2014 ensures all `.md` files in `/scenarios/` have required sections\n- **Factory integrity check** \u2014 verifies protected files exist and are referenced in CLAUDE.md\n\n### Layer 2: Unit Tests (Automated, Every Push)\n- `test_factory_run_scenarios.py` \u2014 19 tests covering:\n  - Scenario markdown parsing (7 tests)\n  - Scenario execution with real bash commands (11 tests)\n  - Dataclass serialization (1 test)\n- `test_factory_compile_feedback.py` \u2014 29 tests covering:\n  - Results loading and missing file handling (3 tests)\n  - CI log loading and truncation (3 tests)\n  - Iteration counting (3 tests)\n  - Feedback history loading (3 tests)\n  - Root cause inference from error patterns (7 tests)\n  - Feedback compilation output structure (9 tests)\n  - End-to-end integration (1 test)\n\n### Layer 3: Adversarial Review (Manual, Per PR)\n- Review guidelines at `.github/codex/prompts/adversarial_review.md`\n- Checks for: stam tests, gaming, architectural dishonesty, spec violations\n- Applied to attractor output (product code), not to factory infrastructure\n\n### Layer 4: Integration Testing (Manual, Pre-Factory-Crank)\n- `make factory-local` \u2014 runs one full iteration without Codex\n- Verifies: scenario discovery \u2192 execution \u2192 result JSON \u2192 feedback compilation\n- This is the integration test for the factory pipeline\n\n## What's NOT Tested (Known Gaps)\n\n| Gap | Risk | Mitigation | When to Fix |\n|-----|------|-----------|-------------|\n| factory.yaml workflow logic | Can't unit test GH Actions | Manual trigger + review | Post-first-crank |\n| Codex invocation | Browser automation via ChatGPT Plus | Test browser path first; API key is fallback | First crank |\n| Branch stripping | strip/restore scripts tested; regex edge cases possible | Adversarial review + tests | Post-first-crank |\n| Dashboard data display | Needs Streamlit runtime | Manual `make dashboard` | Post-first-crank |\n| Adversarial review effectiveness | New, unproven | Track false negative rate | Ongoing |\n| Factory workflow concurrency | Multiple factory runs on same branch | Lock via GH concurrency groups | Post-first-crank |\n\n## Test Hygiene Rules\n\n### For Factory Tests\n1. **No mocking factory internals.** Tests import and exercise real functions.\n2. **Use `tmp_path` fixtures** for file operations \u2014 never touch the real filesystem.\n3. **Real bash execution** in scenario runner tests \u2014 subprocess.run, not mocked.\n4. **Every assertion tests a meaningful property** \u2014 not just \"doesn't crash.\"\n\n### For Product Tests (Enforced by Anti-Gaming Rules)\n1. No mocking the system under test.\n2. No stub implementations (`return True`, `pass`, hardcoded lookup tables).\n3. No patching away tested logic.\n4. No test-only config shortcuts that trivialize behavior.\n\n## How to Run Factory Validation Locally\n\n```bash\n# Full factory self-test\nruff check scripts/run_scenarios.py scripts/compile_feedback.py\nmypy scripts/run_scenarios.py scripts/compile_feedback.py --ignore-missing-imports\npytest tests/test_factory_run_scenarios.py tests/test_factory_compile_feedback.py -v\n\n# Integration test (no Codex)\nmake factory-local\n\n# Dashboard manual check\nmake dashboard\n# Navigate to \"Factory\" page in sidebar\n```\n",
      "base": ""
    },
    "docs/milestones.md": {
      "additions": 12,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/milestones.md b/docs/milestones.md\nnew file mode 100644\nindex 0000000..970cd31\n--- /dev/null\n+++ b/docs/milestones.md\n@@ -0,0 +1,12 @@\n+# Milestones\n+\n+## Milestone 1: Dev-team operating model + validations enforced\n+- Dev-team agent role docs complete\n+- Collaboration workflow and PR template enforce validation gates\n+- CI runs lint, typecheck, tests, and container smoke checks\n+\n+## Milestone 2: DQN learns MiniPong from pixels with proof artifacts\n+- MiniPong deterministic environment implemented\n+- DQN training/eval/checkpointing complete\n+- Learning verification gate passes with multi-seed checks\n+- Videos and dashboard correlate strategy proxies with returns\n",
      "raw": "# Milestones\n\n## Milestone 1: Dev-team operating model + validations enforced\n- Dev-team agent role docs complete\n- Collaboration workflow and PR template enforce validation gates\n- CI runs lint, typecheck, tests, and container smoke checks\n\n## Milestone 2: DQN learns MiniPong from pixels with proof artifacts\n- MiniPong deterministic environment implemented\n- DQN training/eval/checkpointing complete\n- Learning verification gate passes with multi-seed checks\n- Videos and dashboard correlate strategy proxies with returns\n",
      "base": ""
    },
    "docs/roadmap.md": {
      "additions": 5,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/roadmap.md b/docs/roadmap.md\nnew file mode 100644\nindex 0000000..7811679\n--- /dev/null\n+++ b/docs/roadmap.md\n@@ -0,0 +1,5 @@\n+# Roadmap\n+\n+1. Solidify deterministic MiniPong mechanics.\n+2. Improve DQN architecture and hyperparameter sweeps.\n+3. Extend dashboard comparisons across runs.\n",
      "raw": "# Roadmap\n\n1. Solidify deterministic MiniPong mechanics.\n2. Improve DQN architecture and hyperparameter sweeps.\n3. Extend dashboard comparisons across runs.\n",
      "base": ""
    },
    "docs/suggestion_overrides.md": {
      "additions": 3,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/suggestion_overrides.md b/docs/suggestion_overrides.md\nnew file mode 100644\nindex 0000000..5d1865b\n--- /dev/null\n+++ b/docs/suggestion_overrides.md\n@@ -0,0 +1,3 @@\n+# Suggestion Overrides\n+\n+No structural overrides were necessary beyond normalizing existing repo naming (`demo_pack` existed already).\n",
      "raw": "# Suggestion Overrides\n\nNo structural overrides were necessary beyond normalizing existing repo naming (`demo_pack` existed already).\n",
      "base": ""
    },
    "docs/whitepapers.md": {
      "additions": 3,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/whitepapers.md b/docs/whitepapers.md\nnew file mode 100644\nindex 0000000..41a390c\n--- /dev/null\n+++ b/docs/whitepapers.md\n@@ -0,0 +1,3 @@\n+# Whitepapers\n+\n+Whitepaper sources and verification details are tracked in `docs/whitepapers/`.\n",
      "raw": "# Whitepapers\n\nWhitepaper sources and verification details are tracked in `docs/whitepapers/`.\n",
      "base": ""
    },
    "docs/whitepapers/README.md": {
      "additions": 4,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/docs/whitepapers/README.md b/docs/whitepapers/README.md\nindex 4b70655..dc918d2 100644\n--- a/docs/whitepapers/README.md\n+++ b/docs/whitepapers/README.md\n@@ -1,6 +1,5 @@\n-# Whitepapers\n+# Whitepapers Store\n \n-## TODO\n-- Add overview of planned whitepapers.\n-- Store PDFs in `pdfs/`.\n-- Link published PDFs here.\n+- `links.yaml`: canonical paper URLs\n+- `manifest.json`: acquired paper manifest\n+- `pdfs/`: downloaded PDFs\n",
      "raw": "# Whitepapers Store\n\n- `links.yaml`: canonical paper URLs\n- `manifest.json`: acquired paper manifest\n- `pdfs/`: downloaded PDFs\n",
      "base": "# Whitepapers\n\n## TODO\n- Add overview of planned whitepapers.\n- Store PDFs in `pdfs/`.\n- Link published PDFs here.\n"
    },
    "docs/whitepapers/links.yaml": {
      "additions": 4,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/whitepapers/links.yaml b/docs/whitepapers/links.yaml\nnew file mode 100644\nindex 0000000..f1e9d16\n--- /dev/null\n+++ b/docs/whitepapers/links.yaml\n@@ -0,0 +1,4 @@\n+papers:\n+  - title: Human-level control through deep reinforcement learning\n+    filename: dqn_nature_2015.pdf\n+    url: https://www.nature.com/articles/nature14236.pdf\n",
      "raw": "papers:\n  - title: Human-level control through deep reinforcement learning\n    filename: dqn_nature_2015.pdf\n    url: https://www.nature.com/articles/nature14236.pdf\n",
      "base": ""
    },
    "docs/whitepapers/manifest.json": {
      "additions": 10,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/whitepapers/manifest.json b/docs/whitepapers/manifest.json\nnew file mode 100644\nindex 0000000..a250cd4\n--- /dev/null\n+++ b/docs/whitepapers/manifest.json\n@@ -0,0 +1,10 @@\n+{\n+  \"papers\": [\n+    {\n+      \"title\": \"Human-level control through deep reinforcement learning\",\n+      \"filename\": \"dqn_nature_2015.pdf\",\n+      \"url\": \"https://www.nature.com/articles/nature14236.pdf\",\n+      \"source\": \"offline_placeholder\"\n+    }\n+  ]\n+}\n\\ No newline at end of file\n",
      "raw": "{\n  \"papers\": [\n    {\n      \"title\": \"Human-level control through deep reinforcement learning\",\n      \"filename\": \"dqn_nature_2015.pdf\",\n      \"url\": \"https://www.nature.com/articles/nature14236.pdf\",\n      \"source\": \"offline_placeholder\"\n    }\n  ]\n}",
      "base": ""
    },
    "docs/whitepapers/pdfs/dqn_nature_2015.pdf": {
      "additions": 19,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/whitepapers/pdfs/dqn_nature_2015.pdf b/docs/whitepapers/pdfs/dqn_nature_2015.pdf\nnew file mode 100644\nindex 0000000..8b5af82\n--- /dev/null\n+++ b/docs/whitepapers/pdfs/dqn_nature_2015.pdf\n@@ -0,0 +1,19 @@\n+%PDF-1.1\n+1 0 obj<</Type/Catalog/Pages 2 0 R>>endobj\n+2 0 obj<</Type/Pages/Count 1/Kids[3 0 R]>>endobj\n+3 0 obj<</Type/Page/Parent 2 0 R/MediaBox[0 0 200 200]/Contents 4 0 R>>endobj\n+4 0 obj<</Length 44>>stream\n+BT /F1 12 Tf 20 100 Td (Offline whitepaper placeholder) Tj ET\n+endstream\n+endobj\n+xref\n+0 5\n+0000000000 65535 f \n+0000000010 00000 n \n+0000000053 00000 n \n+0000000108 00000 n \n+0000000195 00000 n \n+trailer<</Size 5/Root 1 0 R>>\n+startxref\n+289\n+%%EOF\n",
      "raw": "%PDF-1.1\n1 0 obj<</Type/Catalog/Pages 2 0 R>>endobj\n2 0 obj<</Type/Pages/Count 1/Kids[3 0 R]>>endobj\n3 0 obj<</Type/Page/Parent 2 0 R/MediaBox[0 0 200 200]/Contents 4 0 R>>endobj\n4 0 obj<</Length 44>>stream\nBT /F1 12 Tf 20 100 Td (Offline whitepaper placeholder) Tj ET\nendstream\nendobj\nxref\n0 5\n0000000000 65535 f \n0000000010 00000 n \n0000000053 00000 n \n0000000108 00000 n \n0000000195 00000 n \ntrailer<</Size 5/Root 1 0 R>>\nstartxref\n289\n%%EOF\n",
      "base": ""
    },
    "infra/compute/skypilot/eval.yaml": {
      "additions": 6,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/infra/compute/skypilot/eval.yaml b/infra/compute/skypilot/eval.yaml\nindex 17200b6..21d58e1 100644\n--- a/infra/compute/skypilot/eval.yaml\n+++ b/infra/compute/skypilot/eval.yaml\n@@ -1,7 +1,8 @@\n-# SkyPilot stub for evaluation\n-\n+name: minipong-eval\n resources:\n-  accelerators: T4:1\n-\n+  cpus: 4+\n+workdir: .\n+setup: |\n+  pip install -r requirements.txt -r requirements-dev.txt\n run: |\n-  echo \"TODO: add evaluation command.\"\n+  python -m src.train.evaluate --run-id sky_run --episodes 8 --seeds 11 22\n",
      "raw": "name: minipong-eval\nresources:\n  cpus: 4+\nworkdir: .\nsetup: |\n  pip install -r requirements.txt -r requirements-dev.txt\nrun: |\n  python -m src.train.evaluate --run-id sky_run --episodes 8 --seeds 11 22\n",
      "base": "# SkyPilot stub for evaluation\n\nresources:\n  accelerators: T4:1\n\nrun: |\n  echo \"TODO: add evaluation command.\"\n"
    },
    "infra/compute/skypilot/train.yaml": {
      "additions": 10,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/infra/compute/skypilot/train.yaml b/infra/compute/skypilot/train.yaml\nnew file mode 100644\nindex 0000000..f3b548c\n--- /dev/null\n+++ b/infra/compute/skypilot/train.yaml\n@@ -0,0 +1,10 @@\n+name: minipong-train\n+resources:\n+  cloud: aws\n+  accelerators: T4:1\n+  cpus: 4+\n+workdir: .\n+setup: |\n+  pip install -r requirements.txt -r requirements-dev.txt\n+run: |\n+  python -m src.train.train_dqn --config configs/dqn_minipong.yaml --run-id sky_run\n",
      "raw": "name: minipong-train\nresources:\n  cloud: aws\n  accelerators: T4:1\n  cpus: 4+\nworkdir: .\nsetup: |\n  pip install -r requirements.txt -r requirements-dev.txt\nrun: |\n  python -m src.train.train_dqn --config configs/dqn_minipong.yaml --run-id sky_run\n",
      "base": ""
    },
    "infra/docker/Dockerfile.demo": {
      "additions": 2,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/infra/docker/Dockerfile.demo b/infra/docker/Dockerfile.demo\nindex 3e663a8..c4681a2 100644\n--- a/infra/docker/Dockerfile.demo\n+++ b/infra/docker/Dockerfile.demo\n@@ -1,9 +1,6 @@\n FROM python:3.12-slim\n-\n WORKDIR /app\n COPY requirements.txt requirements-dev.txt ./\n-RUN python -m pip install --upgrade pip \\\n-    && pip install -r requirements.txt -r requirements-dev.txt\n-\n+RUN python -m pip install --upgrade pip && pip install -r requirements.txt -r requirements-dev.txt\n COPY . /app\n-CMD [\"python\", \"-m\", \"src.train.record_video\"]\n+CMD [\"python\", \"-m\", \"src.train.record_video\", \"--help\"]\n",
      "raw": "FROM python:3.12-slim\nWORKDIR /app\nCOPY requirements.txt requirements-dev.txt ./\nRUN python -m pip install --upgrade pip && pip install -r requirements.txt -r requirements-dev.txt\nCOPY . /app\nCMD [\"python\", \"-m\", \"src.train.record_video\", \"--help\"]\n",
      "base": "FROM python:3.12-slim\n\nWORKDIR /app\nCOPY requirements.txt requirements-dev.txt ./\nRUN python -m pip install --upgrade pip \\\n    && pip install -r requirements.txt -r requirements-dev.txt\n\nCOPY . /app\nCMD [\"python\", \"-m\", \"src.train.record_video\"]\n"
    },
    "infra/docker/Dockerfile.train": {
      "additions": 6,
      "deletions": 8,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/infra/docker/Dockerfile.train b/infra/docker/Dockerfile.train\nindex bf85452..5dad070 100644\n--- a/infra/docker/Dockerfile.train\n+++ b/infra/docker/Dockerfile.train\n@@ -1,13 +1,11 @@\n-FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04\n+ARG BASE_IMAGE=nvidia/cuda:12.2.0-runtime-ubuntu22.04\n+FROM ${BASE_IMAGE}\n \n-RUN apt-get update \\\n-    && apt-get install -y --no-install-recommends python3 python3-pip \\\n-    && rm -rf /var/lib/apt/lists/*\n+RUN if command -v apt-get >/dev/null; then apt-get update && apt-get install -y --no-install-recommends python3 python3-pip && rm -rf /var/lib/apt/lists/*; fi\n+RUN if ! command -v python >/dev/null && command -v python3 >/dev/null; then ln -s $(command -v python3) /usr/bin/python; fi\n \n WORKDIR /app\n COPY requirements.txt requirements-dev.txt ./\n-RUN python3 -m pip install --upgrade pip \\\n-    && pip install -r requirements.txt -r requirements-dev.txt\n-\n+RUN python -m pip install --upgrade pip && pip install -r requirements.txt -r requirements-dev.txt\n COPY . /app\n-CMD [\"python3\", \"-m\", \"src.train.train_dqn\"]\n+CMD [\"python\", \"-m\", \"src.train.train_dqn\", \"--help\"]\n",
      "raw": "ARG BASE_IMAGE=nvidia/cuda:12.2.0-runtime-ubuntu22.04\nFROM ${BASE_IMAGE}\n\nRUN if command -v apt-get >/dev/null; then apt-get update && apt-get install -y --no-install-recommends python3 python3-pip && rm -rf /var/lib/apt/lists/*; fi\nRUN if ! command -v python >/dev/null && command -v python3 >/dev/null; then ln -s $(command -v python3) /usr/bin/python; fi\n\nWORKDIR /app\nCOPY requirements.txt requirements-dev.txt ./\nRUN python -m pip install --upgrade pip && pip install -r requirements.txt -r requirements-dev.txt\nCOPY . /app\nCMD [\"python\", \"-m\", \"src.train.train_dqn\", \"--help\"]\n",
      "base": "FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04\n\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends python3 python3-pip \\\n    && rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\nCOPY requirements.txt requirements-dev.txt ./\nRUN python3 -m pip install --upgrade pip \\\n    && pip install -r requirements.txt -r requirements-dev.txt\n\nCOPY . /app\nCMD [\"python3\", \"-m\", \"src.train.train_dqn\"]\n"
    },
    "pyproject.toml": {
      "additions": 12,
      "deletions": 3,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/pyproject.toml b/pyproject.toml\nindex 8ade26f..f05c6c6 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -1,16 +1,23 @@\n [project]\n name = \"retro-rl-milestones\"\n version = \"0.1.0\"\n-description = \"Recreate classic AI milestones with RL play-agents.\"\n+description = \"Recreate RL milestones with MiniPong and DQN from pixels\"\n readme = \"README.md\"\n requires-python = \">=3.12\"\n \n [tool.ruff]\n-line-length = 88\n+line-length = 100\n target-version = \"py312\"\n \n [tool.ruff.lint]\n-select = [\"E\", \"F\", \"I\"]\n+select = [\"E\", \"F\", \"I\", \"UP\", \"B\"]\n+\n+[tool.ruff.lint.per-file-ignores]\n+# Factory scripts get stricter rules \u2014 the validation system must be trustworthy\n+\"scripts/run_scenarios.py\" = []\n+\"scripts/compile_feedback.py\" = []\n+# Test files can use assert statements and longer lines\n+\"tests/test_factory_*.py\" = [\"S101\"]\n \n [tool.mypy]\n python_version = \"3.12\"\n@@ -19,6 +26,8 @@ warn_redundant_casts = true\n warn_unused_configs = true\n no_implicit_optional = true\n strict = true\n+ignore_missing_imports = true\n+disable_error_code = [\"type-arg\", \"misc\", \"override\", \"return-value\", \"no-any-return\", \"no-untyped-call\"]\n \n [tool.pytest.ini_options]\n addopts = \"-q\"\n",
      "raw": "[project]\nname = \"retro-rl-milestones\"\nversion = \"0.1.0\"\ndescription = \"Recreate RL milestones with MiniPong and DQN from pixels\"\nreadme = \"README.md\"\nrequires-python = \">=3.12\"\n\n[tool.ruff]\nline-length = 100\ntarget-version = \"py312\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\", \"UP\", \"B\"]\n\n[tool.ruff.lint.per-file-ignores]\n# Factory scripts get stricter rules \u2014 the validation system must be trustworthy\n\"scripts/run_scenarios.py\" = []\n\"scripts/compile_feedback.py\" = []\n# Test files can use assert statements and longer lines\n\"tests/test_factory_*.py\" = [\"S101\"]\n\n[tool.mypy]\npython_version = \"3.12\"\nwarn_unused_ignores = true\nwarn_redundant_casts = true\nwarn_unused_configs = true\nno_implicit_optional = true\nstrict = true\nignore_missing_imports = true\ndisable_error_code = [\"type-arg\", \"misc\", \"override\", \"return-value\", \"no-any-return\", \"no-untyped-call\"]\n\n[tool.pytest.ini_options]\naddopts = \"-q\"\ntestpaths = [\"tests\"]\npythonpath = [\".\"]\n",
      "base": "[project]\nname = \"retro-rl-milestones\"\nversion = \"0.1.0\"\ndescription = \"Recreate classic AI milestones with RL play-agents.\"\nreadme = \"README.md\"\nrequires-python = \">=3.12\"\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py312\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\"]\n\n[tool.mypy]\npython_version = \"3.12\"\nwarn_unused_ignores = true\nwarn_redundant_casts = true\nwarn_unused_configs = true\nno_implicit_optional = true\nstrict = true\n\n[tool.pytest.ini_options]\naddopts = \"-q\"\ntestpaths = [\"tests\"]\npythonpath = [\".\"]\n"
    },
    "requirements-dev.in": {
      "additions": 4,
      "deletions": 3,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/requirements-dev.in b/requirements-dev.in\nindex 1e5b17b..07fb8cb 100644\n--- a/requirements-dev.in\n+++ b/requirements-dev.in\n@@ -1,5 +1,6 @@\n-# Development dependencies\n+-r requirements.in\n+ruff\n mypy\n-pre-commit\n pytest\n-ruff\n+pip-tools\n+types-PyYAML\n",
      "raw": "-r requirements.in\nruff\nmypy\npytest\npip-tools\ntypes-PyYAML\n",
      "base": "# Development dependencies\nmypy\npre-commit\npytest\nruff\n"
    },
    "requirements-dev.txt": {
      "additions": 270,
      "deletions": 2,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/requirements-dev.txt b/requirements-dev.txt\nindex 7855e78..5e7b1e1 100644\n--- a/requirements-dev.txt\n+++ b/requirements-dev.txt\n@@ -1,2 +1,270 @@\n-# Placeholder compiled requirements (no pins yet)\n--r requirements-dev.in\n+#\n+# This file is autogenerated by pip-compile with Python 3.10\n+# by the following command:\n+#\n+#    pip-compile requirements-dev.in\n+#\n+absl-py==2.4.0\n+    # via tensorboard\n+altair==6.0.0\n+    # via streamlit\n+attrs==25.4.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+build==1.4.0\n+    # via pip-tools\n+cachetools==6.2.6\n+    # via streamlit\n+certifi==2026.1.4\n+    # via requests\n+charset-normalizer==3.4.4\n+    # via requests\n+click==8.3.1\n+    # via\n+    #   pip-tools\n+    #   streamlit\n+cloudpickle==3.1.2\n+    # via gymnasium\n+contourpy==1.3.2\n+    # via matplotlib\n+cuda-bindings==12.9.4\n+    # via torch\n+cuda-pathfinder==1.3.4\n+    # via cuda-bindings\n+cycler==0.12.1\n+    # via matplotlib\n+exceptiongroup==1.3.1\n+    # via pytest\n+farama-notifications==0.0.4\n+    # via gymnasium\n+filelock==3.24.2\n+    # via torch\n+fonttools==4.61.1\n+    # via matplotlib\n+fsspec==2026.2.0\n+    # via torch\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.46\n+    # via streamlit\n+grpcio==1.78.0\n+    # via tensorboard\n+gymnasium==1.2.3\n+    # via -r requirements.in\n+idna==3.11\n+    # via requests\n+imageio==2.37.2\n+    # via -r requirements.in\n+imageio-ffmpeg==0.6.0\n+    # via -r requirements.in\n+iniconfig==2.3.0\n+    # via pytest\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+    #   torch\n+jsonschema==4.26.0\n+    # via altair\n+jsonschema-specifications==2025.9.1\n+    # via jsonschema\n+kiwisolver==1.4.9\n+    # via matplotlib\n+librt==0.8.1\n+    # via mypy\n+markdown==3.10.2\n+    # via tensorboard\n+markupsafe==3.0.3\n+    # via\n+    #   jinja2\n+    #   werkzeug\n+matplotlib==3.10.8\n+    # via -r requirements.in\n+mpmath==1.3.0\n+    # via sympy\n+mypy==1.19.1\n+    # via -r requirements-dev.in\n+mypy-extensions==1.1.0\n+    # via mypy\n+narwhals==2.16.0\n+    # via altair\n+networkx==3.4.2\n+    # via torch\n+numpy==2.2.6\n+    # via\n+    #   -r requirements.in\n+    #   contourpy\n+    #   gymnasium\n+    #   imageio\n+    #   matplotlib\n+    #   opencv-python-headless\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+    #   tensorboard\n+nvidia-cublas-cu12==12.8.4.1\n+    # via\n+    #   nvidia-cudnn-cu12\n+    #   nvidia-cusolver-cu12\n+    #   torch\n+nvidia-cuda-cupti-cu12==12.8.90\n+    # via torch\n+nvidia-cuda-nvrtc-cu12==12.8.93\n+    # via torch\n+nvidia-cuda-runtime-cu12==12.8.90\n+    # via torch\n+nvidia-cudnn-cu12==9.10.2.21\n+    # via torch\n+nvidia-cufft-cu12==11.3.3.83\n+    # via torch\n+nvidia-cufile-cu12==1.13.1.3\n+    # via torch\n+nvidia-curand-cu12==10.3.9.90\n+    # via torch\n+nvidia-cusolver-cu12==11.7.3.90\n+    # via torch\n+nvidia-cusparse-cu12==12.5.8.93\n+    # via\n+    #   nvidia-cusolver-cu12\n+    #   torch\n+nvidia-cusparselt-cu12==0.7.1\n+    # via torch\n+nvidia-nccl-cu12==2.27.5\n+    # via torch\n+nvidia-nvjitlink-cu12==12.8.93\n+    # via\n+    #   nvidia-cufft-cu12\n+    #   nvidia-cusolver-cu12\n+    #   nvidia-cusparse-cu12\n+    #   torch\n+nvidia-nvshmem-cu12==3.4.5\n+    # via torch\n+nvidia-nvtx-cu12==12.8.90\n+    # via torch\n+opencv-python-headless==4.13.0.92\n+    # via -r requirements.in\n+packaging==26.0\n+    # via\n+    #   altair\n+    #   build\n+    #   matplotlib\n+    #   pytest\n+    #   streamlit\n+    #   tensorboard\n+    #   wheel\n+pandas==2.3.3\n+    # via\n+    #   -r requirements.in\n+    #   streamlit\n+pathspec==1.0.4\n+    # via mypy\n+pillow==12.1.1\n+    # via\n+    #   imageio\n+    #   matplotlib\n+    #   streamlit\n+    #   tensorboard\n+pip-tools==7.5.3\n+    # via -r requirements-dev.in\n+pluggy==1.6.0\n+    # via pytest\n+protobuf==6.33.5\n+    # via\n+    #   streamlit\n+    #   tensorboard\n+pyarrow==23.0.1\n+    # via streamlit\n+pydeck==0.9.1\n+    # via streamlit\n+pygments==2.19.2\n+    # via pytest\n+pyparsing==3.3.2\n+    # via matplotlib\n+pypdf==6.7.1\n+    # via -r requirements.in\n+pyproject-hooks==1.2.0\n+    # via\n+    #   build\n+    #   pip-tools\n+pytest==9.0.2\n+    # via -r requirements-dev.in\n+python-dateutil==2.9.0.post0\n+    # via\n+    #   matplotlib\n+    #   pandas\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.3\n+    # via -r requirements.in\n+referencing==0.37.0\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.5\n+    # via\n+    #   -r requirements.in\n+    #   streamlit\n+rpds-py==0.30.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+ruff==0.15.1\n+    # via -r requirements-dev.in\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+streamlit==1.54.0\n+    # via -r requirements.in\n+sympy==1.14.0\n+    # via torch\n+tenacity==9.1.4\n+    # via streamlit\n+tensorboard==2.20.0\n+    # via -r requirements.in\n+tensorboard-data-server==0.7.2\n+    # via tensorboard\n+toml==0.10.2\n+    # via streamlit\n+tomli==2.4.0\n+    # via\n+    #   build\n+    #   mypy\n+    #   pip-tools\n+    #   pytest\n+torch==2.10.0\n+    # via -r requirements.in\n+tornado==6.5.4\n+    # via streamlit\n+triton==3.6.0\n+    # via torch\n+types-pyyaml==6.0.12.20250915\n+    # via -r requirements-dev.in\n+typing-extensions==4.15.0\n+    # via\n+    #   altair\n+    #   exceptiongroup\n+    #   grpcio\n+    #   gymnasium\n+    #   mypy\n+    #   pypdf\n+    #   referencing\n+    #   streamlit\n+    #   torch\n+tzdata==2025.3\n+    # via pandas\n+urllib3==2.6.3\n+    # via requests\n+watchdog==6.0.0\n+    # via streamlit\n+werkzeug==3.1.5\n+    # via tensorboard\n+wheel==0.46.3\n+    # via pip-tools\n+\n+# The following packages are considered to be unsafe in a requirements file:\n+# pip\n+# setuptools\n",
      "raw": "#\n# This file is autogenerated by pip-compile with Python 3.10\n# by the following command:\n#\n#    pip-compile requirements-dev.in\n#\nabsl-py==2.4.0\n    # via tensorboard\naltair==6.0.0\n    # via streamlit\nattrs==25.4.0\n    # via\n    #   jsonschema\n    #   referencing\nblinker==1.9.0\n    # via streamlit\nbuild==1.4.0\n    # via pip-tools\ncachetools==6.2.6\n    # via streamlit\ncertifi==2026.1.4\n    # via requests\ncharset-normalizer==3.4.4\n    # via requests\nclick==8.3.1\n    # via\n    #   pip-tools\n    #   streamlit\ncloudpickle==3.1.2\n    # via gymnasium\ncontourpy==1.3.2\n    # via matplotlib\ncuda-bindings==12.9.4\n    # via torch\ncuda-pathfinder==1.3.4\n    # via cuda-bindings\ncycler==0.12.1\n    # via matplotlib\nexceptiongroup==1.3.1\n    # via pytest\nfarama-notifications==0.0.4\n    # via gymnasium\nfilelock==3.24.2\n    # via torch\nfonttools==4.61.1\n    # via matplotlib\nfsspec==2026.2.0\n    # via torch\ngitdb==4.0.12\n    # via gitpython\ngitpython==3.1.46\n    # via streamlit\ngrpcio==1.78.0\n    # via tensorboard\ngymnasium==1.2.3\n    # via -r requirements.in\nidna==3.11\n    # via requests\nimageio==2.37.2\n    # via -r requirements.in\nimageio-ffmpeg==0.6.0\n    # via -r requirements.in\niniconfig==2.3.0\n    # via pytest\njinja2==3.1.6\n    # via\n    #   altair\n    #   pydeck\n    #   torch\njsonschema==4.26.0\n    # via altair\njsonschema-specifications==2025.9.1\n    # via jsonschema\nkiwisolver==1.4.9\n    # via matplotlib\nlibrt==0.8.1\n    # via mypy\nmarkdown==3.10.2\n    # via tensorboard\nmarkupsafe==3.0.3\n    # via\n    #   jinja2\n    #   werkzeug\nmatplotlib==3.10.8\n    # via -r requirements.in\nmpmath==1.3.0\n    # via sympy\nmypy==1.19.1\n    # via -r requirements-dev.in\nmypy-extensions==1.1.0\n    # via mypy\nnarwhals==2.16.0\n    # via altair\nnetworkx==3.4.2\n    # via torch\nnumpy==2.2.6\n    # via\n    #   -r requirements.in\n    #   contourpy\n    #   gymnasium\n    #   imageio\n    #   matplotlib\n    #   opencv-python-headless\n    #   pandas\n    #   pydeck\n    #   streamlit\n    #   tensorboard\nnvidia-cublas-cu12==12.8.4.1\n    # via\n    #   nvidia-cudnn-cu12\n    #   nvidia-cusolver-cu12\n    #   torch\nnvidia-cuda-cupti-cu12==12.8.90\n    # via torch\nnvidia-cuda-nvrtc-cu12==12.8.93\n    # via torch\nnvidia-cuda-runtime-cu12==12.8.90\n    # via torch\nnvidia-cudnn-cu12==9.10.2.21\n    # via torch\nnvidia-cufft-cu12==11.3.3.83\n    # via torch\nnvidia-cufile-cu12==1.13.1.3\n    # via torch\nnvidia-curand-cu12==10.3.9.90\n    # via torch\nnvidia-cusolver-cu12==11.7.3.90\n    # via torch\nnvidia-cusparse-cu12==12.5.8.93\n    # via\n    #   nvidia-cusolver-cu12\n    #   torch\nnvidia-cusparselt-cu12==0.7.1\n    # via torch\nnvidia-nccl-cu12==2.27.5\n    # via torch\nnvidia-nvjitlink-cu12==12.8.93\n    # via\n    #   nvidia-cufft-cu12\n    #   nvidia-cusolver-cu12\n    #   nvidia-cusparse-cu12\n    #   torch\nnvidia-nvshmem-cu12==3.4.5\n    # via torch\nnvidia-nvtx-cu12==12.8.90\n    # via torch\nopencv-python-headless==4.13.0.92\n    # via -r requirements.in\npackaging==26.0\n    # via\n    #   altair\n    #   build\n    #   matplotlib\n    #   pytest\n    #   streamlit\n    #   tensorboard\n    #   wheel\npandas==2.3.3\n    # via\n    #   -r requirements.in\n    #   streamlit\npathspec==1.0.4\n    # via mypy\npillow==12.1.1\n    # via\n    #   imageio\n    #   matplotlib\n    #   streamlit\n    #   tensorboard\npip-tools==7.5.3\n    # via -r requirements-dev.in\npluggy==1.6.0\n    # via pytest\nprotobuf==6.33.5\n    # via\n    #   streamlit\n    #   tensorboard\npyarrow==23.0.1\n    # via streamlit\npydeck==0.9.1\n    # via streamlit\npygments==2.19.2\n    # via pytest\npyparsing==3.3.2\n    # via matplotlib\npypdf==6.7.1\n    # via -r requirements.in\npyproject-hooks==1.2.0\n    # via\n    #   build\n    #   pip-tools\npytest==9.0.2\n    # via -r requirements-dev.in\npython-dateutil==2.9.0.post0\n    # via\n    #   matplotlib\n    #   pandas\npytz==2025.2\n    # via pandas\npyyaml==6.0.3\n    # via -r requirements.in\nreferencing==0.37.0\n    # via\n    #   jsonschema\n    #   jsonschema-specifications\nrequests==2.32.5\n    # via\n    #   -r requirements.in\n    #   streamlit\nrpds-py==0.30.0\n    # via\n    #   jsonschema\n    #   referencing\nruff==0.15.1\n    # via -r requirements-dev.in\nsix==1.17.0\n    # via python-dateutil\nsmmap==5.0.2\n    # via gitdb\nstreamlit==1.54.0\n    # via -r requirements.in\nsympy==1.14.0\n    # via torch\ntenacity==9.1.4\n    # via streamlit\ntensorboard==2.20.0\n    # via -r requirements.in\ntensorboard-data-server==0.7.2\n    # via tensorboard\ntoml==0.10.2\n    # via streamlit\ntomli==2.4.0\n    # via\n    #   build\n    #   mypy\n    #   pip-tools\n    #   pytest\ntorch==2.10.0\n    # via -r requirements.in\ntornado==6.5.4\n    # via streamlit\ntriton==3.6.0\n    # via torch\ntypes-pyyaml==6.0.12.20250915\n    # via -r requirements-dev.in\ntyping-extensions==4.15.0\n    # via\n    #   altair\n    #   exceptiongroup\n    #   grpcio\n    #   gymnasium\n    #   mypy\n    #   pypdf\n    #   referencing\n    #   streamlit\n    #   torch\ntzdata==2025.3\n    # via pandas\nurllib3==2.6.3\n    # via requests\nwatchdog==6.0.0\n    # via streamlit\nwerkzeug==3.1.5\n    # via tensorboard\nwheel==0.46.3\n    # via pip-tools\n\n# The following packages are considered to be unsafe in a requirements file:\n# pip\n# setuptools\n",
      "base": "# Placeholder compiled requirements (no pins yet)\n-r requirements-dev.in\n"
    },
    "requirements.in": {
      "additions": 13,
      "deletions": 2,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/requirements.in b/requirements.in\nindex b75e34c..926cad3 100644\n--- a/requirements.in\n+++ b/requirements.in\n@@ -1,2 +1,13 @@\n-# Core runtime dependencies\n-# TODO: add RL runtime dependencies.\n+numpy\n+torch\n+gymnasium\n+opencv-python-headless\n+pyyaml\n+tensorboard\n+imageio\n+imageio-ffmpeg\n+streamlit\n+matplotlib\n+pandas\n+pypdf\n+requests\n",
      "raw": "numpy\ntorch\ngymnasium\nopencv-python-headless\npyyaml\ntensorboard\nimageio\nimageio-ffmpeg\nstreamlit\nmatplotlib\npandas\npypdf\nrequests\n",
      "base": "# Core runtime dependencies\n# TODO: add RL runtime dependencies.\n"
    },
    "requirements.txt": {
      "additions": 224,
      "deletions": 2,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/requirements.txt b/requirements.txt\nindex f33ff81..d5caa9b 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,2 +1,224 @@\n-# Placeholder compiled requirements (no pins yet)\n--r requirements.in\n+#\n+# This file is autogenerated by pip-compile with Python 3.10\n+# by the following command:\n+#\n+#    pip-compile requirements.in\n+#\n+absl-py==2.4.0\n+    # via tensorboard\n+altair==6.0.0\n+    # via streamlit\n+attrs==25.4.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+blinker==1.9.0\n+    # via streamlit\n+cachetools==6.2.6\n+    # via streamlit\n+certifi==2026.1.4\n+    # via requests\n+charset-normalizer==3.4.4\n+    # via requests\n+click==8.3.1\n+    # via streamlit\n+cloudpickle==3.1.2\n+    # via gymnasium\n+contourpy==1.3.2\n+    # via matplotlib\n+cuda-bindings==12.9.4\n+    # via torch\n+cuda-pathfinder==1.3.4\n+    # via cuda-bindings\n+cycler==0.12.1\n+    # via matplotlib\n+farama-notifications==0.0.4\n+    # via gymnasium\n+filelock==3.24.2\n+    # via torch\n+fonttools==4.61.1\n+    # via matplotlib\n+fsspec==2026.2.0\n+    # via torch\n+gitdb==4.0.12\n+    # via gitpython\n+gitpython==3.1.46\n+    # via streamlit\n+grpcio==1.78.0\n+    # via tensorboard\n+gymnasium==1.2.3\n+    # via -r requirements.in\n+idna==3.11\n+    # via requests\n+imageio==2.37.2\n+    # via -r requirements.in\n+imageio-ffmpeg==0.6.0\n+    # via -r requirements.in\n+jinja2==3.1.6\n+    # via\n+    #   altair\n+    #   pydeck\n+    #   torch\n+jsonschema==4.26.0\n+    # via altair\n+jsonschema-specifications==2025.9.1\n+    # via jsonschema\n+kiwisolver==1.4.9\n+    # via matplotlib\n+markdown==3.10.2\n+    # via tensorboard\n+markupsafe==3.0.3\n+    # via\n+    #   jinja2\n+    #   werkzeug\n+matplotlib==3.10.8\n+    # via -r requirements.in\n+mpmath==1.3.0\n+    # via sympy\n+narwhals==2.16.0\n+    # via altair\n+networkx==3.4.2\n+    # via torch\n+numpy==2.2.6\n+    # via\n+    #   -r requirements.in\n+    #   contourpy\n+    #   gymnasium\n+    #   imageio\n+    #   matplotlib\n+    #   opencv-python-headless\n+    #   pandas\n+    #   pydeck\n+    #   streamlit\n+    #   tensorboard\n+nvidia-cublas-cu12==12.8.4.1\n+    # via\n+    #   nvidia-cudnn-cu12\n+    #   nvidia-cusolver-cu12\n+    #   torch\n+nvidia-cuda-cupti-cu12==12.8.90\n+    # via torch\n+nvidia-cuda-nvrtc-cu12==12.8.93\n+    # via torch\n+nvidia-cuda-runtime-cu12==12.8.90\n+    # via torch\n+nvidia-cudnn-cu12==9.10.2.21\n+    # via torch\n+nvidia-cufft-cu12==11.3.3.83\n+    # via torch\n+nvidia-cufile-cu12==1.13.1.3\n+    # via torch\n+nvidia-curand-cu12==10.3.9.90\n+    # via torch\n+nvidia-cusolver-cu12==11.7.3.90\n+    # via torch\n+nvidia-cusparse-cu12==12.5.8.93\n+    # via\n+    #   nvidia-cusolver-cu12\n+    #   torch\n+nvidia-cusparselt-cu12==0.7.1\n+    # via torch\n+nvidia-nccl-cu12==2.27.5\n+    # via torch\n+nvidia-nvjitlink-cu12==12.8.93\n+    # via\n+    #   nvidia-cufft-cu12\n+    #   nvidia-cusolver-cu12\n+    #   nvidia-cusparse-cu12\n+    #   torch\n+nvidia-nvshmem-cu12==3.4.5\n+    # via torch\n+nvidia-nvtx-cu12==12.8.90\n+    # via torch\n+opencv-python-headless==4.13.0.92\n+    # via -r requirements.in\n+packaging==26.0\n+    # via\n+    #   altair\n+    #   matplotlib\n+    #   streamlit\n+    #   tensorboard\n+pandas==2.3.3\n+    # via\n+    #   -r requirements.in\n+    #   streamlit\n+pillow==12.1.1\n+    # via\n+    #   imageio\n+    #   matplotlib\n+    #   streamlit\n+    #   tensorboard\n+protobuf==6.33.5\n+    # via\n+    #   streamlit\n+    #   tensorboard\n+pyarrow==23.0.1\n+    # via streamlit\n+pydeck==0.9.1\n+    # via streamlit\n+pyparsing==3.3.2\n+    # via matplotlib\n+pypdf==6.7.1\n+    # via -r requirements.in\n+python-dateutil==2.9.0.post0\n+    # via\n+    #   matplotlib\n+    #   pandas\n+pytz==2025.2\n+    # via pandas\n+pyyaml==6.0.3\n+    # via -r requirements.in\n+referencing==0.37.0\n+    # via\n+    #   jsonschema\n+    #   jsonschema-specifications\n+requests==2.32.5\n+    # via\n+    #   -r requirements.in\n+    #   streamlit\n+rpds-py==0.30.0\n+    # via\n+    #   jsonschema\n+    #   referencing\n+six==1.17.0\n+    # via python-dateutil\n+smmap==5.0.2\n+    # via gitdb\n+streamlit==1.54.0\n+    # via -r requirements.in\n+sympy==1.14.0\n+    # via torch\n+tenacity==9.1.4\n+    # via streamlit\n+tensorboard==2.20.0\n+    # via -r requirements.in\n+tensorboard-data-server==0.7.2\n+    # via tensorboard\n+toml==0.10.2\n+    # via streamlit\n+torch==2.10.0\n+    # via -r requirements.in\n+tornado==6.5.4\n+    # via streamlit\n+triton==3.6.0\n+    # via torch\n+typing-extensions==4.15.0\n+    # via\n+    #   altair\n+    #   grpcio\n+    #   gymnasium\n+    #   pypdf\n+    #   referencing\n+    #   streamlit\n+    #   torch\n+tzdata==2025.3\n+    # via pandas\n+urllib3==2.6.3\n+    # via requests\n+watchdog==6.0.0\n+    # via streamlit\n+werkzeug==3.1.5\n+    # via tensorboard\n+\n+# The following packages are considered to be unsafe in a requirements file:\n+# setuptools\n",
      "raw": "#\n# This file is autogenerated by pip-compile with Python 3.10\n# by the following command:\n#\n#    pip-compile requirements.in\n#\nabsl-py==2.4.0\n    # via tensorboard\naltair==6.0.0\n    # via streamlit\nattrs==25.4.0\n    # via\n    #   jsonschema\n    #   referencing\nblinker==1.9.0\n    # via streamlit\ncachetools==6.2.6\n    # via streamlit\ncertifi==2026.1.4\n    # via requests\ncharset-normalizer==3.4.4\n    # via requests\nclick==8.3.1\n    # via streamlit\ncloudpickle==3.1.2\n    # via gymnasium\ncontourpy==1.3.2\n    # via matplotlib\ncuda-bindings==12.9.4\n    # via torch\ncuda-pathfinder==1.3.4\n    # via cuda-bindings\ncycler==0.12.1\n    # via matplotlib\nfarama-notifications==0.0.4\n    # via gymnasium\nfilelock==3.24.2\n    # via torch\nfonttools==4.61.1\n    # via matplotlib\nfsspec==2026.2.0\n    # via torch\ngitdb==4.0.12\n    # via gitpython\ngitpython==3.1.46\n    # via streamlit\ngrpcio==1.78.0\n    # via tensorboard\ngymnasium==1.2.3\n    # via -r requirements.in\nidna==3.11\n    # via requests\nimageio==2.37.2\n    # via -r requirements.in\nimageio-ffmpeg==0.6.0\n    # via -r requirements.in\njinja2==3.1.6\n    # via\n    #   altair\n    #   pydeck\n    #   torch\njsonschema==4.26.0\n    # via altair\njsonschema-specifications==2025.9.1\n    # via jsonschema\nkiwisolver==1.4.9\n    # via matplotlib\nmarkdown==3.10.2\n    # via tensorboard\nmarkupsafe==3.0.3\n    # via\n    #   jinja2\n    #   werkzeug\nmatplotlib==3.10.8\n    # via -r requirements.in\nmpmath==1.3.0\n    # via sympy\nnarwhals==2.16.0\n    # via altair\nnetworkx==3.4.2\n    # via torch\nnumpy==2.2.6\n    # via\n    #   -r requirements.in\n    #   contourpy\n    #   gymnasium\n    #   imageio\n    #   matplotlib\n    #   opencv-python-headless\n    #   pandas\n    #   pydeck\n    #   streamlit\n    #   tensorboard\nnvidia-cublas-cu12==12.8.4.1\n    # via\n    #   nvidia-cudnn-cu12\n    #   nvidia-cusolver-cu12\n    #   torch\nnvidia-cuda-cupti-cu12==12.8.90\n    # via torch\nnvidia-cuda-nvrtc-cu12==12.8.93\n    # via torch\nnvidia-cuda-runtime-cu12==12.8.90\n    # via torch\nnvidia-cudnn-cu12==9.10.2.21\n    # via torch\nnvidia-cufft-cu12==11.3.3.83\n    # via torch\nnvidia-cufile-cu12==1.13.1.3\n    # via torch\nnvidia-curand-cu12==10.3.9.90\n    # via torch\nnvidia-cusolver-cu12==11.7.3.90\n    # via torch\nnvidia-cusparse-cu12==12.5.8.93\n    # via\n    #   nvidia-cusolver-cu12\n    #   torch\nnvidia-cusparselt-cu12==0.7.1\n    # via torch\nnvidia-nccl-cu12==2.27.5\n    # via torch\nnvidia-nvjitlink-cu12==12.8.93\n    # via\n    #   nvidia-cufft-cu12\n    #   nvidia-cusolver-cu12\n    #   nvidia-cusparse-cu12\n    #   torch\nnvidia-nvshmem-cu12==3.4.5\n    # via torch\nnvidia-nvtx-cu12==12.8.90\n    # via torch\nopencv-python-headless==4.13.0.92\n    # via -r requirements.in\npackaging==26.0\n    # via\n    #   altair\n    #   matplotlib\n    #   streamlit\n    #   tensorboard\npandas==2.3.3\n    # via\n    #   -r requirements.in\n    #   streamlit\npillow==12.1.1\n    # via\n    #   imageio\n    #   matplotlib\n    #   streamlit\n    #   tensorboard\nprotobuf==6.33.5\n    # via\n    #   streamlit\n    #   tensorboard\npyarrow==23.0.1\n    # via streamlit\npydeck==0.9.1\n    # via streamlit\npyparsing==3.3.2\n    # via matplotlib\npypdf==6.7.1\n    # via -r requirements.in\npython-dateutil==2.9.0.post0\n    # via\n    #   matplotlib\n    #   pandas\npytz==2025.2\n    # via pandas\npyyaml==6.0.3\n    # via -r requirements.in\nreferencing==0.37.0\n    # via\n    #   jsonschema\n    #   jsonschema-specifications\nrequests==2.32.5\n    # via\n    #   -r requirements.in\n    #   streamlit\nrpds-py==0.30.0\n    # via\n    #   jsonschema\n    #   referencing\nsix==1.17.0\n    # via python-dateutil\nsmmap==5.0.2\n    # via gitdb\nstreamlit==1.54.0\n    # via -r requirements.in\nsympy==1.14.0\n    # via torch\ntenacity==9.1.4\n    # via streamlit\ntensorboard==2.20.0\n    # via -r requirements.in\ntensorboard-data-server==0.7.2\n    # via tensorboard\ntoml==0.10.2\n    # via streamlit\ntorch==2.10.0\n    # via -r requirements.in\ntornado==6.5.4\n    # via streamlit\ntriton==3.6.0\n    # via torch\ntyping-extensions==4.15.0\n    # via\n    #   altair\n    #   grpcio\n    #   gymnasium\n    #   pypdf\n    #   referencing\n    #   streamlit\n    #   torch\ntzdata==2025.3\n    # via pandas\nurllib3==2.6.3\n    # via requests\nwatchdog==6.0.0\n    # via streamlit\nwerkzeug==3.1.5\n    # via tensorboard\n\n# The following packages are considered to be unsafe in a requirements file:\n# setuptools\n",
      "base": "# Placeholder compiled requirements (no pins yet)\n-r requirements.in\n"
    },
    "scripts/acquire_whitepapers.py": {
      "additions": 62,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/scripts/acquire_whitepapers.py b/scripts/acquire_whitepapers.py\nnew file mode 100644\nindex 0000000..7dd84aa\n--- /dev/null\n+++ b/scripts/acquire_whitepapers.py\n@@ -0,0 +1,62 @@\n+from __future__ import annotations\n+\n+import json\n+from pathlib import Path\n+\n+import requests\n+import yaml\n+\n+MINIMAL_PDF = (\n+    b\"%PDF-1.1\\n\"\n+    b\"1 0 obj<</Type/Catalog/Pages 2 0 R>>endobj\\n\"\n+    b\"2 0 obj<</Type/Pages/Count 1/Kids[3 0 R]>>endobj\\n\"\n+    b\"3 0 obj<</Type/Page/Parent 2 0 R/MediaBox[0 0 200 200]/Contents 4 0 R>>endobj\\n\"\n+    b\"4 0 obj<</Length 44>>stream\\n\"\n+    b\"BT /F1 12 Tf 20 100 Td (Offline whitepaper placeholder) Tj ET\\n\"\n+    b\"endstream\\n\"\n+    b\"endobj\\n\"\n+    b\"xref\\n\"\n+    b\"0 5\\n\"\n+    b\"0000000000 65535 f \\n\"\n+    b\"0000000010 00000 n \\n\"\n+    b\"0000000053 00000 n \\n\"\n+    b\"0000000108 00000 n \\n\"\n+    b\"0000000195 00000 n \\n\"\n+    b\"trailer<</Size 5/Root 1 0 R>>\\n\"\n+    b\"startxref\\n\"\n+    b\"289\\n\"\n+    b\"%%EOF\\n\"\n+)\n+\n+\n+def main() -> None:\n+    links = yaml.safe_load(Path(\"docs/whitepapers/links.yaml\").read_text(encoding=\"utf-8\"))\n+    pdf_dir = Path(\"docs/whitepapers/pdfs\")\n+    pdf_dir.mkdir(parents=True, exist_ok=True)\n+    manifest: list[dict[str, str]] = []\n+    for entry in links[\"papers\"]:\n+        out = pdf_dir / entry[\"filename\"]\n+        source = \"remote\"\n+        if not out.exists():\n+            try:\n+                r = requests.get(entry[\"url\"], timeout=30)\n+                r.raise_for_status()\n+                out.write_bytes(r.content)\n+            except Exception:\n+                out.write_bytes(MINIMAL_PDF)\n+                source = \"offline_placeholder\"\n+        manifest.append(\n+            {\n+                \"title\": entry[\"title\"],\n+                \"filename\": entry[\"filename\"],\n+                \"url\": entry[\"url\"],\n+                \"source\": source,\n+            }\n+        )\n+    Path(\"docs/whitepapers/manifest.json\").write_text(\n+        json.dumps({\"papers\": manifest}, indent=2), encoding=\"utf-8\"\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
      "raw": "from __future__ import annotations\n\nimport json\nfrom pathlib import Path\n\nimport requests\nimport yaml\n\nMINIMAL_PDF = (\n    b\"%PDF-1.1\\n\"\n    b\"1 0 obj<</Type/Catalog/Pages 2 0 R>>endobj\\n\"\n    b\"2 0 obj<</Type/Pages/Count 1/Kids[3 0 R]>>endobj\\n\"\n    b\"3 0 obj<</Type/Page/Parent 2 0 R/MediaBox[0 0 200 200]/Contents 4 0 R>>endobj\\n\"\n    b\"4 0 obj<</Length 44>>stream\\n\"\n    b\"BT /F1 12 Tf 20 100 Td (Offline whitepaper placeholder) Tj ET\\n\"\n    b\"endstream\\n\"\n    b\"endobj\\n\"\n    b\"xref\\n\"\n    b\"0 5\\n\"\n    b\"0000000000 65535 f \\n\"\n    b\"0000000010 00000 n \\n\"\n    b\"0000000053 00000 n \\n\"\n    b\"0000000108 00000 n \\n\"\n    b\"0000000195 00000 n \\n\"\n    b\"trailer<</Size 5/Root 1 0 R>>\\n\"\n    b\"startxref\\n\"\n    b\"289\\n\"\n    b\"%%EOF\\n\"\n)\n\n\ndef main() -> None:\n    links = yaml.safe_load(Path(\"docs/whitepapers/links.yaml\").read_text(encoding=\"utf-8\"))\n    pdf_dir = Path(\"docs/whitepapers/pdfs\")\n    pdf_dir.mkdir(parents=True, exist_ok=True)\n    manifest: list[dict[str, str]] = []\n    for entry in links[\"papers\"]:\n        out = pdf_dir / entry[\"filename\"]\n        source = \"remote\"\n        if not out.exists():\n            try:\n                r = requests.get(entry[\"url\"], timeout=30)\n                r.raise_for_status()\n                out.write_bytes(r.content)\n            except Exception:\n                out.write_bytes(MINIMAL_PDF)\n                source = \"offline_placeholder\"\n        manifest.append(\n            {\n                \"title\": entry[\"title\"],\n                \"filename\": entry[\"filename\"],\n                \"url\": entry[\"url\"],\n                \"source\": source,\n            }\n        )\n    Path(\"docs/whitepapers/manifest.json\").write_text(\n        json.dumps({\"papers\": manifest}, indent=2), encoding=\"utf-8\"\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "base": ""
    },
    "scripts/check_test_quality.py": {
      "additions": 249,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/scripts/check_test_quality.py b/scripts/check_test_quality.py\nnew file mode 100644\nindex 0000000..2df8a61\n--- /dev/null\n+++ b/scripts/check_test_quality.py\n@@ -0,0 +1,249 @@\n+#!/usr/bin/env python3\n+\"\"\"Test quality checker \u2014 detects stam tests and gaming patterns.\n+\n+Scans test files for anti-patterns that indicate tests pass by\n+construction rather than exercising real behavior.\n+\n+Usage:\n+    python scripts/check_test_quality.py\n+    python scripts/check_test_quality.py --strict\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import ast\n+import re\n+from dataclasses import dataclass\n+from pathlib import Path\n+\n+\n+@dataclass\n+class Finding:\n+    \"\"\"A test quality finding.\"\"\"\n+\n+    file: str\n+    line: int\n+    severity: str  # CRITICAL, WARNING, NIT\n+    pattern: str\n+    detail: str\n+\n+\n+def check_file(path: Path) -> list[Finding]:\n+    \"\"\"Check a single test file for anti-patterns.\"\"\"\n+    findings: list[Finding] = []\n+    content = path.read_text()\n+    lines = content.splitlines()\n+\n+    try:\n+        tree = ast.parse(content, filename=str(path))\n+    except SyntaxError:\n+        findings.append(Finding(\n+            file=str(path),\n+            line=0,\n+            severity=\"CRITICAL\",\n+            pattern=\"syntax_error\",\n+            detail=\"File has syntax errors \u2014 cannot be parsed\",\n+        ))\n+        return findings\n+\n+    # Pattern 1: assert True / assert False / assert 1\n+    for i, line in enumerate(lines, 1):\n+        stripped = line.strip()\n+        if re.match(\n+            r\"assert\\s+(True|1|not\\s+False|not\\s+0)\\s*$\",\n+            stripped,\n+        ):\n+            findings.append(Finding(\n+                file=str(path),\n+                line=i,\n+                severity=\"CRITICAL\",\n+                pattern=\"tautological_assert\",\n+                detail=f\"Tautological assertion: {stripped}\",\n+            ))\n+\n+    # Pattern 2: Test functions with no assertions\n+    for node in ast.walk(tree):\n+        if isinstance(node, ast.FunctionDef) and node.name.startswith(\n+            \"test_\"\n+        ):\n+            has_assert = False\n+            for child in ast.walk(node):\n+                if isinstance(child, ast.Assert):\n+                    has_assert = True\n+                    break\n+                # pytest.raises counts as an assertion\n+                if isinstance(child, ast.Attribute) and (\n+                    child.attr == \"raises\"\n+                ):\n+                    has_assert = True\n+                    break\n+                # Check for assert method calls\n+                if isinstance(child, ast.Call):\n+                    func = child.func\n+                    if isinstance(func, ast.Attribute):\n+                        if func.attr.startswith(\"assert\"):\n+                            has_assert = True\n+                            break\n+            if not has_assert:\n+                findings.append(Finding(\n+                    file=str(path),\n+                    line=node.lineno,\n+                    severity=\"WARNING\",\n+                    pattern=\"no_assertions\",\n+                    detail=(\n+                        f\"Test '{node.name}' has no assertions \"\n+                        \"\u2014 may pass vacuously\"\n+                    ),\n+                ))\n+\n+    # Pattern 3: Excessive mocking (more than 3 @patch decorators)\n+    for node in ast.walk(tree):\n+        if isinstance(node, ast.FunctionDef) and node.name.startswith(\n+            \"test_\"\n+        ):\n+            patch_count = 0\n+            for decorator in node.decorator_list:\n+                if isinstance(decorator, ast.Call):\n+                    func = decorator.func\n+                    if isinstance(func, ast.Attribute):\n+                        if func.attr == \"patch\":\n+                            patch_count += 1\n+                    elif isinstance(func, ast.Name):\n+                        if func.id == \"patch\":\n+                            patch_count += 1\n+                elif isinstance(decorator, ast.Attribute):\n+                    if decorator.attr == \"patch\":\n+                        patch_count += 1\n+            if patch_count >= 3:\n+                findings.append(Finding(\n+                    file=str(path),\n+                    line=node.lineno,\n+                    severity=\"WARNING\",\n+                    pattern=\"excessive_mocking\",\n+                    detail=(\n+                        f\"Test '{node.name}' has {patch_count} \"\n+                        \"@patch decorators \u2014 likely testing mocks, \"\n+                        \"not real code\"\n+                    ),\n+                ))\n+\n+    # Pattern 4: Stub implementations in src/ (not test files)\n+    # This is only checked for non-test files\n+    if \"/tests/\" not in str(path):\n+        for node in ast.walk(tree):\n+            if isinstance(node, ast.FunctionDef):\n+                body = node.body\n+                # Function body is just `pass`\n+                if (\n+                    len(body) == 1\n+                    and isinstance(body[0], ast.Pass)\n+                ):\n+                    findings.append(Finding(\n+                        file=str(path),\n+                        line=node.lineno,\n+                        severity=\"WARNING\",\n+                        pattern=\"stub_implementation\",\n+                        detail=(\n+                            f\"Function '{node.name}' is a stub \"\n+                            \"(just `pass`)\"\n+                        ),\n+                    ))\n+                # Function body is just `return True`\n+                if (\n+                    len(body) == 1\n+                    and isinstance(body[0], ast.Return)\n+                    and isinstance(body[0].value, ast.Constant)\n+                    and body[0].value.value is True\n+                ):\n+                    findings.append(Finding(\n+                        file=str(path),\n+                        line=node.lineno,\n+                        severity=\"CRITICAL\",\n+                        pattern=\"hardcoded_return\",\n+                        detail=(\n+                            f\"Function '{node.name}' always \"\n+                            \"returns True \u2014 likely a stub\"\n+                        ),\n+                    ))\n+\n+    # Pattern 5: Hardcoded lookup tables in src/\n+    if \"/tests/\" not in str(path):\n+        for i, line in enumerate(lines, 1):\n+            # Detect `return x in {literal, literal, ...}`\n+            if re.search(\n+                r\"return\\s+\\w+\\s+in\\s+\\{[\\d,\\s]+\\}\",\n+                line,\n+            ):\n+                findings.append(Finding(\n+                    file=str(path),\n+                    line=i,\n+                    severity=\"WARNING\",\n+                    pattern=\"lookup_table\",\n+                    detail=(\n+                        \"Function uses hardcoded set membership \"\n+                        \"\u2014 may be overfitted to test inputs\"\n+                    ),\n+                ))\n+\n+    return findings\n+\n+\n+def main() -> int:\n+    parser = argparse.ArgumentParser(\n+        description=\"Check test quality for anti-patterns\"\n+    )\n+    parser.add_argument(\n+        \"--strict\",\n+        action=\"store_true\",\n+        help=\"Treat warnings as errors\",\n+    )\n+    parser.add_argument(\n+        \"--path\",\n+        type=str,\n+        default=None,\n+        help=\"Specific path to check (default: tests/ and src/)\",\n+    )\n+    args = parser.parse_args()\n+\n+    repo_root = Path(__file__).resolve().parent.parent\n+    if args.path:\n+        paths = list(Path(args.path).rglob(\"*.py\"))\n+    else:\n+        paths = (\n+            list((repo_root / \"tests\").rglob(\"*.py\"))\n+            + list((repo_root / \"src\").rglob(\"*.py\"))\n+        )\n+\n+    all_findings: list[Finding] = []\n+    for path in paths:\n+        findings = check_file(path)\n+        all_findings.extend(findings)\n+\n+    if not all_findings:\n+        print(\"No test quality issues found.\")\n+        return 0\n+\n+    # Group by severity\n+    critical = [f for f in all_findings if f.severity == \"CRITICAL\"]\n+    warnings = [f for f in all_findings if f.severity == \"WARNING\"]\n+    nits = [f for f in all_findings if f.severity == \"NIT\"]\n+\n+    for finding in all_findings:\n+        print(\n+            f\"[{finding.severity}] {finding.file}:{finding.line} \"\n+            f\"({finding.pattern}): {finding.detail}\"\n+        )\n+\n+    print(f\"\\nTotal: {len(critical)} critical, \"\n+          f\"{len(warnings)} warnings, {len(nits)} nits\")\n+\n+    if critical:\n+        return 1\n+    if args.strict and warnings:\n+        return 1\n+    return 0\n+\n+\n+if __name__ == \"__main__\":\n+    raise SystemExit(main())\n",
      "raw": "#!/usr/bin/env python3\n\"\"\"Test quality checker \u2014 detects stam tests and gaming patterns.\n\nScans test files for anti-patterns that indicate tests pass by\nconstruction rather than exercising real behavior.\n\nUsage:\n    python scripts/check_test_quality.py\n    python scripts/check_test_quality.py --strict\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport ast\nimport re\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\n\n@dataclass\nclass Finding:\n    \"\"\"A test quality finding.\"\"\"\n\n    file: str\n    line: int\n    severity: str  # CRITICAL, WARNING, NIT\n    pattern: str\n    detail: str\n\n\ndef check_file(path: Path) -> list[Finding]:\n    \"\"\"Check a single test file for anti-patterns.\"\"\"\n    findings: list[Finding] = []\n    content = path.read_text()\n    lines = content.splitlines()\n\n    try:\n        tree = ast.parse(content, filename=str(path))\n    except SyntaxError:\n        findings.append(Finding(\n            file=str(path),\n            line=0,\n            severity=\"CRITICAL\",\n            pattern=\"syntax_error\",\n            detail=\"File has syntax errors \u2014 cannot be parsed\",\n        ))\n        return findings\n\n    # Pattern 1: assert True / assert False / assert 1\n    for i, line in enumerate(lines, 1):\n        stripped = line.strip()\n        if re.match(\n            r\"assert\\s+(True|1|not\\s+False|not\\s+0)\\s*$\",\n            stripped,\n        ):\n            findings.append(Finding(\n                file=str(path),\n                line=i,\n                severity=\"CRITICAL\",\n                pattern=\"tautological_assert\",\n                detail=f\"Tautological assertion: {stripped}\",\n            ))\n\n    # Pattern 2: Test functions with no assertions\n    for node in ast.walk(tree):\n        if isinstance(node, ast.FunctionDef) and node.name.startswith(\n            \"test_\"\n        ):\n            has_assert = False\n            for child in ast.walk(node):\n                if isinstance(child, ast.Assert):\n                    has_assert = True\n                    break\n                # pytest.raises counts as an assertion\n                if isinstance(child, ast.Attribute) and (\n                    child.attr == \"raises\"\n                ):\n                    has_assert = True\n                    break\n                # Check for assert method calls\n                if isinstance(child, ast.Call):\n                    func = child.func\n                    if isinstance(func, ast.Attribute):\n                        if func.attr.startswith(\"assert\"):\n                            has_assert = True\n                            break\n            if not has_assert:\n                findings.append(Finding(\n                    file=str(path),\n                    line=node.lineno,\n                    severity=\"WARNING\",\n                    pattern=\"no_assertions\",\n                    detail=(\n                        f\"Test '{node.name}' has no assertions \"\n                        \"\u2014 may pass vacuously\"\n                    ),\n                ))\n\n    # Pattern 3: Excessive mocking (more than 3 @patch decorators)\n    for node in ast.walk(tree):\n        if isinstance(node, ast.FunctionDef) and node.name.startswith(\n            \"test_\"\n        ):\n            patch_count = 0\n            for decorator in node.decorator_list:\n                if isinstance(decorator, ast.Call):\n                    func = decorator.func\n                    if isinstance(func, ast.Attribute):\n                        if func.attr == \"patch\":\n                            patch_count += 1\n                    elif isinstance(func, ast.Name):\n                        if func.id == \"patch\":\n                            patch_count += 1\n                elif isinstance(decorator, ast.Attribute):\n                    if decorator.attr == \"patch\":\n                        patch_count += 1\n            if patch_count >= 3:\n                findings.append(Finding(\n                    file=str(path),\n                    line=node.lineno,\n                    severity=\"WARNING\",\n                    pattern=\"excessive_mocking\",\n                    detail=(\n                        f\"Test '{node.name}' has {patch_count} \"\n                        \"@patch decorators \u2014 likely testing mocks, \"\n                        \"not real code\"\n                    ),\n                ))\n\n    # Pattern 4: Stub implementations in src/ (not test files)\n    # This is only checked for non-test files\n    if \"/tests/\" not in str(path):\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef):\n                body = node.body\n                # Function body is just `pass`\n                if (\n                    len(body) == 1\n                    and isinstance(body[0], ast.Pass)\n                ):\n                    findings.append(Finding(\n                        file=str(path),\n                        line=node.lineno,\n                        severity=\"WARNING\",\n                        pattern=\"stub_implementation\",\n                        detail=(\n                            f\"Function '{node.name}' is a stub \"\n                            \"(just `pass`)\"\n                        ),\n                    ))\n                # Function body is just `return True`\n                if (\n                    len(body) == 1\n                    and isinstance(body[0], ast.Return)\n                    and isinstance(body[0].value, ast.Constant)\n                    and body[0].value.value is True\n                ):\n                    findings.append(Finding(\n                        file=str(path),\n                        line=node.lineno,\n                        severity=\"CRITICAL\",\n                        pattern=\"hardcoded_return\",\n                        detail=(\n                            f\"Function '{node.name}' always \"\n                            \"returns True \u2014 likely a stub\"\n                        ),\n                    ))\n\n    # Pattern 5: Hardcoded lookup tables in src/\n    if \"/tests/\" not in str(path):\n        for i, line in enumerate(lines, 1):\n            # Detect `return x in {literal, literal, ...}`\n            if re.search(\n                r\"return\\s+\\w+\\s+in\\s+\\{[\\d,\\s]+\\}\",\n                line,\n            ):\n                findings.append(Finding(\n                    file=str(path),\n                    line=i,\n                    severity=\"WARNING\",\n                    pattern=\"lookup_table\",\n                    detail=(\n                        \"Function uses hardcoded set membership \"\n                        \"\u2014 may be overfitted to test inputs\"\n                    ),\n                ))\n\n    return findings\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(\n        description=\"Check test quality for anti-patterns\"\n    )\n    parser.add_argument(\n        \"--strict\",\n        action=\"store_true\",\n        help=\"Treat warnings as errors\",\n    )\n    parser.add_argument(\n        \"--path\",\n        type=str,\n        default=None,\n        help=\"Specific path to check (default: tests/ and src/)\",\n    )\n    args = parser.parse_args()\n\n    repo_root = Path(__file__).resolve().parent.parent\n    if args.path:\n        paths = list(Path(args.path).rglob(\"*.py\"))\n    else:\n        paths = (\n            list((repo_root / \"tests\").rglob(\"*.py\"))\n            + list((repo_root / \"src\").rglob(\"*.py\"))\n        )\n\n    all_findings: list[Finding] = []\n    for path in paths:\n        findings = check_file(path)\n        all_findings.extend(findings)\n\n    if not all_findings:\n        print(\"No test quality issues found.\")\n        return 0\n\n    # Group by severity\n    critical = [f for f in all_findings if f.severity == \"CRITICAL\"]\n    warnings = [f for f in all_findings if f.severity == \"WARNING\"]\n    nits = [f for f in all_findings if f.severity == \"NIT\"]\n\n    for finding in all_findings:\n        print(\n            f\"[{finding.severity}] {finding.file}:{finding.line} \"\n            f\"({finding.pattern}): {finding.detail}\"\n        )\n\n    print(f\"\\nTotal: {len(critical)} critical, \"\n          f\"{len(warnings)} warnings, {len(nits)} nits\")\n\n    if critical:\n        return 1\n    if args.strict and warnings:\n        return 1\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n",
      "base": ""
    },
    "scripts/compile_feedback.py": {
      "additions": 10,
      "deletions": 8,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/scripts/compile_feedback.py b/scripts/compile_feedback.py\nindex 4baa811..c68ec76 100644\n--- a/scripts/compile_feedback.py\n+++ b/scripts/compile_feedback.py\n@@ -24,7 +24,8 @@ def load_scenario_results(\n     \"\"\"Load scenario results JSON.\"\"\"\n     if not path.exists():\n         return None\n-    return json.loads(path.read_text())  # type: ignore[no-any-return]\n+    data: dict[str, object] = json.loads(path.read_text())\n+    return data\n \n \n def load_ci_log(path: Path) -> str:\n@@ -109,18 +110,19 @@ def infer_causes(\n         combined = stderr + stdout\n         name = str(r.get(\"name\", \"unknown\"))\n \n+        # Classify by primary error type (elif prevents double-counting)\n         if \"ModuleNotFoundError\" in combined:\n             import_errors.append(name)\n         elif \"ImportError\" in combined:\n             import_errors.append(name)\n-        if \"AssertionError\" in combined:\n-            assertion_errors.append(name)\n-        if \"TIMEOUT\" in combined:\n+        elif \"TIMEOUT\" in combined:\n             timeout_errors.append(name)\n-        if \"FileNotFoundError\" in combined:\n+        elif \"FileNotFoundError\" in combined:\n             file_not_found.append(name)\n         elif \"No such file\" in combined:\n             file_not_found.append(name)\n+        elif \"AssertionError\" in combined:\n+            assertion_errors.append(name)\n \n     n = len(import_errors)\n     if import_errors:\n@@ -203,6 +205,7 @@ def compile_feedback(\n         out.append(\"|-----------|---------|\")\n         for iter_num, summary in previous_feedback:\n             first = summary.splitlines()[0] if summary else \"\"\n+            first = first.replace(\"|\", \"\\\\|\")\n             out.append(f\"| {iter_num} | {first} |\")\n         out.append(\"\")\n \n@@ -323,9 +326,8 @@ def main() -> int:\n     output_path = factory_dir / f\"feedback_iter_{iteration}.md\"\n     output_path.write_text(feedback)\n \n-    # Update iteration count\n-    count_path = factory_dir / \"iteration_count.txt\"\n-    count_path.write_text(str(iteration) + \"\\n\")\n+    # NOTE: iteration_count.txt is owned by the workflow/Makefile,\n+    # not by this script. Do not write it here to avoid double-increment.\n \n     print(f\"Feedback compiled: {output_path}\")\n     print(f\"Iteration: {iteration}\")\n",
      "raw": "#!/usr/bin/env python3\n\"\"\"Feedback compiler for the dark factory convergence loop.\n\nReads scenario results, CI logs, and previous iteration feedback\nto produce structured feedback markdown that Codex can act on.\n\nUsage:\n    python scripts/compile_feedback.py\n    python scripts/compile_feedback.py --iteration 3\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport glob\nimport json\nimport time\nfrom pathlib import Path\n\n\ndef load_scenario_results(\n    path: Path,\n) -> dict[str, object] | None:\n    \"\"\"Load scenario results JSON.\"\"\"\n    if not path.exists():\n        return None\n    data: dict[str, object] = json.loads(path.read_text())\n    return data\n\n\ndef load_ci_log(path: Path) -> str:\n    \"\"\"Load CI output log.\"\"\"\n    if not path.exists():\n        return \"(no CI log available)\"\n    content = path.read_text()\n    if len(content) > 10000:\n        return (\n            content[:5000]\n            + \"\\n\\n... [truncated] ...\\n\\n\"\n            + content[-5000:]\n        )\n    return content\n\n\ndef get_iteration_count(factory_dir: Path) -> int:\n    \"\"\"Read current iteration count.\"\"\"\n    count_file = factory_dir / \"iteration_count.txt\"\n    if count_file.exists():\n        try:\n            return int(count_file.read_text().strip())\n        except ValueError:\n            return 0\n    return 0\n\n\ndef get_previous_feedback(\n    factory_dir: Path,\n) -> list[tuple[int, str]]:\n    \"\"\"Load previous feedback files for trajectory tracking.\"\"\"\n    feedbacks: list[tuple[int, str]] = []\n    pattern = str(factory_dir / \"feedback_iter_*.md\")\n    for f in sorted(glob.glob(pattern)):\n        path = Path(f)\n        try:\n            iter_num = int(path.stem.split(\"_\")[-1])\n        except ValueError:\n            continue\n        content = path.read_text()\n        lines = content.splitlines()\n        summary_lines: list[str] = []\n        in_summary = False\n        for line in lines:\n            if line.startswith(\"## Summary\"):\n                in_summary = True\n                continue\n            if in_summary and line.startswith(\"## \"):\n                break\n            if in_summary:\n                summary_lines.append(line)\n        feedbacks.append(\n            (iter_num, \"\\n\".join(summary_lines).strip())\n        )\n    return feedbacks\n\n\ndef _fmt_list(names: list[str]) -> str:\n    \"\"\"Format a list of scenario names for display.\"\"\"\n    return \", \".join(names)\n\n\ndef infer_causes(\n    results: dict[str, object],\n) -> list[str]:\n    \"\"\"Infer likely root causes from error patterns.\"\"\"\n    causes: list[str] = []\n    result_list: list[dict[str, object]] = results.get(\n        \"results\", []\n    )  # type: ignore[assignment]\n\n    import_errors: list[str] = []\n    assertion_errors: list[str] = []\n    timeout_errors: list[str] = []\n    file_not_found: list[str] = []\n\n    for r in result_list:\n        if r.get(\"passed\"):\n            continue\n        stderr = str(r.get(\"stderr\", \"\"))\n        stdout = str(r.get(\"stdout\", \"\"))\n        combined = stderr + stdout\n        name = str(r.get(\"name\", \"unknown\"))\n\n        # Classify by primary error type (elif prevents double-counting)\n        if \"ModuleNotFoundError\" in combined:\n            import_errors.append(name)\n        elif \"ImportError\" in combined:\n            import_errors.append(name)\n        elif \"TIMEOUT\" in combined:\n            timeout_errors.append(name)\n        elif \"FileNotFoundError\" in combined:\n            file_not_found.append(name)\n        elif \"No such file\" in combined:\n            file_not_found.append(name)\n        elif \"AssertionError\" in combined:\n            assertion_errors.append(name)\n\n    n = len(import_errors)\n    if import_errors:\n        causes.append(\n            f\"Import errors in {n} scenario(s): \"\n            f\"{_fmt_list(import_errors)}. \"\n            \"Likely missing module or wrong import path.\"\n        )\n    n = len(assertion_errors)\n    if assertion_errors:\n        causes.append(\n            f\"Assertion failures in {n} scenario(s): \"\n            f\"{_fmt_list(assertion_errors)}. \"\n            \"Check the specific assertion messages.\"\n        )\n    n = len(timeout_errors)\n    if timeout_errors:\n        causes.append(\n            f\"Timeouts in {n} scenario(s): \"\n            f\"{_fmt_list(timeout_errors)}. \"\n            \"Possible infinite loop or slow computation.\"\n        )\n    n = len(file_not_found)\n    if file_not_found:\n        causes.append(\n            f\"Missing files in {n} scenario(s): \"\n            f\"{_fmt_list(file_not_found)}. \"\n            \"Expected artifacts not being created.\"\n        )\n    if not causes:\n        causes.append(\n            \"No clear pattern detected \"\n            \"\u2014 review individual failure details below.\"\n        )\n\n    return causes\n\n\ndef compile_feedback(\n    results: dict[str, object] | None,\n    ci_log: str,\n    iteration: int,\n    previous_feedback: list[tuple[int, str]],\n) -> str:\n    \"\"\"Compile all inputs into structured feedback markdown.\"\"\"\n    out: list[str] = []\n\n    # Header\n    ts = time.strftime(\"%Y-%m-%d %H:%M:%S UTC\", time.gmtime())\n    out.append(f\"# Factory Feedback \u2014 Iteration {iteration}\")\n    out.append(f\"Generated: {ts}\")\n    out.append(\"\")\n\n    # Summary\n    out.append(\"## Summary\")\n    if results:\n        total = results.get(\"total\", 0)\n        passed = results.get(\"passed\", 0)\n        failed = results.get(\"failed\", 0)\n        score = results.get(\"satisfaction_score\", 0.0)\n        out.append(\n            f\"- **Satisfaction score: {score:.0%}** \"\n            f\"({passed}/{total} scenarios passed)\"\n        )\n        out.append(\n            f\"- Passed: {passed} | Failed: {failed} \"\n            f\"| Total: {total}\"\n        )\n    else:\n        out.append(\n            \"- **No scenario results available** \"\n            \"\u2014 scenarios did not run (likely Layer 1 failure)\"\n        )\n    out.append(\"\")\n\n    # Convergence trajectory\n    if previous_feedback:\n        out.append(\"## Convergence Trajectory\")\n        out.append(\"| Iteration | Summary |\")\n        out.append(\"|-----------|---------|\")\n        for iter_num, summary in previous_feedback:\n            first = summary.splitlines()[0] if summary else \"\"\n            first = first.replace(\"|\", \"\\\\|\")\n            out.append(f\"| {iter_num} | {first} |\")\n        out.append(\"\")\n\n    # Inferred causes\n    if results:\n        causes = infer_causes(results)\n        out.append(\"## Likely Root Causes\")\n        for i, cause in enumerate(causes, 1):\n            out.append(f\"{i}. {cause}\")\n        out.append(\"\")\n\n    # Failed scenario details\n    if results:\n        rlist: list[dict[str, object]] = results.get(\n            \"results\", []\n        )  # type: ignore[assignment]\n        failed = [r for r in rlist if not r.get(\"passed\")]\n        if failed:\n            out.append(\"## Failed Scenarios \u2014 Full Details\")\n            out.append(\"\")\n            for r in failed:\n                name = r.get(\"name\", \"Unknown\")\n                cat = r.get(\"category\", \"unknown\")\n                code = r.get(\"exit_code\", \"N/A\")\n                dur = r.get(\"duration_seconds\", 0)\n                err = r.get(\"error_summary\", \"N/A\")\n                out.append(f\"### {name}\")\n                out.append(f\"**Category:** {cat}\")\n                out.append(f\"**Exit code:** {code}\")\n                out.append(f\"**Duration:** {dur}s\")\n                out.append(f\"**Error summary:** {err}\")\n                out.append(\"\")\n                se = str(r.get(\"stderr\", \"\")).strip()\n                so = str(r.get(\"stdout\", \"\")).strip()\n                if se:\n                    out.append(\"**stderr:**\")\n                    out.append(f\"```\\n{se}\\n```\")\n                if so:\n                    out.append(\"**stdout:**\")\n                    out.append(f\"```\\n{so}\\n```\")\n                out.append(\"\")\n\n    # CI log\n    if ci_log and ci_log != \"(no CI log available)\":\n        out.append(\"## CI Log Output\")\n        out.append(f\"```\\n{ci_log}\\n```\")\n        out.append(\"\")\n\n    # Instructions\n    out.append(\"## Instructions for Coding Agent\")\n    out.append(\"\")\n    out.append(\"Fix the failures above. Priorities:\")\n    out.append(\"1. Import errors and missing modules first\")\n    out.append(\"2. File/artifact production issues next\")\n    out.append(\"3. Behavioral assertion failures last\")\n    out.append(\"\")\n    out.append(\"Constraints:\")\n    out.append(\n        \"- Do NOT modify /scenarios/, /scripts/, \"\n        \"or /.github/workflows/factory.yaml\"\n    )\n    out.append(\n        \"- Do NOT modify /specs/ \u2014 read them as requirements\"\n    )\n    out.append(\n        \"- Keep changes minimal \u2014 fix what's broken, \"\n        \"don't refactor\"\n    )\n    out.append(\"\")\n\n    return \"\\n\".join(out)\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(\n        description=\"Compile factory feedback\"\n    )\n    parser.add_argument(\n        \"--iteration\",\n        type=int,\n        default=None,\n        help=\"Override iteration number (default: auto)\",\n    )\n    parser.add_argument(\n        \"--factory-dir\",\n        type=str,\n        default=None,\n        help=\"Factory artifacts dir (default: artifacts/factory/)\",\n    )\n    args = parser.parse_args()\n\n    repo_root = Path(__file__).resolve().parent.parent\n    if args.factory_dir:\n        factory_dir = Path(args.factory_dir)\n    else:\n        factory_dir = repo_root / \"artifacts\" / \"factory\"\n    factory_dir.mkdir(parents=True, exist_ok=True)\n\n    # Determine iteration\n    if args.iteration is not None:\n        iteration = args.iteration\n    else:\n        iteration = get_iteration_count(factory_dir) + 1\n\n    # Load inputs\n    results = load_scenario_results(\n        factory_dir / \"scenario_results.json\"\n    )\n    ci_log = load_ci_log(factory_dir / \"ci_output.log\")\n    previous_feedback = get_previous_feedback(factory_dir)\n\n    # Compile\n    feedback = compile_feedback(\n        results, ci_log, iteration, previous_feedback\n    )\n\n    # Write feedback file\n    output_path = factory_dir / f\"feedback_iter_{iteration}.md\"\n    output_path.write_text(feedback)\n\n    # NOTE: iteration_count.txt is owned by the workflow/Makefile,\n    # not by this script. Do not write it here to avoid double-increment.\n\n    print(f\"Feedback compiled: {output_path}\")\n    print(f\"Iteration: {iteration}\")\n    if results:\n        score = results.get(\"satisfaction_score\", 0)\n        print(f\"Satisfaction: {score:.0%}\")\n    else:\n        print(\"Satisfaction: N/A (no scenario results)\")\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n",
      "base": "#!/usr/bin/env python3\n\"\"\"Feedback compiler for the dark factory convergence loop.\n\nReads scenario results, CI logs, and previous iteration feedback\nto produce structured feedback markdown that Codex can act on.\n\nUsage:\n    python scripts/compile_feedback.py\n    python scripts/compile_feedback.py --iteration 3\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport glob\nimport json\nimport time\nfrom pathlib import Path\n\n\ndef load_scenario_results(\n    path: Path,\n) -> dict[str, object] | None:\n    \"\"\"Load scenario results JSON.\"\"\"\n    if not path.exists():\n        return None\n    return json.loads(path.read_text())  # type: ignore[no-any-return]\n\n\ndef load_ci_log(path: Path) -> str:\n    \"\"\"Load CI output log.\"\"\"\n    if not path.exists():\n        return \"(no CI log available)\"\n    content = path.read_text()\n    if len(content) > 10000:\n        return (\n            content[:5000]\n            + \"\\n\\n... [truncated] ...\\n\\n\"\n            + content[-5000:]\n        )\n    return content\n\n\ndef get_iteration_count(factory_dir: Path) -> int:\n    \"\"\"Read current iteration count.\"\"\"\n    count_file = factory_dir / \"iteration_count.txt\"\n    if count_file.exists():\n        try:\n            return int(count_file.read_text().strip())\n        except ValueError:\n            return 0\n    return 0\n\n\ndef get_previous_feedback(\n    factory_dir: Path,\n) -> list[tuple[int, str]]:\n    \"\"\"Load previous feedback files for trajectory tracking.\"\"\"\n    feedbacks: list[tuple[int, str]] = []\n    pattern = str(factory_dir / \"feedback_iter_*.md\")\n    for f in sorted(glob.glob(pattern)):\n        path = Path(f)\n        try:\n            iter_num = int(path.stem.split(\"_\")[-1])\n        except ValueError:\n            continue\n        content = path.read_text()\n        lines = content.splitlines()\n        summary_lines: list[str] = []\n        in_summary = False\n        for line in lines:\n            if line.startswith(\"## Summary\"):\n                in_summary = True\n                continue\n            if in_summary and line.startswith(\"## \"):\n                break\n            if in_summary:\n                summary_lines.append(line)\n        feedbacks.append(\n            (iter_num, \"\\n\".join(summary_lines).strip())\n        )\n    return feedbacks\n\n\ndef _fmt_list(names: list[str]) -> str:\n    \"\"\"Format a list of scenario names for display.\"\"\"\n    return \", \".join(names)\n\n\ndef infer_causes(\n    results: dict[str, object],\n) -> list[str]:\n    \"\"\"Infer likely root causes from error patterns.\"\"\"\n    causes: list[str] = []\n    result_list: list[dict[str, object]] = results.get(\n        \"results\", []\n    )  # type: ignore[assignment]\n\n    import_errors: list[str] = []\n    assertion_errors: list[str] = []\n    timeout_errors: list[str] = []\n    file_not_found: list[str] = []\n\n    for r in result_list:\n        if r.get(\"passed\"):\n            continue\n        stderr = str(r.get(\"stderr\", \"\"))\n        stdout = str(r.get(\"stdout\", \"\"))\n        combined = stderr + stdout\n        name = str(r.get(\"name\", \"unknown\"))\n\n        if \"ModuleNotFoundError\" in combined:\n            import_errors.append(name)\n        elif \"ImportError\" in combined:\n            import_errors.append(name)\n        if \"AssertionError\" in combined:\n            assertion_errors.append(name)\n        if \"TIMEOUT\" in combined:\n            timeout_errors.append(name)\n        if \"FileNotFoundError\" in combined:\n            file_not_found.append(name)\n        elif \"No such file\" in combined:\n            file_not_found.append(name)\n\n    n = len(import_errors)\n    if import_errors:\n        causes.append(\n            f\"Import errors in {n} scenario(s): \"\n            f\"{_fmt_list(import_errors)}. \"\n            \"Likely missing module or wrong import path.\"\n        )\n    n = len(assertion_errors)\n    if assertion_errors:\n        causes.append(\n            f\"Assertion failures in {n} scenario(s): \"\n            f\"{_fmt_list(assertion_errors)}. \"\n            \"Check the specific assertion messages.\"\n        )\n    n = len(timeout_errors)\n    if timeout_errors:\n        causes.append(\n            f\"Timeouts in {n} scenario(s): \"\n            f\"{_fmt_list(timeout_errors)}. \"\n            \"Possible infinite loop or slow computation.\"\n        )\n    n = len(file_not_found)\n    if file_not_found:\n        causes.append(\n            f\"Missing files in {n} scenario(s): \"\n            f\"{_fmt_list(file_not_found)}. \"\n            \"Expected artifacts not being created.\"\n        )\n    if not causes:\n        causes.append(\n            \"No clear pattern detected \"\n            \"\u2014 review individual failure details below.\"\n        )\n\n    return causes\n\n\ndef compile_feedback(\n    results: dict[str, object] | None,\n    ci_log: str,\n    iteration: int,\n    previous_feedback: list[tuple[int, str]],\n) -> str:\n    \"\"\"Compile all inputs into structured feedback markdown.\"\"\"\n    out: list[str] = []\n\n    # Header\n    ts = time.strftime(\"%Y-%m-%d %H:%M:%S UTC\", time.gmtime())\n    out.append(f\"# Factory Feedback \u2014 Iteration {iteration}\")\n    out.append(f\"Generated: {ts}\")\n    out.append(\"\")\n\n    # Summary\n    out.append(\"## Summary\")\n    if results:\n        total = results.get(\"total\", 0)\n        passed = results.get(\"passed\", 0)\n        failed = results.get(\"failed\", 0)\n        score = results.get(\"satisfaction_score\", 0.0)\n        out.append(\n            f\"- **Satisfaction score: {score:.0%}** \"\n            f\"({passed}/{total} scenarios passed)\"\n        )\n        out.append(\n            f\"- Passed: {passed} | Failed: {failed} \"\n            f\"| Total: {total}\"\n        )\n    else:\n        out.append(\n            \"- **No scenario results available** \"\n            \"\u2014 scenarios did not run (likely Layer 1 failure)\"\n        )\n    out.append(\"\")\n\n    # Convergence trajectory\n    if previous_feedback:\n        out.append(\"## Convergence Trajectory\")\n        out.append(\"| Iteration | Summary |\")\n        out.append(\"|-----------|---------|\")\n        for iter_num, summary in previous_feedback:\n            first = summary.splitlines()[0] if summary else \"\"\n            out.append(f\"| {iter_num} | {first} |\")\n        out.append(\"\")\n\n    # Inferred causes\n    if results:\n        causes = infer_causes(results)\n        out.append(\"## Likely Root Causes\")\n        for i, cause in enumerate(causes, 1):\n            out.append(f\"{i}. {cause}\")\n        out.append(\"\")\n\n    # Failed scenario details\n    if results:\n        rlist: list[dict[str, object]] = results.get(\n            \"results\", []\n        )  # type: ignore[assignment]\n        failed = [r for r in rlist if not r.get(\"passed\")]\n        if failed:\n            out.append(\"## Failed Scenarios \u2014 Full Details\")\n            out.append(\"\")\n            for r in failed:\n                name = r.get(\"name\", \"Unknown\")\n                cat = r.get(\"category\", \"unknown\")\n                code = r.get(\"exit_code\", \"N/A\")\n                dur = r.get(\"duration_seconds\", 0)\n                err = r.get(\"error_summary\", \"N/A\")\n                out.append(f\"### {name}\")\n                out.append(f\"**Category:** {cat}\")\n                out.append(f\"**Exit code:** {code}\")\n                out.append(f\"**Duration:** {dur}s\")\n                out.append(f\"**Error summary:** {err}\")\n                out.append(\"\")\n                se = str(r.get(\"stderr\", \"\")).strip()\n                so = str(r.get(\"stdout\", \"\")).strip()\n                if se:\n                    out.append(\"**stderr:**\")\n                    out.append(f\"```\\n{se}\\n```\")\n                if so:\n                    out.append(\"**stdout:**\")\n                    out.append(f\"```\\n{so}\\n```\")\n                out.append(\"\")\n\n    # CI log\n    if ci_log and ci_log != \"(no CI log available)\":\n        out.append(\"## CI Log Output\")\n        out.append(f\"```\\n{ci_log}\\n```\")\n        out.append(\"\")\n\n    # Instructions\n    out.append(\"## Instructions for Coding Agent\")\n    out.append(\"\")\n    out.append(\"Fix the failures above. Priorities:\")\n    out.append(\"1. Import errors and missing modules first\")\n    out.append(\"2. File/artifact production issues next\")\n    out.append(\"3. Behavioral assertion failures last\")\n    out.append(\"\")\n    out.append(\"Constraints:\")\n    out.append(\n        \"- Do NOT modify /scenarios/, /scripts/, \"\n        \"or /.github/workflows/factory.yaml\"\n    )\n    out.append(\n        \"- Do NOT modify /specs/ \u2014 read them as requirements\"\n    )\n    out.append(\n        \"- Keep changes minimal \u2014 fix what's broken, \"\n        \"don't refactor\"\n    )\n    out.append(\"\")\n\n    return \"\\n\".join(out)\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(\n        description=\"Compile factory feedback\"\n    )\n    parser.add_argument(\n        \"--iteration\",\n        type=int,\n        default=None,\n        help=\"Override iteration number (default: auto)\",\n    )\n    parser.add_argument(\n        \"--factory-dir\",\n        type=str,\n        default=None,\n        help=\"Factory artifacts dir (default: artifacts/factory/)\",\n    )\n    args = parser.parse_args()\n\n    repo_root = Path(__file__).resolve().parent.parent\n    if args.factory_dir:\n        factory_dir = Path(args.factory_dir)\n    else:\n        factory_dir = repo_root / \"artifacts\" / \"factory\"\n    factory_dir.mkdir(parents=True, exist_ok=True)\n\n    # Determine iteration\n    if args.iteration is not None:\n        iteration = args.iteration\n    else:\n        iteration = get_iteration_count(factory_dir) + 1\n\n    # Load inputs\n    results = load_scenario_results(\n        factory_dir / \"scenario_results.json\"\n    )\n    ci_log = load_ci_log(factory_dir / \"ci_output.log\")\n    previous_feedback = get_previous_feedback(factory_dir)\n\n    # Compile\n    feedback = compile_feedback(\n        results, ci_log, iteration, previous_feedback\n    )\n\n    # Write feedback file\n    output_path = factory_dir / f\"feedback_iter_{iteration}.md\"\n    output_path.write_text(feedback)\n\n    # Update iteration count\n    count_path = factory_dir / \"iteration_count.txt\"\n    count_path.write_text(str(iteration) + \"\\n\")\n\n    print(f\"Feedback compiled: {output_path}\")\n    print(f\"Iteration: {iteration}\")\n    if results:\n        score = results.get(\"satisfaction_score\", 0)\n        print(f\"Satisfaction: {score:.0%}\")\n    else:\n        print(\"Satisfaction: N/A (no scenario results)\")\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n"
    },
    "scripts/nfr_checks.py": {
      "additions": 442,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/scripts/nfr_checks.py b/scripts/nfr_checks.py\nnew file mode 100644\nindex 0000000..74348cd\n--- /dev/null\n+++ b/scripts/nfr_checks.py\n@@ -0,0 +1,442 @@\n+#!/usr/bin/env python3\n+\"\"\"Non-Functional Requirements (NFR) checker \u2014 Gate 2.\n+\n+Extensible framework for running non-blocking quality checks beyond\n+lint/typecheck/test. Each check produces findings that feed into\n+feedback and the LLM-as-judge's holistic evaluation.\n+\n+Adding a new NFR check:\n+    1. Create a function: def check_<name>(repo_root: Path) -> list[NFRFinding]\n+    2. Register it in NFR_CHECKS dict at the bottom of this file\n+    3. The factory loop will pick it up automatically\n+\n+Usage:\n+    python scripts/nfr_checks.py                    # run all checks\n+    python scripts/nfr_checks.py --check complexity  # run one check\n+    python scripts/nfr_checks.py --json              # JSON output for automation\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import subprocess\n+import sys\n+from dataclasses import asdict, dataclass\n+from pathlib import Path\n+\n+\n+@dataclass\n+class NFRFinding:\n+    \"\"\"A single NFR finding.\"\"\"\n+\n+    nfr: str  # Which NFR this belongs to\n+    severity: str  # CRITICAL, WARNING, NIT, INFO\n+    message: str  # Human-readable description\n+    file: str = \"\"  # Optional: specific file\n+    line: int = 0  # Optional: specific line\n+    metric: str = \"\"  # Optional: metric name\n+    value: str = \"\"  # Optional: metric value\n+    threshold: str = \"\"  # Optional: threshold that was exceeded\n+\n+\n+@dataclass\n+class NFRResult:\n+    \"\"\"Result of running one NFR check.\"\"\"\n+\n+    name: str\n+    status: str  # passed, failed, skipped, error\n+    findings: list[NFRFinding]\n+    tool: str  # What tool was used\n+    summary: str  # One-line summary\n+\n+\n+def _run_tool(\n+    cmd: list[str], repo_root: Path\n+) -> subprocess.CompletedProcess[str]:\n+    \"\"\"Run a tool, returning the result without raising on failure.\"\"\"\n+    try:\n+        return subprocess.run(\n+            cmd,\n+            cwd=str(repo_root),\n+            capture_output=True,\n+            text=True,\n+            timeout=60,\n+        )\n+    except FileNotFoundError:\n+        result = subprocess.CompletedProcess(\n+            cmd, returncode=-1, stdout=\"\", stderr=f\"Tool not found: {cmd[0]}\"\n+        )\n+        return result\n+    except subprocess.TimeoutExpired:\n+        result = subprocess.CompletedProcess(\n+            cmd, returncode=-2, stdout=\"\", stderr=f\"Timeout: {' '.join(cmd)}\"\n+        )\n+        return result\n+\n+\n+# \u2500\u2500 NFR Check Implementations \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+def check_code_quality(repo_root: Path) -> list[NFRFinding]:\n+    \"\"\"Extended code quality beyond basic lint.\n+\n+    Uses ruff with broader rule selection including:\n+    - C90: McCabe complexity\n+    - S: Security (bandit-equivalent)\n+    - SIM: Simplification suggestions\n+    - RET: Return statement issues\n+    \"\"\"\n+    findings: list[NFRFinding] = []\n+\n+    result = _run_tool(\n+        [\n+            \"ruff\",\n+            \"check\",\n+            \"src/\",\n+            \"--select\",\n+            \"C90,S,SIM,RET,PTH,ERA\",\n+            \"--no-fix\",\n+            \"--output-format\",\n+            \"json\",\n+        ],\n+        repo_root,\n+    )\n+\n+    if result.returncode == -1:\n+        findings.append(\n+            NFRFinding(\n+                nfr=\"code_quality\",\n+                severity=\"INFO\",\n+                message=\"ruff not available \u2014 install with: pip install ruff\",\n+            )\n+        )\n+        return findings\n+\n+    if result.stdout.strip():\n+        try:\n+            issues = json.loads(result.stdout)\n+            for issue in issues:\n+                findings.append(\n+                    NFRFinding(\n+                        nfr=\"code_quality\",\n+                        severity=\"WARNING\",\n+                        message=f\"{issue.get('code', '?')}: {issue.get('message', 'unknown')}\",\n+                        file=issue.get(\"filename\", \"\"),\n+                        line=issue.get(\"location\", {}).get(\"row\", 0),\n+                    )\n+                )\n+        except json.JSONDecodeError:\n+            # Fall back to text output\n+            for line in result.stdout.strip().splitlines():\n+                findings.append(\n+                    NFRFinding(\n+                        nfr=\"code_quality\",\n+                        severity=\"WARNING\",\n+                        message=line.strip(),\n+                    )\n+                )\n+\n+    return findings\n+\n+\n+def check_complexity(repo_root: Path) -> list[NFRFinding]:\n+    \"\"\"Cyclomatic complexity via radon.\n+\n+    Flags functions with complexity grade C or worse.\n+    \"\"\"\n+    findings: list[NFRFinding] = []\n+\n+    result = _run_tool(\n+        [\"radon\", \"cc\", \"src/\", \"--min\", \"C\", \"--json\"],\n+        repo_root,\n+    )\n+\n+    if result.returncode == -1:\n+        findings.append(\n+            NFRFinding(\n+                nfr=\"complexity\",\n+                severity=\"INFO\",\n+                message=\"radon not available \u2014 install with: pip install radon\",\n+            )\n+        )\n+        return findings\n+\n+    if result.stdout.strip():\n+        try:\n+            data = json.loads(result.stdout)\n+            for filepath, blocks in data.items():\n+                for block in blocks:\n+                    findings.append(\n+                        NFRFinding(\n+                            nfr=\"complexity\",\n+                            severity=\"WARNING\",\n+                            message=(\n+                                f\"{block.get('type', '?')} '{block.get('name', '?')}' \"\n+                                f\"has complexity {block.get('complexity', '?')} \"\n+                                f\"(grade {block.get('rank', '?')})\"\n+                            ),\n+                            file=filepath,\n+                            line=block.get(\"lineno\", 0),\n+                            metric=\"cyclomatic_complexity\",\n+                            value=str(block.get(\"complexity\", \"\")),\n+                            threshold=\"C\",\n+                        )\n+                    )\n+        except json.JSONDecodeError:\n+            pass\n+\n+    return findings\n+\n+\n+def check_dead_code(repo_root: Path) -> list[NFRFinding]:\n+    \"\"\"Dead code detection via vulture.\"\"\"\n+    findings: list[NFRFinding] = []\n+\n+    result = _run_tool(\n+        [\"vulture\", \"src/\", \"--min-confidence\", \"80\"],\n+        repo_root,\n+    )\n+\n+    if result.returncode == -1:\n+        findings.append(\n+            NFRFinding(\n+                nfr=\"dead_code\",\n+                severity=\"INFO\",\n+                message=\"vulture not available \u2014 install with: pip install vulture\",\n+            )\n+        )\n+        return findings\n+\n+    if result.stdout.strip():\n+        for line in result.stdout.strip().splitlines():\n+            # vulture output: \"src/file.py:10: unused function 'foo' (90% confidence)\"\n+            findings.append(\n+                NFRFinding(\n+                    nfr=\"dead_code\",\n+                    severity=\"WARNING\",\n+                    message=line.strip(),\n+                )\n+            )\n+\n+    return findings\n+\n+\n+def check_security(repo_root: Path) -> list[NFRFinding]:\n+    \"\"\"Security vulnerability detection via bandit.\"\"\"\n+    findings: list[NFRFinding] = []\n+\n+    result = _run_tool(\n+        [\"bandit\", \"-r\", \"src/\", \"-f\", \"json\", \"-q\"],\n+        repo_root,\n+    )\n+\n+    if result.returncode == -1:\n+        findings.append(\n+            NFRFinding(\n+                nfr=\"security\",\n+                severity=\"INFO\",\n+                message=\"bandit not available \u2014 install with: pip install bandit\",\n+            )\n+        )\n+        return findings\n+\n+    if result.stdout.strip():\n+        try:\n+            data = json.loads(result.stdout)\n+            for issue in data.get(\"results\", []):\n+                severity_map = {\n+                    \"HIGH\": \"CRITICAL\",\n+                    \"MEDIUM\": \"WARNING\",\n+                    \"LOW\": \"NIT\",\n+                }\n+                findings.append(\n+                    NFRFinding(\n+                        nfr=\"security\",\n+                        severity=severity_map.get(\n+                            issue.get(\"issue_severity\", \"\"), \"WARNING\"\n+                        ),\n+                        message=(\n+                            f\"{issue.get('test_id', '?')}: \"\n+                            f\"{issue.get('issue_text', 'unknown')}\"\n+                        ),\n+                        file=issue.get(\"filename\", \"\"),\n+                        line=issue.get(\"line_number\", 0),\n+                    )\n+                )\n+        except json.JSONDecodeError:\n+            pass\n+\n+    return findings\n+\n+\n+# \u2500\u2500 NFR Registry \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+# Add new checks here. The key is the check name, the value is\n+# (function, tool_name, description).\n+\n+NFR_CHECKS: dict[str, tuple[object, str, str]] = {\n+    \"code_quality\": (\n+        check_code_quality,\n+        \"ruff (extended)\",\n+        \"Extended code quality: complexity, security, simplification, return patterns\",\n+    ),\n+    \"complexity\": (\n+        check_complexity,\n+        \"radon\",\n+        \"Cyclomatic complexity analysis (grade C+ flagged)\",\n+    ),\n+    \"dead_code\": (\n+        check_dead_code,\n+        \"vulture\",\n+        \"Unused code detection (80%+ confidence)\",\n+    ),\n+    \"security\": (\n+        check_security,\n+        \"bandit\",\n+        \"Security vulnerability patterns\",\n+    ),\n+    # \u2500\u2500 Planned checks (uncomment when ready) \u2500\u2500\n+    # \"duplication\": (check_duplication, \"jscpd\", \"Code duplication detection\"),\n+    # \"coverage\": (check_coverage, \"pytest-cov\", \"Test coverage (60% minimum)\"),\n+    # \"import_hygiene\": (check_imports, \"custom\", \"Orphan files, circular imports\"),\n+    # \"maintainability\": (check_maintainability, \"radon mi\", \"Maintainability index\"),\n+    # \"reliability\": (check_reliability, \"custom\", \"Scenario consistency across runs\"),\n+}\n+\n+\n+def run_checks(\n+    repo_root: Path,\n+    selected: str | None = None,\n+) -> list[NFRResult]:\n+    \"\"\"Run NFR checks and return results.\"\"\"\n+    results: list[NFRResult] = []\n+\n+    for name, (check_fn, tool, _description) in NFR_CHECKS.items():\n+        if selected and name != selected:\n+            continue\n+\n+        try:\n+            findings = check_fn(repo_root)  # type: ignore[operator]\n+            # Determine status\n+            has_critical = any(f.severity == \"CRITICAL\" for f in findings)\n+            has_warning = any(f.severity == \"WARNING\" for f in findings)\n+            info_only = all(f.severity == \"INFO\" for f in findings)\n+\n+            if info_only and findings:\n+                status = \"skipped\"\n+                summary = findings[0].message\n+            elif has_critical:\n+                status = \"failed\"\n+                n_crit = sum(1 for f in findings if f.severity == \"CRITICAL\")\n+                summary = f\"{len(findings)} findings ({n_crit} critical)\"\n+            elif has_warning:\n+                status = \"passed\"  # Non-blocking\n+                summary = f\"{len(findings)} warnings\"\n+            elif not findings:\n+                status = \"passed\"\n+                summary = \"Clean\"\n+            else:\n+                status = \"passed\"\n+                summary = f\"{len(findings)} findings\"\n+\n+            results.append(\n+                NFRResult(\n+                    name=name,\n+                    status=status,\n+                    findings=findings,\n+                    tool=tool,\n+                    summary=summary,\n+                )\n+            )\n+        except Exception as e:\n+            results.append(\n+                NFRResult(\n+                    name=name,\n+                    status=\"error\",\n+                    findings=[\n+                        NFRFinding(\n+                            nfr=name,\n+                            severity=\"WARNING\",\n+                            message=f\"Check failed: {e}\",\n+                        )\n+                    ],\n+                    tool=tool,\n+                    summary=f\"Error: {e}\",\n+                )\n+            )\n+\n+    return results\n+\n+\n+def main() -> int:\n+    parser = argparse.ArgumentParser(\n+        description=\"Run Non-Functional Requirement checks (Gate 2)\"\n+    )\n+    parser.add_argument(\n+        \"--check\",\n+        type=str,\n+        default=None,\n+        help=f\"Run a specific check ({', '.join(NFR_CHECKS.keys())})\",\n+    )\n+    parser.add_argument(\n+        \"--json\",\n+        action=\"store_true\",\n+        help=\"Output results as JSON\",\n+    )\n+    parser.add_argument(\n+        \"--output\",\n+        type=str,\n+        default=None,\n+        help=\"Write JSON results to file\",\n+    )\n+    args = parser.parse_args()\n+\n+    repo_root = Path(__file__).resolve().parent.parent\n+    results = run_checks(repo_root, selected=args.check)\n+\n+    if args.json or args.output:\n+        output = json.dumps(\n+            [asdict(r) for r in results], indent=2\n+        )\n+        if args.output:\n+            Path(args.output).parent.mkdir(parents=True, exist_ok=True)\n+            Path(args.output).write_text(output + \"\\n\")\n+            print(f\"Results written to {args.output}\")\n+        else:\n+            print(output)\n+    else:\n+        # Human-readable output\n+        print(\"=\" * 60)\n+        print(\"GATE 2: Non-Functional Requirements\")\n+        print(\"=\" * 60)\n+\n+        total_findings = 0\n+        for r in results:\n+            icon = {\n+                \"passed\": \"PASS\",\n+                \"failed\": \"FAIL\",\n+                \"skipped\": \"SKIP\",\n+                \"error\": \"ERR \",\n+            }.get(r.status, \"????\")\n+            print(f\"\\n[{icon}] {r.name} ({r.tool}): {r.summary}\")\n+\n+            for f in r.findings:\n+                if f.severity == \"INFO\":\n+                    continue\n+                loc = \"\"\n+                if f.file:\n+                    loc = f\" {f.file}\"\n+                    if f.line:\n+                        loc += f\":{f.line}\"\n+                print(f\"  [{f.severity}]{loc} {f.message}\")\n+                total_findings += 1\n+\n+        print(f\"\\n{'=' * 60}\")\n+        print(f\"Total: {total_findings} findings across {len(results)} checks\")\n+        print(\"NOTE: Gate 2 is non-blocking. Findings feed into feedback.\")\n+\n+    return 0\n+\n+\n+if __name__ == \"__main__\":\n+    sys.exit(main())\n",
      "raw": "#!/usr/bin/env python3\n\"\"\"Non-Functional Requirements (NFR) checker \u2014 Gate 2.\n\nExtensible framework for running non-blocking quality checks beyond\nlint/typecheck/test. Each check produces findings that feed into\nfeedback and the LLM-as-judge's holistic evaluation.\n\nAdding a new NFR check:\n    1. Create a function: def check_<name>(repo_root: Path) -> list[NFRFinding]\n    2. Register it in NFR_CHECKS dict at the bottom of this file\n    3. The factory loop will pick it up automatically\n\nUsage:\n    python scripts/nfr_checks.py                    # run all checks\n    python scripts/nfr_checks.py --check complexity  # run one check\n    python scripts/nfr_checks.py --json              # JSON output for automation\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport subprocess\nimport sys\nfrom dataclasses import asdict, dataclass\nfrom pathlib import Path\n\n\n@dataclass\nclass NFRFinding:\n    \"\"\"A single NFR finding.\"\"\"\n\n    nfr: str  # Which NFR this belongs to\n    severity: str  # CRITICAL, WARNING, NIT, INFO\n    message: str  # Human-readable description\n    file: str = \"\"  # Optional: specific file\n    line: int = 0  # Optional: specific line\n    metric: str = \"\"  # Optional: metric name\n    value: str = \"\"  # Optional: metric value\n    threshold: str = \"\"  # Optional: threshold that was exceeded\n\n\n@dataclass\nclass NFRResult:\n    \"\"\"Result of running one NFR check.\"\"\"\n\n    name: str\n    status: str  # passed, failed, skipped, error\n    findings: list[NFRFinding]\n    tool: str  # What tool was used\n    summary: str  # One-line summary\n\n\ndef _run_tool(\n    cmd: list[str], repo_root: Path\n) -> subprocess.CompletedProcess[str]:\n    \"\"\"Run a tool, returning the result without raising on failure.\"\"\"\n    try:\n        return subprocess.run(\n            cmd,\n            cwd=str(repo_root),\n            capture_output=True,\n            text=True,\n            timeout=60,\n        )\n    except FileNotFoundError:\n        result = subprocess.CompletedProcess(\n            cmd, returncode=-1, stdout=\"\", stderr=f\"Tool not found: {cmd[0]}\"\n        )\n        return result\n    except subprocess.TimeoutExpired:\n        result = subprocess.CompletedProcess(\n            cmd, returncode=-2, stdout=\"\", stderr=f\"Timeout: {' '.join(cmd)}\"\n        )\n        return result\n\n\n# \u2500\u2500 NFR Check Implementations \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ndef check_code_quality(repo_root: Path) -> list[NFRFinding]:\n    \"\"\"Extended code quality beyond basic lint.\n\n    Uses ruff with broader rule selection including:\n    - C90: McCabe complexity\n    - S: Security (bandit-equivalent)\n    - SIM: Simplification suggestions\n    - RET: Return statement issues\n    \"\"\"\n    findings: list[NFRFinding] = []\n\n    result = _run_tool(\n        [\n            \"ruff\",\n            \"check\",\n            \"src/\",\n            \"--select\",\n            \"C90,S,SIM,RET,PTH,ERA\",\n            \"--no-fix\",\n            \"--output-format\",\n            \"json\",\n        ],\n        repo_root,\n    )\n\n    if result.returncode == -1:\n        findings.append(\n            NFRFinding(\n                nfr=\"code_quality\",\n                severity=\"INFO\",\n                message=\"ruff not available \u2014 install with: pip install ruff\",\n            )\n        )\n        return findings\n\n    if result.stdout.strip():\n        try:\n            issues = json.loads(result.stdout)\n            for issue in issues:\n                findings.append(\n                    NFRFinding(\n                        nfr=\"code_quality\",\n                        severity=\"WARNING\",\n                        message=f\"{issue.get('code', '?')}: {issue.get('message', 'unknown')}\",\n                        file=issue.get(\"filename\", \"\"),\n                        line=issue.get(\"location\", {}).get(\"row\", 0),\n                    )\n                )\n        except json.JSONDecodeError:\n            # Fall back to text output\n            for line in result.stdout.strip().splitlines():\n                findings.append(\n                    NFRFinding(\n                        nfr=\"code_quality\",\n                        severity=\"WARNING\",\n                        message=line.strip(),\n                    )\n                )\n\n    return findings\n\n\ndef check_complexity(repo_root: Path) -> list[NFRFinding]:\n    \"\"\"Cyclomatic complexity via radon.\n\n    Flags functions with complexity grade C or worse.\n    \"\"\"\n    findings: list[NFRFinding] = []\n\n    result = _run_tool(\n        [\"radon\", \"cc\", \"src/\", \"--min\", \"C\", \"--json\"],\n        repo_root,\n    )\n\n    if result.returncode == -1:\n        findings.append(\n            NFRFinding(\n                nfr=\"complexity\",\n                severity=\"INFO\",\n                message=\"radon not available \u2014 install with: pip install radon\",\n            )\n        )\n        return findings\n\n    if result.stdout.strip():\n        try:\n            data = json.loads(result.stdout)\n            for filepath, blocks in data.items():\n                for block in blocks:\n                    findings.append(\n                        NFRFinding(\n                            nfr=\"complexity\",\n                            severity=\"WARNING\",\n                            message=(\n                                f\"{block.get('type', '?')} '{block.get('name', '?')}' \"\n                                f\"has complexity {block.get('complexity', '?')} \"\n                                f\"(grade {block.get('rank', '?')})\"\n                            ),\n                            file=filepath,\n                            line=block.get(\"lineno\", 0),\n                            metric=\"cyclomatic_complexity\",\n                            value=str(block.get(\"complexity\", \"\")),\n                            threshold=\"C\",\n                        )\n                    )\n        except json.JSONDecodeError:\n            pass\n\n    return findings\n\n\ndef check_dead_code(repo_root: Path) -> list[NFRFinding]:\n    \"\"\"Dead code detection via vulture.\"\"\"\n    findings: list[NFRFinding] = []\n\n    result = _run_tool(\n        [\"vulture\", \"src/\", \"--min-confidence\", \"80\"],\n        repo_root,\n    )\n\n    if result.returncode == -1:\n        findings.append(\n            NFRFinding(\n                nfr=\"dead_code\",\n                severity=\"INFO\",\n                message=\"vulture not available \u2014 install with: pip install vulture\",\n            )\n        )\n        return findings\n\n    if result.stdout.strip():\n        for line in result.stdout.strip().splitlines():\n            # vulture output: \"src/file.py:10: unused function 'foo' (90% confidence)\"\n            findings.append(\n                NFRFinding(\n                    nfr=\"dead_code\",\n                    severity=\"WARNING\",\n                    message=line.strip(),\n                )\n            )\n\n    return findings\n\n\ndef check_security(repo_root: Path) -> list[NFRFinding]:\n    \"\"\"Security vulnerability detection via bandit.\"\"\"\n    findings: list[NFRFinding] = []\n\n    result = _run_tool(\n        [\"bandit\", \"-r\", \"src/\", \"-f\", \"json\", \"-q\"],\n        repo_root,\n    )\n\n    if result.returncode == -1:\n        findings.append(\n            NFRFinding(\n                nfr=\"security\",\n                severity=\"INFO\",\n                message=\"bandit not available \u2014 install with: pip install bandit\",\n            )\n        )\n        return findings\n\n    if result.stdout.strip():\n        try:\n            data = json.loads(result.stdout)\n            for issue in data.get(\"results\", []):\n                severity_map = {\n                    \"HIGH\": \"CRITICAL\",\n                    \"MEDIUM\": \"WARNING\",\n                    \"LOW\": \"NIT\",\n                }\n                findings.append(\n                    NFRFinding(\n                        nfr=\"security\",\n                        severity=severity_map.get(\n                            issue.get(\"issue_severity\", \"\"), \"WARNING\"\n                        ),\n                        message=(\n                            f\"{issue.get('test_id', '?')}: \"\n                            f\"{issue.get('issue_text', 'unknown')}\"\n                        ),\n                        file=issue.get(\"filename\", \"\"),\n                        line=issue.get(\"line_number\", 0),\n                    )\n                )\n        except json.JSONDecodeError:\n            pass\n\n    return findings\n\n\n# \u2500\u2500 NFR Registry \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Add new checks here. The key is the check name, the value is\n# (function, tool_name, description).\n\nNFR_CHECKS: dict[str, tuple[object, str, str]] = {\n    \"code_quality\": (\n        check_code_quality,\n        \"ruff (extended)\",\n        \"Extended code quality: complexity, security, simplification, return patterns\",\n    ),\n    \"complexity\": (\n        check_complexity,\n        \"radon\",\n        \"Cyclomatic complexity analysis (grade C+ flagged)\",\n    ),\n    \"dead_code\": (\n        check_dead_code,\n        \"vulture\",\n        \"Unused code detection (80%+ confidence)\",\n    ),\n    \"security\": (\n        check_security,\n        \"bandit\",\n        \"Security vulnerability patterns\",\n    ),\n    # \u2500\u2500 Planned checks (uncomment when ready) \u2500\u2500\n    # \"duplication\": (check_duplication, \"jscpd\", \"Code duplication detection\"),\n    # \"coverage\": (check_coverage, \"pytest-cov\", \"Test coverage (60% minimum)\"),\n    # \"import_hygiene\": (check_imports, \"custom\", \"Orphan files, circular imports\"),\n    # \"maintainability\": (check_maintainability, \"radon mi\", \"Maintainability index\"),\n    # \"reliability\": (check_reliability, \"custom\", \"Scenario consistency across runs\"),\n}\n\n\ndef run_checks(\n    repo_root: Path,\n    selected: str | None = None,\n) -> list[NFRResult]:\n    \"\"\"Run NFR checks and return results.\"\"\"\n    results: list[NFRResult] = []\n\n    for name, (check_fn, tool, _description) in NFR_CHECKS.items():\n        if selected and name != selected:\n            continue\n\n        try:\n            findings = check_fn(repo_root)  # type: ignore[operator]\n            # Determine status\n            has_critical = any(f.severity == \"CRITICAL\" for f in findings)\n            has_warning = any(f.severity == \"WARNING\" for f in findings)\n            info_only = all(f.severity == \"INFO\" for f in findings)\n\n            if info_only and findings:\n                status = \"skipped\"\n                summary = findings[0].message\n            elif has_critical:\n                status = \"failed\"\n                n_crit = sum(1 for f in findings if f.severity == \"CRITICAL\")\n                summary = f\"{len(findings)} findings ({n_crit} critical)\"\n            elif has_warning:\n                status = \"passed\"  # Non-blocking\n                summary = f\"{len(findings)} warnings\"\n            elif not findings:\n                status = \"passed\"\n                summary = \"Clean\"\n            else:\n                status = \"passed\"\n                summary = f\"{len(findings)} findings\"\n\n            results.append(\n                NFRResult(\n                    name=name,\n                    status=status,\n                    findings=findings,\n                    tool=tool,\n                    summary=summary,\n                )\n            )\n        except Exception as e:\n            results.append(\n                NFRResult(\n                    name=name,\n                    status=\"error\",\n                    findings=[\n                        NFRFinding(\n                            nfr=name,\n                            severity=\"WARNING\",\n                            message=f\"Check failed: {e}\",\n                        )\n                    ],\n                    tool=tool,\n                    summary=f\"Error: {e}\",\n                )\n            )\n\n    return results\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(\n        description=\"Run Non-Functional Requirement checks (Gate 2)\"\n    )\n    parser.add_argument(\n        \"--check\",\n        type=str,\n        default=None,\n        help=f\"Run a specific check ({', '.join(NFR_CHECKS.keys())})\",\n    )\n    parser.add_argument(\n        \"--json\",\n        action=\"store_true\",\n        help=\"Output results as JSON\",\n    )\n    parser.add_argument(\n        \"--output\",\n        type=str,\n        default=None,\n        help=\"Write JSON results to file\",\n    )\n    args = parser.parse_args()\n\n    repo_root = Path(__file__).resolve().parent.parent\n    results = run_checks(repo_root, selected=args.check)\n\n    if args.json or args.output:\n        output = json.dumps(\n            [asdict(r) for r in results], indent=2\n        )\n        if args.output:\n            Path(args.output).parent.mkdir(parents=True, exist_ok=True)\n            Path(args.output).write_text(output + \"\\n\")\n            print(f\"Results written to {args.output}\")\n        else:\n            print(output)\n    else:\n        # Human-readable output\n        print(\"=\" * 60)\n        print(\"GATE 2: Non-Functional Requirements\")\n        print(\"=\" * 60)\n\n        total_findings = 0\n        for r in results:\n            icon = {\n                \"passed\": \"PASS\",\n                \"failed\": \"FAIL\",\n                \"skipped\": \"SKIP\",\n                \"error\": \"ERR \",\n            }.get(r.status, \"????\")\n            print(f\"\\n[{icon}] {r.name} ({r.tool}): {r.summary}\")\n\n            for f in r.findings:\n                if f.severity == \"INFO\":\n                    continue\n                loc = \"\"\n                if f.file:\n                    loc = f\" {f.file}\"\n                    if f.line:\n                        loc += f\":{f.line}\"\n                print(f\"  [{f.severity}]{loc} {f.message}\")\n                total_findings += 1\n\n        print(f\"\\n{'=' * 60}\")\n        print(f\"Total: {total_findings} findings across {len(results)} checks\")\n        print(\"NOTE: Gate 2 is non-blocking. Findings feed into feedback.\")\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
      "base": ""
    },
    "scripts/package_artifacts.py": {
      "additions": 21,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/scripts/package_artifacts.py b/scripts/package_artifacts.py\nnew file mode 100644\nindex 0000000..65a1ac1\n--- /dev/null\n+++ b/scripts/package_artifacts.py\n@@ -0,0 +1,21 @@\n+from __future__ import annotations\n+\n+import argparse\n+import shutil\n+from pathlib import Path\n+\n+\n+def main() -> None:\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--run-id\", required=True)\n+    args = parser.parse_args()\n+    src = Path(\"artifacts\") / args.run_id\n+    dst = Path(\"demo_pack/training_data\") / args.run_id\n+    if dst.exists():\n+        shutil.rmtree(dst)\n+    shutil.copytree(src, dst)\n+    print(dst)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
      "raw": "from __future__ import annotations\n\nimport argparse\nimport shutil\nfrom pathlib import Path\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--run-id\", required=True)\n    args = parser.parse_args()\n    src = Path(\"artifacts\") / args.run_id\n    dst = Path(\"demo_pack/training_data\") / args.run_id\n    if dst.exists():\n        shutil.rmtree(dst)\n    shutil.copytree(src, dst)\n    print(dst)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "base": ""
    },
    "scripts/restore_holdout.py": {
      "additions": 227,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/scripts/restore_holdout.py b/scripts/restore_holdout.py\nnew file mode 100644\nindex 0000000..f49e2fb\n--- /dev/null\n+++ b/scripts/restore_holdout.py\n@@ -0,0 +1,227 @@\n+#!/usr/bin/env python3\n+\"\"\"Restore holdout scenarios from a known git ref.\n+\n+Symmetric counterpart to strip_holdout.py. Restores /scenarios/\n+and Makefile targets from a clean source (default: origin/main).\n+\n+This script does NOT commit \u2014 the caller decides when to commit.\n+\n+Usage:\n+    python scripts/restore_holdout.py                    # restore from origin/main\n+    python scripts/restore_holdout.py --ref origin/main  # explicit ref\n+    python scripts/restore_holdout.py --dry-run          # show what would be restored\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import re\n+import subprocess\n+import sys\n+from pathlib import Path\n+\n+STRIP_MARKER = \"[factory:holdout-stripped]\"\n+\n+\n+def restore_scenarios(\n+    repo_root: Path, ref: str, dry_run: bool = False\n+) -> list[str]:\n+    \"\"\"Restore /scenarios/ from a git ref.\n+\n+    Uses `git checkout <ref> -- scenarios/` to restore files\n+    from the specified ref without changing the current branch.\n+\n+    Returns list of restored paths.\n+    \"\"\"\n+    # Check if scenarios exist at the ref\n+    result = subprocess.run(\n+        [\"git\", \"ls-tree\", \"-r\", \"--name-only\", ref, \"scenarios/\"],\n+        cwd=str(repo_root),\n+        capture_output=True,\n+        text=True,\n+    )\n+\n+    if result.returncode != 0:\n+        print(f\"ERROR: Could not list scenarios at ref '{ref}'\")\n+        print(f\"  stderr: {result.stderr.strip()}\")\n+        return []\n+\n+    files = [f.strip() for f in result.stdout.strip().splitlines() if f.strip()]\n+    if not files:\n+        print(f\"WARNING: No scenario files found at ref '{ref}'\")\n+        return []\n+\n+    if dry_run:\n+        print(f\"Would restore {len(files)} files from '{ref}':\")\n+        for f in files:\n+            print(f\"  + {f}\")\n+        return files\n+\n+    # Restore from ref\n+    subprocess.run(\n+        [\"git\", \"checkout\", ref, \"--\", \"scenarios/\"],\n+        cwd=str(repo_root),\n+        check=True,\n+        capture_output=True,\n+    )\n+\n+    # Unstage the restored files (leave them as working tree changes)\n+    subprocess.run(\n+        [\"git\", \"reset\", \"HEAD\", \"scenarios/\"],\n+        cwd=str(repo_root),\n+        capture_output=True,\n+    )\n+\n+    return files\n+\n+\n+def restore_makefile_targets(\n+    repo_root: Path, ref: str, dry_run: bool = False\n+) -> list[str]:\n+    \"\"\"Restore Makefile targets that were commented out by strip_holdout.py.\n+\n+    Instead of trying to uncomment, we restore the entire Makefile from\n+    the ref and keep all non-scenario changes from the current version.\n+\n+    Simpler approach: just uncomment the marked blocks.\n+\n+    Returns list of restored target names.\n+    \"\"\"\n+    makefile = repo_root / \"Makefile\"\n+    if not makefile.exists():\n+        return []\n+\n+    content = makefile.read_text()\n+    restored_targets: list[str] = []\n+\n+    # Find and uncomment all marked blocks\n+    escaped_marker = re.escape(STRIP_MARKER)\n+    pattern = (\n+        rf\"# {escaped_marker}\\s*\u2014 stripped by strip_holdout\\.py\\n\"\n+        rf\"((?:# .*\\n)*)\"\n+        rf\"# end {escaped_marker}\"\n+    )\n+\n+    for match in re.finditer(pattern, content):\n+        commented_block = match.group(1)\n+        # Uncomment: remove leading \"# \" from each line\n+        uncommented_lines = []\n+        for line in commented_block.splitlines():\n+            if line.startswith(\"# \"):\n+                uncommented_lines.append(line[2:])\n+            elif line == \"#\":\n+                uncommented_lines.append(\"\")\n+            else:\n+                uncommented_lines.append(line)\n+        uncommented = \"\\n\".join(uncommented_lines)\n+\n+        # Extract target name for logging\n+        target_match = re.match(r\"(\\w[\\w-]*):\", uncommented)\n+        if target_match:\n+            restored_targets.append(target_match.group(1))\n+\n+        if not dry_run:\n+            content = content.replace(match.group(0), uncommented)\n+\n+    if restored_targets and not dry_run:\n+        makefile.write_text(content)\n+\n+    return restored_targets\n+\n+\n+def verify_restored(repo_root: Path, expected_count: int) -> list[str]:\n+    \"\"\"Verify that restoration was complete.\n+\n+    Returns list of verification failures (empty = success).\n+    \"\"\"\n+    failures: list[str] = []\n+\n+    scenarios_dir = repo_root / \"scenarios\"\n+    if not scenarios_dir.exists():\n+        failures.append(\"scenarios/ directory does not exist\")\n+        return failures\n+\n+    actual = list(scenarios_dir.rglob(\"*.md\"))\n+    if len(actual) == 0:\n+        failures.append(\"scenarios/ exists but has no .md files\")\n+    elif expected_count > 0 and len(actual) != expected_count:\n+        failures.append(\n+            f\"Expected {expected_count} scenario files, \"\n+            f\"found {len(actual)}\"\n+        )\n+\n+    return failures\n+\n+\n+def main() -> int:\n+    parser = argparse.ArgumentParser(\n+        description=\"Restore holdout scenarios from a git ref\"\n+    )\n+    parser.add_argument(\n+        \"--ref\",\n+        type=str,\n+        default=\"origin/main\",\n+        help=\"Git ref to restore scenarios from (default: origin/main)\",\n+    )\n+    parser.add_argument(\n+        \"--dry-run\",\n+        action=\"store_true\",\n+        help=\"Show what would be restored without making changes\",\n+    )\n+    args = parser.parse_args()\n+\n+    repo_root = Path(__file__).resolve().parent.parent\n+\n+    # Fetch latest from remote (so origin/main is current)\n+    print(\"Fetching latest from remote...\")\n+    subprocess.run(\n+        [\"git\", \"fetch\", \"origin\"],\n+        cwd=str(repo_root),\n+        capture_output=True,\n+    )\n+\n+    print(\n+        f\"{'DRY RUN \u2014 ' if args.dry_run else ''}\"\n+        f\"Restoring holdout scenarios from '{args.ref}'...\"\n+    )\n+\n+    # Step 1: Restore scenario files\n+    restored = restore_scenarios(\n+        repo_root, args.ref, dry_run=args.dry_run\n+    )\n+    if restored:\n+        print(f\"  Restored {len(restored)} scenario files\")\n+    else:\n+        print(\"  No scenario files restored\")\n+        return 1\n+\n+    # Step 2: Restore Makefile targets\n+    targets = restore_makefile_targets(\n+        repo_root, args.ref, dry_run=args.dry_run\n+    )\n+    if targets:\n+        print(f\"  Restored Makefile targets: {', '.join(targets)}\")\n+    else:\n+        print(\"  No Makefile targets needed restoration\")\n+\n+    if args.dry_run:\n+        print(\"\\nDry run complete \u2014 no changes made\")\n+        return 0\n+\n+    # Step 3: Verify\n+    failures = verify_restored(repo_root, expected_count=len(restored))\n+    if failures:\n+        print(\"\\nVERIFICATION FAILED:\")\n+        for f in failures:\n+            print(f\"  - {f}\")\n+        return 1\n+\n+    print(\"\\nVerification passed \u2014 scenarios fully restored\")\n+    print(\"NOTE: Changes are in working tree, NOT committed.\")\n+    print(\"  Stage and commit when ready: git add scenarios/ Makefile\")\n+\n+    return 0\n+\n+\n+if __name__ == \"__main__\":\n+    sys.exit(main())\n",
      "raw": "#!/usr/bin/env python3\n\"\"\"Restore holdout scenarios from a known git ref.\n\nSymmetric counterpart to strip_holdout.py. Restores /scenarios/\nand Makefile targets from a clean source (default: origin/main).\n\nThis script does NOT commit \u2014 the caller decides when to commit.\n\nUsage:\n    python scripts/restore_holdout.py                    # restore from origin/main\n    python scripts/restore_holdout.py --ref origin/main  # explicit ref\n    python scripts/restore_holdout.py --dry-run          # show what would be restored\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nSTRIP_MARKER = \"[factory:holdout-stripped]\"\n\n\ndef restore_scenarios(\n    repo_root: Path, ref: str, dry_run: bool = False\n) -> list[str]:\n    \"\"\"Restore /scenarios/ from a git ref.\n\n    Uses `git checkout <ref> -- scenarios/` to restore files\n    from the specified ref without changing the current branch.\n\n    Returns list of restored paths.\n    \"\"\"\n    # Check if scenarios exist at the ref\n    result = subprocess.run(\n        [\"git\", \"ls-tree\", \"-r\", \"--name-only\", ref, \"scenarios/\"],\n        cwd=str(repo_root),\n        capture_output=True,\n        text=True,\n    )\n\n    if result.returncode != 0:\n        print(f\"ERROR: Could not list scenarios at ref '{ref}'\")\n        print(f\"  stderr: {result.stderr.strip()}\")\n        return []\n\n    files = [f.strip() for f in result.stdout.strip().splitlines() if f.strip()]\n    if not files:\n        print(f\"WARNING: No scenario files found at ref '{ref}'\")\n        return []\n\n    if dry_run:\n        print(f\"Would restore {len(files)} files from '{ref}':\")\n        for f in files:\n            print(f\"  + {f}\")\n        return files\n\n    # Restore from ref\n    subprocess.run(\n        [\"git\", \"checkout\", ref, \"--\", \"scenarios/\"],\n        cwd=str(repo_root),\n        check=True,\n        capture_output=True,\n    )\n\n    # Unstage the restored files (leave them as working tree changes)\n    subprocess.run(\n        [\"git\", \"reset\", \"HEAD\", \"scenarios/\"],\n        cwd=str(repo_root),\n        capture_output=True,\n    )\n\n    return files\n\n\ndef restore_makefile_targets(\n    repo_root: Path, ref: str, dry_run: bool = False\n) -> list[str]:\n    \"\"\"Restore Makefile targets that were commented out by strip_holdout.py.\n\n    Instead of trying to uncomment, we restore the entire Makefile from\n    the ref and keep all non-scenario changes from the current version.\n\n    Simpler approach: just uncomment the marked blocks.\n\n    Returns list of restored target names.\n    \"\"\"\n    makefile = repo_root / \"Makefile\"\n    if not makefile.exists():\n        return []\n\n    content = makefile.read_text()\n    restored_targets: list[str] = []\n\n    # Find and uncomment all marked blocks\n    escaped_marker = re.escape(STRIP_MARKER)\n    pattern = (\n        rf\"# {escaped_marker}\\s*\u2014 stripped by strip_holdout\\.py\\n\"\n        rf\"((?:# .*\\n)*)\"\n        rf\"# end {escaped_marker}\"\n    )\n\n    for match in re.finditer(pattern, content):\n        commented_block = match.group(1)\n        # Uncomment: remove leading \"# \" from each line\n        uncommented_lines = []\n        for line in commented_block.splitlines():\n            if line.startswith(\"# \"):\n                uncommented_lines.append(line[2:])\n            elif line == \"#\":\n                uncommented_lines.append(\"\")\n            else:\n                uncommented_lines.append(line)\n        uncommented = \"\\n\".join(uncommented_lines)\n\n        # Extract target name for logging\n        target_match = re.match(r\"(\\w[\\w-]*):\", uncommented)\n        if target_match:\n            restored_targets.append(target_match.group(1))\n\n        if not dry_run:\n            content = content.replace(match.group(0), uncommented)\n\n    if restored_targets and not dry_run:\n        makefile.write_text(content)\n\n    return restored_targets\n\n\ndef verify_restored(repo_root: Path, expected_count: int) -> list[str]:\n    \"\"\"Verify that restoration was complete.\n\n    Returns list of verification failures (empty = success).\n    \"\"\"\n    failures: list[str] = []\n\n    scenarios_dir = repo_root / \"scenarios\"\n    if not scenarios_dir.exists():\n        failures.append(\"scenarios/ directory does not exist\")\n        return failures\n\n    actual = list(scenarios_dir.rglob(\"*.md\"))\n    if len(actual) == 0:\n        failures.append(\"scenarios/ exists but has no .md files\")\n    elif expected_count > 0 and len(actual) != expected_count:\n        failures.append(\n            f\"Expected {expected_count} scenario files, \"\n            f\"found {len(actual)}\"\n        )\n\n    return failures\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(\n        description=\"Restore holdout scenarios from a git ref\"\n    )\n    parser.add_argument(\n        \"--ref\",\n        type=str,\n        default=\"origin/main\",\n        help=\"Git ref to restore scenarios from (default: origin/main)\",\n    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=\"Show what would be restored without making changes\",\n    )\n    args = parser.parse_args()\n\n    repo_root = Path(__file__).resolve().parent.parent\n\n    # Fetch latest from remote (so origin/main is current)\n    print(\"Fetching latest from remote...\")\n    subprocess.run(\n        [\"git\", \"fetch\", \"origin\"],\n        cwd=str(repo_root),\n        capture_output=True,\n    )\n\n    print(\n        f\"{'DRY RUN \u2014 ' if args.dry_run else ''}\"\n        f\"Restoring holdout scenarios from '{args.ref}'...\"\n    )\n\n    # Step 1: Restore scenario files\n    restored = restore_scenarios(\n        repo_root, args.ref, dry_run=args.dry_run\n    )\n    if restored:\n        print(f\"  Restored {len(restored)} scenario files\")\n    else:\n        print(\"  No scenario files restored\")\n        return 1\n\n    # Step 2: Restore Makefile targets\n    targets = restore_makefile_targets(\n        repo_root, args.ref, dry_run=args.dry_run\n    )\n    if targets:\n        print(f\"  Restored Makefile targets: {', '.join(targets)}\")\n    else:\n        print(\"  No Makefile targets needed restoration\")\n\n    if args.dry_run:\n        print(\"\\nDry run complete \u2014 no changes made\")\n        return 0\n\n    # Step 3: Verify\n    failures = verify_restored(repo_root, expected_count=len(restored))\n    if failures:\n        print(\"\\nVERIFICATION FAILED:\")\n        for f in failures:\n            print(f\"  - {f}\")\n        return 1\n\n    print(\"\\nVerification passed \u2014 scenarios fully restored\")\n    print(\"NOTE: Changes are in working tree, NOT committed.\")\n    print(\"  Stage and commit when ready: git add scenarios/ Makefile\")\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
      "base": ""
    },
    "scripts/run_scenarios.py": {
      "additions": 14,
      "deletions": 0,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/scripts/run_scenarios.py b/scripts/run_scenarios.py\nindex 5d540af..4161759 100644\n--- a/scripts/run_scenarios.py\n+++ b/scripts/run_scenarios.py\n@@ -120,6 +120,20 @@ def run_scenario(scenario: Scenario, timeout: int, repo_root: Path) -> ScenarioR\n     \"\"\"Execute a single scenario's evaluation method.\"\"\"\n     start = time.time()\n \n+    # Guard: empty evaluation commands must not pass silently\n+    if not scenario.evaluation_method.strip():\n+        return ScenarioResult(\n+            name=scenario.name,\n+            file_path=scenario.file_path,\n+            category=scenario.category,\n+            passed=False,\n+            exit_code=-3,\n+            stdout=\"\",\n+            stderr=\"EMPTY: evaluation_method is empty \u2014 cannot evaluate\",\n+            duration_seconds=0.0,\n+            error_summary=\"Empty evaluation command\",\n+        )\n+\n     try:\n         result = subprocess.run(\n             [\"bash\", \"-c\", scenario.evaluation_method],\n",
      "raw": "#!/usr/bin/env python3\n\"\"\"Scenario runner for the dark factory holdout evaluation.\n\nReads structured markdown scenarios from /scenarios/, executes their\nevaluation methods, and produces a JSON report at\nartifacts/factory/scenario_results.json.\n\nUsage:\n    python scripts/run_scenarios.py\n    python scripts/run_scenarios.py --category environment\n    python scripts/run_scenarios.py --timeout 120\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nimport time\nfrom dataclasses import asdict, dataclass, field\nfrom pathlib import Path\n\n\n@dataclass\nclass Scenario:\n    \"\"\"A parsed scenario from a markdown file.\"\"\"\n\n    name: str\n    file_path: str\n    category: str\n    preconditions: list[str]\n    behavioral_expectation: str\n    evaluation_method: str\n    pass_criteria: str\n    evidence_required: list[str]\n\n\n@dataclass\nclass ScenarioResult:\n    \"\"\"Result of running a single scenario.\"\"\"\n\n    name: str\n    file_path: str\n    category: str\n    passed: bool\n    exit_code: int\n    stdout: str\n    stderr: str\n    duration_seconds: float\n    error_summary: str = \"\"\n\n\n@dataclass\nclass ScenarioReport:\n    \"\"\"Full report from running all scenarios.\"\"\"\n\n    timestamp: str\n    total: int\n    passed: int\n    failed: int\n    skipped: int\n    satisfaction_score: float\n    results: list[dict[str, object]] = field(default_factory=list)\n\n\ndef parse_scenario(path: Path) -> Scenario:\n    \"\"\"Parse a structured markdown scenario file.\"\"\"\n    content = path.read_text()\n\n    def extract_section(heading: str) -> str:\n        pattern = rf\"## {heading}\\s*\\n(.*?)(?=\\n## |\\Z)\"\n        match = re.search(pattern, content, re.DOTALL)\n        return match.group(1).strip() if match else \"\"\n\n    # Extract name from H1\n    name_match = re.search(r\"# Scenario:\\s*(.+)\", content)\n    name = name_match.group(1).strip() if name_match else path.stem\n\n    category = extract_section(\"Category\").strip().lower()\n\n    preconditions_raw = extract_section(\"Preconditions\")\n    preconditions = [\n        line.lstrip(\"- \").strip()\n        for line in preconditions_raw.splitlines()\n        if line.strip().startswith(\"-\")\n    ]\n\n    behavioral_expectation = extract_section(\"Behavioral Expectation\")\n\n    eval_raw = extract_section(\"Evaluation Method\")\n    # Extract code block content\n    code_match = re.search(r\"```(?:bash|sh)?\\s*\\n(.*?)```\", eval_raw, re.DOTALL)\n    evaluation_method = code_match.group(1).strip() if code_match else eval_raw\n\n    pass_criteria = extract_section(\"Pass Criteria\")\n\n    evidence_raw = extract_section(\"Evidence Required\")\n    evidence_required = [\n        line.lstrip(\"- \").strip()\n        for line in evidence_raw.splitlines()\n        if line.strip().startswith(\"-\")\n    ]\n\n    return Scenario(\n        name=name,\n        file_path=str(path),\n        category=category,\n        preconditions=preconditions,\n        behavioral_expectation=behavioral_expectation,\n        evaluation_method=evaluation_method,\n        pass_criteria=pass_criteria,\n        evidence_required=evidence_required,\n    )\n\n\ndef run_scenario(scenario: Scenario, timeout: int, repo_root: Path) -> ScenarioResult:\n    \"\"\"Execute a single scenario's evaluation method.\"\"\"\n    start = time.time()\n\n    # Guard: empty evaluation commands must not pass silently\n    if not scenario.evaluation_method.strip():\n        return ScenarioResult(\n            name=scenario.name,\n            file_path=scenario.file_path,\n            category=scenario.category,\n            passed=False,\n            exit_code=-3,\n            stdout=\"\",\n            stderr=\"EMPTY: evaluation_method is empty \u2014 cannot evaluate\",\n            duration_seconds=0.0,\n            error_summary=\"Empty evaluation command\",\n        )\n\n    try:\n        result = subprocess.run(\n            [\"bash\", \"-c\", scenario.evaluation_method],\n            capture_output=True,\n            text=True,\n            timeout=timeout,\n            cwd=str(repo_root),\n            env={**os.environ, \"PYTHONPATH\": str(repo_root)},\n        )\n        duration = time.time() - start\n        passed = result.returncode == 0\n\n        error_summary = \"\"\n        if not passed:\n            # Extract the last meaningful error line\n            lines = (result.stderr + result.stdout).strip().splitlines()\n            error_lines = [\n                line\n                for line in lines\n                if \"error\" in line.lower()\n                or \"assert\" in line.lower()\n                or \"FAIL\" in line\n                or \"Traceback\" in line\n            ]\n            if error_lines:\n                error_summary = error_lines[-1]\n            elif lines:\n                error_summary = lines[-1]\n            else:\n                error_summary = \"Unknown error\"\n\n        return ScenarioResult(\n            name=scenario.name,\n            file_path=scenario.file_path,\n            category=scenario.category,\n            passed=passed,\n            exit_code=result.returncode,\n            stdout=result.stdout[-5000:],  # Cap output size\n            stderr=result.stderr[-5000:],\n            duration_seconds=round(duration, 2),\n            error_summary=error_summary,\n        )\n\n    except subprocess.TimeoutExpired:\n        duration = time.time() - start\n        return ScenarioResult(\n            name=scenario.name,\n            file_path=scenario.file_path,\n            category=scenario.category,\n            passed=False,\n            exit_code=-1,\n            stdout=\"\",\n            stderr=f\"TIMEOUT: scenario exceeded {timeout}s limit\",\n            duration_seconds=round(duration, 2),\n            error_summary=f\"Timeout after {timeout}s\",\n        )\n    except Exception as e:\n        duration = time.time() - start\n        return ScenarioResult(\n            name=scenario.name,\n            file_path=scenario.file_path,\n            category=scenario.category,\n            passed=False,\n            exit_code=-2,\n            stdout=\"\",\n            stderr=str(e),\n            duration_seconds=round(duration, 2),\n            error_summary=str(e),\n        )\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Run dark factory holdout scenarios\")\n    parser.add_argument(\n        \"--category\",\n        type=str,\n        default=None,\n        help=\"Filter by category (environment, training, etc.)\",\n    )\n    parser.add_argument(\n        \"--timeout\",\n        type=int,\n        default=300,\n        help=\"Timeout per scenario in seconds (default: 300)\",\n    )\n    parser.add_argument(\n        \"--scenarios-dir\",\n        type=str,\n        default=None,\n        help=\"Path to scenarios directory (default: <repo_root>/scenarios/)\",\n    )\n    parser.add_argument(\n        \"--output\",\n        type=str,\n        default=None,\n        help=\"Output JSON path (default: artifacts/factory/scenario_results.json)\",\n    )\n    args = parser.parse_args()\n\n    # Find repo root (where this script lives in scripts/)\n    repo_root = Path(__file__).resolve().parent.parent\n    if args.scenarios_dir:\n        scenarios_dir = Path(args.scenarios_dir)\n    else:\n        scenarios_dir = repo_root / \"scenarios\"\n    if args.output:\n        output_path = Path(args.output)\n    else:\n        output_path = (\n            repo_root / \"artifacts\" / \"factory\" / \"scenario_results.json\"\n        )\n\n    # Discover and parse scenarios\n    scenario_files = sorted(scenarios_dir.glob(\"*.md\"))\n    if not scenario_files:\n        print(f\"ERROR: No scenario files found in {scenarios_dir}\", file=sys.stderr)\n        return 1\n\n    scenarios = [parse_scenario(f) for f in scenario_files]\n\n    # Filter by category if requested\n    if args.category:\n        scenarios = [s for s in scenarios if s.category == args.category.lower()]\n        if not scenarios:\n            msg = f\"No scenarios match category '{args.category}'\"\n            print(f\"ERROR: {msg}\", file=sys.stderr)\n            return 1\n\n    print(f\"Running {len(scenarios)} scenario(s)...\")\n    print(\"=\" * 60)\n\n    results: list[ScenarioResult] = []\n    for i, scenario in enumerate(scenarios, 1):\n        print(f\"\\n[{i}/{len(scenarios)}] {scenario.name} ({scenario.category})\")\n        print(\"-\" * 40)\n\n        result = run_scenario(scenario, args.timeout, repo_root)\n        results.append(result)\n\n        status = \"PASS\" if result.passed else \"FAIL\"\n        print(f\"  {status} ({result.duration_seconds}s)\")\n        if not result.passed and result.error_summary:\n            print(f\"  Error: {result.error_summary}\")\n\n    # Compute satisfaction score\n    passed_count = sum(1 for r in results if r.passed)\n    failed_count = sum(1 for r in results if not r.passed)\n    total = len(results)\n    satisfaction = passed_count / total if total > 0 else 0.0\n\n    # Build report\n    report = ScenarioReport(\n        timestamp=time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n        total=total,\n        passed=passed_count,\n        failed=failed_count,\n        skipped=0,\n        satisfaction_score=round(satisfaction, 4),\n        results=[asdict(r) for r in results],\n    )\n\n    # Write output\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    output_path.write_text(json.dumps(asdict(report), indent=2) + \"\\n\")\n\n    # Summary\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"RESULTS: {passed_count}/{total} passed | Satisfaction: {satisfaction:.0%}\")\n    print(f\"Report: {output_path}\")\n\n    if failed_count > 0:\n        print(\"\\nFailed scenarios:\")\n        for r in results:\n            if not r.passed:\n                print(f\"  - {r.name}: {r.error_summary}\")\n        return 1\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n",
      "base": "#!/usr/bin/env python3\n\"\"\"Scenario runner for the dark factory holdout evaluation.\n\nReads structured markdown scenarios from /scenarios/, executes their\nevaluation methods, and produces a JSON report at\nartifacts/factory/scenario_results.json.\n\nUsage:\n    python scripts/run_scenarios.py\n    python scripts/run_scenarios.py --category environment\n    python scripts/run_scenarios.py --timeout 120\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nimport time\nfrom dataclasses import asdict, dataclass, field\nfrom pathlib import Path\n\n\n@dataclass\nclass Scenario:\n    \"\"\"A parsed scenario from a markdown file.\"\"\"\n\n    name: str\n    file_path: str\n    category: str\n    preconditions: list[str]\n    behavioral_expectation: str\n    evaluation_method: str\n    pass_criteria: str\n    evidence_required: list[str]\n\n\n@dataclass\nclass ScenarioResult:\n    \"\"\"Result of running a single scenario.\"\"\"\n\n    name: str\n    file_path: str\n    category: str\n    passed: bool\n    exit_code: int\n    stdout: str\n    stderr: str\n    duration_seconds: float\n    error_summary: str = \"\"\n\n\n@dataclass\nclass ScenarioReport:\n    \"\"\"Full report from running all scenarios.\"\"\"\n\n    timestamp: str\n    total: int\n    passed: int\n    failed: int\n    skipped: int\n    satisfaction_score: float\n    results: list[dict[str, object]] = field(default_factory=list)\n\n\ndef parse_scenario(path: Path) -> Scenario:\n    \"\"\"Parse a structured markdown scenario file.\"\"\"\n    content = path.read_text()\n\n    def extract_section(heading: str) -> str:\n        pattern = rf\"## {heading}\\s*\\n(.*?)(?=\\n## |\\Z)\"\n        match = re.search(pattern, content, re.DOTALL)\n        return match.group(1).strip() if match else \"\"\n\n    # Extract name from H1\n    name_match = re.search(r\"# Scenario:\\s*(.+)\", content)\n    name = name_match.group(1).strip() if name_match else path.stem\n\n    category = extract_section(\"Category\").strip().lower()\n\n    preconditions_raw = extract_section(\"Preconditions\")\n    preconditions = [\n        line.lstrip(\"- \").strip()\n        for line in preconditions_raw.splitlines()\n        if line.strip().startswith(\"-\")\n    ]\n\n    behavioral_expectation = extract_section(\"Behavioral Expectation\")\n\n    eval_raw = extract_section(\"Evaluation Method\")\n    # Extract code block content\n    code_match = re.search(r\"```(?:bash|sh)?\\s*\\n(.*?)```\", eval_raw, re.DOTALL)\n    evaluation_method = code_match.group(1).strip() if code_match else eval_raw\n\n    pass_criteria = extract_section(\"Pass Criteria\")\n\n    evidence_raw = extract_section(\"Evidence Required\")\n    evidence_required = [\n        line.lstrip(\"- \").strip()\n        for line in evidence_raw.splitlines()\n        if line.strip().startswith(\"-\")\n    ]\n\n    return Scenario(\n        name=name,\n        file_path=str(path),\n        category=category,\n        preconditions=preconditions,\n        behavioral_expectation=behavioral_expectation,\n        evaluation_method=evaluation_method,\n        pass_criteria=pass_criteria,\n        evidence_required=evidence_required,\n    )\n\n\ndef run_scenario(scenario: Scenario, timeout: int, repo_root: Path) -> ScenarioResult:\n    \"\"\"Execute a single scenario's evaluation method.\"\"\"\n    start = time.time()\n\n    try:\n        result = subprocess.run(\n            [\"bash\", \"-c\", scenario.evaluation_method],\n            capture_output=True,\n            text=True,\n            timeout=timeout,\n            cwd=str(repo_root),\n            env={**os.environ, \"PYTHONPATH\": str(repo_root)},\n        )\n        duration = time.time() - start\n        passed = result.returncode == 0\n\n        error_summary = \"\"\n        if not passed:\n            # Extract the last meaningful error line\n            lines = (result.stderr + result.stdout).strip().splitlines()\n            error_lines = [\n                line\n                for line in lines\n                if \"error\" in line.lower()\n                or \"assert\" in line.lower()\n                or \"FAIL\" in line\n                or \"Traceback\" in line\n            ]\n            if error_lines:\n                error_summary = error_lines[-1]\n            elif lines:\n                error_summary = lines[-1]\n            else:\n                error_summary = \"Unknown error\"\n\n        return ScenarioResult(\n            name=scenario.name,\n            file_path=scenario.file_path,\n            category=scenario.category,\n            passed=passed,\n            exit_code=result.returncode,\n            stdout=result.stdout[-5000:],  # Cap output size\n            stderr=result.stderr[-5000:],\n            duration_seconds=round(duration, 2),\n            error_summary=error_summary,\n        )\n\n    except subprocess.TimeoutExpired:\n        duration = time.time() - start\n        return ScenarioResult(\n            name=scenario.name,\n            file_path=scenario.file_path,\n            category=scenario.category,\n            passed=False,\n            exit_code=-1,\n            stdout=\"\",\n            stderr=f\"TIMEOUT: scenario exceeded {timeout}s limit\",\n            duration_seconds=round(duration, 2),\n            error_summary=f\"Timeout after {timeout}s\",\n        )\n    except Exception as e:\n        duration = time.time() - start\n        return ScenarioResult(\n            name=scenario.name,\n            file_path=scenario.file_path,\n            category=scenario.category,\n            passed=False,\n            exit_code=-2,\n            stdout=\"\",\n            stderr=str(e),\n            duration_seconds=round(duration, 2),\n            error_summary=str(e),\n        )\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Run dark factory holdout scenarios\")\n    parser.add_argument(\n        \"--category\",\n        type=str,\n        default=None,\n        help=\"Filter by category (environment, training, etc.)\",\n    )\n    parser.add_argument(\n        \"--timeout\",\n        type=int,\n        default=300,\n        help=\"Timeout per scenario in seconds (default: 300)\",\n    )\n    parser.add_argument(\n        \"--scenarios-dir\",\n        type=str,\n        default=None,\n        help=\"Path to scenarios directory (default: <repo_root>/scenarios/)\",\n    )\n    parser.add_argument(\n        \"--output\",\n        type=str,\n        default=None,\n        help=\"Output JSON path (default: artifacts/factory/scenario_results.json)\",\n    )\n    args = parser.parse_args()\n\n    # Find repo root (where this script lives in scripts/)\n    repo_root = Path(__file__).resolve().parent.parent\n    if args.scenarios_dir:\n        scenarios_dir = Path(args.scenarios_dir)\n    else:\n        scenarios_dir = repo_root / \"scenarios\"\n    if args.output:\n        output_path = Path(args.output)\n    else:\n        output_path = (\n            repo_root / \"artifacts\" / \"factory\" / \"scenario_results.json\"\n        )\n\n    # Discover and parse scenarios\n    scenario_files = sorted(scenarios_dir.glob(\"*.md\"))\n    if not scenario_files:\n        print(f\"ERROR: No scenario files found in {scenarios_dir}\", file=sys.stderr)\n        return 1\n\n    scenarios = [parse_scenario(f) for f in scenario_files]\n\n    # Filter by category if requested\n    if args.category:\n        scenarios = [s for s in scenarios if s.category == args.category.lower()]\n        if not scenarios:\n            msg = f\"No scenarios match category '{args.category}'\"\n            print(f\"ERROR: {msg}\", file=sys.stderr)\n            return 1\n\n    print(f\"Running {len(scenarios)} scenario(s)...\")\n    print(\"=\" * 60)\n\n    results: list[ScenarioResult] = []\n    for i, scenario in enumerate(scenarios, 1):\n        print(f\"\\n[{i}/{len(scenarios)}] {scenario.name} ({scenario.category})\")\n        print(\"-\" * 40)\n\n        result = run_scenario(scenario, args.timeout, repo_root)\n        results.append(result)\n\n        status = \"PASS\" if result.passed else \"FAIL\"\n        print(f\"  {status} ({result.duration_seconds}s)\")\n        if not result.passed and result.error_summary:\n            print(f\"  Error: {result.error_summary}\")\n\n    # Compute satisfaction score\n    passed_count = sum(1 for r in results if r.passed)\n    failed_count = sum(1 for r in results if not r.passed)\n    total = len(results)\n    satisfaction = passed_count / total if total > 0 else 0.0\n\n    # Build report\n    report = ScenarioReport(\n        timestamp=time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n        total=total,\n        passed=passed_count,\n        failed=failed_count,\n        skipped=0,\n        satisfaction_score=round(satisfaction, 4),\n        results=[asdict(r) for r in results],\n    )\n\n    # Write output\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    output_path.write_text(json.dumps(asdict(report), indent=2) + \"\\n\")\n\n    # Summary\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"RESULTS: {passed_count}/{total} passed | Satisfaction: {satisfaction:.0%}\")\n    print(f\"Report: {output_path}\")\n\n    if failed_count > 0:\n        print(\"\\nFailed scenarios:\")\n        for r in results:\n            if not r.passed:\n                print(f\"  - {r.name}: {r.error_summary}\")\n        return 1\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n"
    },
    "scripts/strip_holdout.py": {
      "additions": 214,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/scripts/strip_holdout.py b/scripts/strip_holdout.py\nnew file mode 100644\nindex 0000000..96a3fc6\n--- /dev/null\n+++ b/scripts/strip_holdout.py\n@@ -0,0 +1,214 @@\n+#!/usr/bin/env python3\n+\"\"\"Strip holdout scenarios from the working tree.\n+\n+Deterministic script that removes /scenarios/ and scenario-related\n+Makefile targets before providing a branch to the attractor (Codex).\n+\n+This is a factory gate \u2014 non-circumventable. The attractor literally\n+cannot see evaluation criteria because they don't exist on its branch.\n+\n+Usage:\n+    python scripts/strip_holdout.py              # strip and commit\n+    python scripts/strip_holdout.py --dry-run    # show what would be removed\n+    python scripts/strip_holdout.py --no-commit  # strip but don't commit\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import re\n+import shutil\n+import subprocess\n+import sys\n+from pathlib import Path\n+\n+MARKER = \"[factory:holdout-stripped]\"\n+\n+# Makefile targets that reference scenarios (removed during stripping)\n+SCENARIO_TARGETS = [\n+    \"run-scenarios\",\n+    \"compile-feedback\",\n+]\n+\n+\n+def strip_scenarios(repo_root: Path, dry_run: bool = False) -> list[str]:\n+    \"\"\"Remove /scenarios/ directory entirely.\n+\n+    Returns list of removed paths for logging.\n+    \"\"\"\n+    scenarios_dir = repo_root / \"scenarios\"\n+    removed: list[str] = []\n+\n+    if scenarios_dir.exists():\n+        for f in sorted(scenarios_dir.rglob(\"*\")):\n+            if f.is_file():\n+                removed.append(str(f.relative_to(repo_root)))\n+        if not dry_run:\n+            shutil.rmtree(scenarios_dir)\n+    else:\n+        print(\"WARNING: /scenarios/ directory not found \u2014 already stripped?\")\n+\n+    return removed\n+\n+\n+def strip_makefile_targets(\n+    repo_root: Path, dry_run: bool = False\n+) -> list[str]:\n+    \"\"\"Comment out scenario-related Makefile targets.\n+\n+    Instead of deleting lines (which makes restoration harder),\n+    we wrap them in a clearly-marked block comment.\n+\n+    Returns list of target names that were commented out.\n+    \"\"\"\n+    makefile = repo_root / \"Makefile\"\n+    if not makefile.exists():\n+        return []\n+\n+    content = makefile.read_text()\n+    commented_targets: list[str] = []\n+\n+    for target in SCENARIO_TARGETS:\n+        # Match the target and its recipe (indented lines following it)\n+        pattern = rf\"^({target}:.*(?:\\n\\t.*)*)\"\n+        match = re.search(pattern, content, re.MULTILINE)\n+        if match:\n+            original = match.group(1)\n+            replacement = (\n+                f\"# {MARKER} \u2014 stripped by strip_holdout.py\\n\"\n+                + \"\\n\".join(f\"# {line}\" for line in original.splitlines())\n+                + f\"\\n# end {MARKER}\"\n+            )\n+            if not dry_run:\n+                content = content.replace(original, replacement)\n+            commented_targets.append(target)\n+\n+    if commented_targets and not dry_run:\n+        makefile.write_text(content)\n+\n+    return commented_targets\n+\n+\n+def verify_stripped(repo_root: Path) -> list[str]:\n+    \"\"\"Verify that stripping was complete.\n+\n+    Returns list of verification failures (empty = success).\n+    \"\"\"\n+    failures: list[str] = []\n+\n+    scenarios_dir = repo_root / \"scenarios\"\n+    if scenarios_dir.exists():\n+        remaining = [f for f in scenarios_dir.rglob(\"*\") if f.is_file()]\n+        if remaining:\n+            failures.append(\n+                f\"scenarios/ still has {len(remaining)} files: \"\n+                + \", \".join(f.name for f in remaining[:5])\n+            )\n+        elif scenarios_dir.is_dir():\n+            # Directory exists but is empty \u2014 still a signal to Codex\n+            failures.append(\n+                \"scenarios/ directory still exists (should be fully removed)\"\n+            )\n+\n+    return failures\n+\n+\n+def git_commit_strip(repo_root: Path) -> bool:\n+    \"\"\"Commit the stripping as a marker commit.\n+\n+    Returns True if commit was created, False if nothing to commit.\n+    \"\"\"\n+    subprocess.run(\n+        [\"git\", \"add\", \"-A\"],\n+        cwd=str(repo_root),\n+        check=True,\n+        capture_output=True,\n+    )\n+\n+    # Check if there's anything to commit\n+    result = subprocess.run(\n+        [\"git\", \"diff\", \"--staged\", \"--quiet\"],\n+        cwd=str(repo_root),\n+        capture_output=True,\n+    )\n+    if result.returncode == 0:\n+        print(\"Nothing to commit \u2014 working tree already clean\")\n+        return False\n+\n+    subprocess.run(\n+        [\n+            \"git\",\n+            \"commit\",\n+            \"-m\",\n+            f\"{MARKER} Strip holdout scenarios before attractor\\n\\n\"\n+            \"Deterministic removal of /scenarios/ and related Makefile targets.\\n\"\n+            \"Attractor (Codex) cannot see evaluation criteria on this branch.\\n\"\n+            \"Restore with: python scripts/restore_holdout.py\",\n+        ],\n+        cwd=str(repo_root),\n+        check=True,\n+        capture_output=True,\n+    )\n+    return True\n+\n+\n+def main() -> int:\n+    parser = argparse.ArgumentParser(\n+        description=\"Strip holdout scenarios from working tree\"\n+    )\n+    parser.add_argument(\n+        \"--dry-run\",\n+        action=\"store_true\",\n+        help=\"Show what would be removed without making changes\",\n+    )\n+    parser.add_argument(\n+        \"--no-commit\",\n+        action=\"store_true\",\n+        help=\"Strip but don't create a git commit\",\n+    )\n+    args = parser.parse_args()\n+\n+    repo_root = Path(__file__).resolve().parent.parent\n+\n+    print(f\"{'DRY RUN \u2014 ' if args.dry_run else ''}Stripping holdout scenarios...\")\n+\n+    # Step 1: Remove scenarios\n+    removed = strip_scenarios(repo_root, dry_run=args.dry_run)\n+    if removed:\n+        print(f\"  Removed {len(removed)} scenario files\")\n+        for f in removed:\n+            print(f\"    - {f}\")\n+    else:\n+        print(\"  No scenario files found\")\n+\n+    # Step 2: Comment out Makefile targets\n+    commented = strip_makefile_targets(repo_root, dry_run=args.dry_run)\n+    if commented:\n+        print(f\"  Commented out Makefile targets: {', '.join(commented)}\")\n+\n+    if args.dry_run:\n+        print(\"\\nDry run complete \u2014 no changes made\")\n+        return 0\n+\n+    # Step 3: Verify\n+    failures = verify_stripped(repo_root)\n+    if failures:\n+        print(\"\\nVERIFICATION FAILED:\")\n+        for f in failures:\n+            print(f\"  - {f}\")\n+        return 1\n+\n+    print(\"\\nVerification passed \u2014 scenarios fully stripped\")\n+\n+    # Step 4: Commit (optional)\n+    if not args.no_commit:\n+        if git_commit_strip(repo_root):\n+            print(f\"Committed with marker: {MARKER}\")\n+        else:\n+            print(\"No commit needed\")\n+\n+    return 0\n+\n+\n+if __name__ == \"__main__\":\n+    sys.exit(main())\n",
      "raw": "#!/usr/bin/env python3\n\"\"\"Strip holdout scenarios from the working tree.\n\nDeterministic script that removes /scenarios/ and scenario-related\nMakefile targets before providing a branch to the attractor (Codex).\n\nThis is a factory gate \u2014 non-circumventable. The attractor literally\ncannot see evaluation criteria because they don't exist on its branch.\n\nUsage:\n    python scripts/strip_holdout.py              # strip and commit\n    python scripts/strip_holdout.py --dry-run    # show what would be removed\n    python scripts/strip_holdout.py --no-commit  # strip but don't commit\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport re\nimport shutil\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nMARKER = \"[factory:holdout-stripped]\"\n\n# Makefile targets that reference scenarios (removed during stripping)\nSCENARIO_TARGETS = [\n    \"run-scenarios\",\n    \"compile-feedback\",\n]\n\n\ndef strip_scenarios(repo_root: Path, dry_run: bool = False) -> list[str]:\n    \"\"\"Remove /scenarios/ directory entirely.\n\n    Returns list of removed paths for logging.\n    \"\"\"\n    scenarios_dir = repo_root / \"scenarios\"\n    removed: list[str] = []\n\n    if scenarios_dir.exists():\n        for f in sorted(scenarios_dir.rglob(\"*\")):\n            if f.is_file():\n                removed.append(str(f.relative_to(repo_root)))\n        if not dry_run:\n            shutil.rmtree(scenarios_dir)\n    else:\n        print(\"WARNING: /scenarios/ directory not found \u2014 already stripped?\")\n\n    return removed\n\n\ndef strip_makefile_targets(\n    repo_root: Path, dry_run: bool = False\n) -> list[str]:\n    \"\"\"Comment out scenario-related Makefile targets.\n\n    Instead of deleting lines (which makes restoration harder),\n    we wrap them in a clearly-marked block comment.\n\n    Returns list of target names that were commented out.\n    \"\"\"\n    makefile = repo_root / \"Makefile\"\n    if not makefile.exists():\n        return []\n\n    content = makefile.read_text()\n    commented_targets: list[str] = []\n\n    for target in SCENARIO_TARGETS:\n        # Match the target and its recipe (indented lines following it)\n        pattern = rf\"^({target}:.*(?:\\n\\t.*)*)\"\n        match = re.search(pattern, content, re.MULTILINE)\n        if match:\n            original = match.group(1)\n            replacement = (\n                f\"# {MARKER} \u2014 stripped by strip_holdout.py\\n\"\n                + \"\\n\".join(f\"# {line}\" for line in original.splitlines())\n                + f\"\\n# end {MARKER}\"\n            )\n            if not dry_run:\n                content = content.replace(original, replacement)\n            commented_targets.append(target)\n\n    if commented_targets and not dry_run:\n        makefile.write_text(content)\n\n    return commented_targets\n\n\ndef verify_stripped(repo_root: Path) -> list[str]:\n    \"\"\"Verify that stripping was complete.\n\n    Returns list of verification failures (empty = success).\n    \"\"\"\n    failures: list[str] = []\n\n    scenarios_dir = repo_root / \"scenarios\"\n    if scenarios_dir.exists():\n        remaining = [f for f in scenarios_dir.rglob(\"*\") if f.is_file()]\n        if remaining:\n            failures.append(\n                f\"scenarios/ still has {len(remaining)} files: \"\n                + \", \".join(f.name for f in remaining[:5])\n            )\n        elif scenarios_dir.is_dir():\n            # Directory exists but is empty \u2014 still a signal to Codex\n            failures.append(\n                \"scenarios/ directory still exists (should be fully removed)\"\n            )\n\n    return failures\n\n\ndef git_commit_strip(repo_root: Path) -> bool:\n    \"\"\"Commit the stripping as a marker commit.\n\n    Returns True if commit was created, False if nothing to commit.\n    \"\"\"\n    subprocess.run(\n        [\"git\", \"add\", \"-A\"],\n        cwd=str(repo_root),\n        check=True,\n        capture_output=True,\n    )\n\n    # Check if there's anything to commit\n    result = subprocess.run(\n        [\"git\", \"diff\", \"--staged\", \"--quiet\"],\n        cwd=str(repo_root),\n        capture_output=True,\n    )\n    if result.returncode == 0:\n        print(\"Nothing to commit \u2014 working tree already clean\")\n        return False\n\n    subprocess.run(\n        [\n            \"git\",\n            \"commit\",\n            \"-m\",\n            f\"{MARKER} Strip holdout scenarios before attractor\\n\\n\"\n            \"Deterministic removal of /scenarios/ and related Makefile targets.\\n\"\n            \"Attractor (Codex) cannot see evaluation criteria on this branch.\\n\"\n            \"Restore with: python scripts/restore_holdout.py\",\n        ],\n        cwd=str(repo_root),\n        check=True,\n        capture_output=True,\n    )\n    return True\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(\n        description=\"Strip holdout scenarios from working tree\"\n    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=\"Show what would be removed without making changes\",\n    )\n    parser.add_argument(\n        \"--no-commit\",\n        action=\"store_true\",\n        help=\"Strip but don't create a git commit\",\n    )\n    args = parser.parse_args()\n\n    repo_root = Path(__file__).resolve().parent.parent\n\n    print(f\"{'DRY RUN \u2014 ' if args.dry_run else ''}Stripping holdout scenarios...\")\n\n    # Step 1: Remove scenarios\n    removed = strip_scenarios(repo_root, dry_run=args.dry_run)\n    if removed:\n        print(f\"  Removed {len(removed)} scenario files\")\n        for f in removed:\n            print(f\"    - {f}\")\n    else:\n        print(\"  No scenario files found\")\n\n    # Step 2: Comment out Makefile targets\n    commented = strip_makefile_targets(repo_root, dry_run=args.dry_run)\n    if commented:\n        print(f\"  Commented out Makefile targets: {', '.join(commented)}\")\n\n    if args.dry_run:\n        print(\"\\nDry run complete \u2014 no changes made\")\n        return 0\n\n    # Step 3: Verify\n    failures = verify_stripped(repo_root)\n    if failures:\n        print(\"\\nVERIFICATION FAILED:\")\n        for f in failures:\n            print(f\"  - {f}\")\n        return 1\n\n    print(\"\\nVerification passed \u2014 scenarios fully stripped\")\n\n    # Step 4: Commit (optional)\n    if not args.no_commit:\n        if git_commit_strip(repo_root):\n            print(f\"Committed with marker: {MARKER}\")\n        else:\n            print(\"No commit needed\")\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
      "base": ""
    },
    "scripts/verify_whitepapers.py": {
      "additions": 22,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/scripts/verify_whitepapers.py b/scripts/verify_whitepapers.py\nnew file mode 100644\nindex 0000000..09d96e5\n--- /dev/null\n+++ b/scripts/verify_whitepapers.py\n@@ -0,0 +1,22 @@\n+from __future__ import annotations\n+\n+import json\n+from pathlib import Path\n+\n+from pypdf import PdfReader\n+\n+\n+def main() -> None:\n+    manifest = json.loads(Path(\"docs/whitepapers/manifest.json\").read_text(encoding=\"utf-8\"))\n+    for paper in manifest[\"papers\"]:\n+        path = Path(\"docs/whitepapers/pdfs\") / paper[\"filename\"]\n+        if not path.exists():\n+            raise SystemExit(f\"Missing pdf: {path}\")\n+        reader = PdfReader(str(path))\n+        if len(reader.pages) == 0:\n+            raise SystemExit(f\"Invalid pdf: {path}\")\n+    print(\"whitepapers verified\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
      "raw": "from __future__ import annotations\n\nimport json\nfrom pathlib import Path\n\nfrom pypdf import PdfReader\n\n\ndef main() -> None:\n    manifest = json.loads(Path(\"docs/whitepapers/manifest.json\").read_text(encoding=\"utf-8\"))\n    for paper in manifest[\"papers\"]:\n        path = Path(\"docs/whitepapers/pdfs\") / paper[\"filename\"]\n        if not path.exists():\n            raise SystemExit(f\"Missing pdf: {path}\")\n        reader = PdfReader(str(path))\n        if len(reader.pages) == 0:\n            raise SystemExit(f\"Invalid pdf: {path}\")\n    print(\"whitepapers verified\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "base": ""
    },
    "specs/system.md": {
      "additions": 1,
      "deletions": 1,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/specs/system.md b/specs/system.md\nindex 955e708..1be28a6 100644\n--- a/specs/system.md\n+++ b/specs/system.md\n@@ -22,7 +22,7 @@ The final proof is ALL of:\n \n - Python 3.12+\n - pip-based dependencies using requirements.in / requirements-dev.in compiled to requirements.txt / requirements-dev.txt (use pip-tools)\n-- High code quality: ruff, mypy, pytest, pre-commit, GitHub Actions (non-negotiable)\n+- High code quality: ruff, mypy, pytest, git hooks (`make install-hooks`), GitHub Actions CI (non-negotiable)\n - Testing philosophy: no mocks/stubs/patches \u2014 favor functional and behavioral tests\n \n ## Component Architecture\n",
      "raw": "# System Spec\n\n## Purpose\n\nBuild a complete, public, end-to-end system that proves a reinforcement learning agent can learn to play a simple 80s-style game from pixels + controls. The system must be reproducible and self-verifying.\n\n## Proof Requirements\n\nThe final proof is ALL of:\n1. Coded/system verification that learning happened (metrics and statistical checks)\n2. A video demonstration showing qualitative improvement across training checkpoints\n3. A dashboard that correlates learning curves with game outcomes and simple \"strategy\" indicators\n4. All subcomponents validated for functional, architectural, and integration requirements\n\n## Licensing Constraint\n\n- No proprietary game ROMs\n- MiniPong is built from scratch as an open-source Gymnasium environment (MIT license)\n- The agent receives only pixels as observation and discrete controls as input\n\n## Repository Standards\n\n- Python 3.12+\n- pip-based dependencies using requirements.in / requirements-dev.in compiled to requirements.txt / requirements-dev.txt (use pip-tools)\n- High code quality: ruff, mypy, pytest, git hooks (`make install-hooks`), GitHub Actions CI (non-negotiable)\n- Testing philosophy: no mocks/stubs/patches \u2014 favor functional and behavioral tests\n\n## Component Architecture\n\n| Component | Location | Purpose |\n|-----------|----------|---------|\n| MiniPong Environment | `src/envs/minipong.py` | Custom Gymnasium env with deterministic physics |\n| DQN Agent | `src/agents/dqn_agent.py`, `src/rl/` | Classic DQN with replay, target net, epsilon-greedy |\n| Training Pipeline | `src/train/train_dqn.py` | Orchestrates training, eval, checkpointing |\n| Evaluation | `src/train/evaluate.py` | Fixed-seed policy evaluation |\n| Video Recording | `src/train/record_video.py` | Evaluation video capture |\n| Learning Verification | `src/train/verify_learning.py` | Statistical proof that learning occurred |\n| Dashboard | `src/dashboard/app.py` | Streamlit app for observability |\n| Montage | `src/train/make_montage.py` | Video progression compilation |\n\n## Makefile Targets (must implement)\n\nmake deps, make lint, make typecheck, make test, make docker-build, make docker-smoke, make env-smoke, make train-smoke, make eval-smoke, make verify-learning, make dashboard, make validate (lint + typecheck + test + docker-build + docker-smoke + env-smoke)\n\n## Docker\n\n- `Dockerfile.train` \u2014 CUDA default, allow python:3.12-slim fallback\n- `Dockerfile.demo` \u2014 python:3.12-slim\n\n## CI\n\nGitHub Actions \u2014 install deps, ruff, mypy, pytest, docker build, docker smoke. Must not require GPU.\n",
      "base": "# System Spec\n\n## Purpose\n\nBuild a complete, public, end-to-end system that proves a reinforcement learning agent can learn to play a simple 80s-style game from pixels + controls. The system must be reproducible and self-verifying.\n\n## Proof Requirements\n\nThe final proof is ALL of:\n1. Coded/system verification that learning happened (metrics and statistical checks)\n2. A video demonstration showing qualitative improvement across training checkpoints\n3. A dashboard that correlates learning curves with game outcomes and simple \"strategy\" indicators\n4. All subcomponents validated for functional, architectural, and integration requirements\n\n## Licensing Constraint\n\n- No proprietary game ROMs\n- MiniPong is built from scratch as an open-source Gymnasium environment (MIT license)\n- The agent receives only pixels as observation and discrete controls as input\n\n## Repository Standards\n\n- Python 3.12+\n- pip-based dependencies using requirements.in / requirements-dev.in compiled to requirements.txt / requirements-dev.txt (use pip-tools)\n- High code quality: ruff, mypy, pytest, pre-commit, GitHub Actions (non-negotiable)\n- Testing philosophy: no mocks/stubs/patches \u2014 favor functional and behavioral tests\n\n## Component Architecture\n\n| Component | Location | Purpose |\n|-----------|----------|---------|\n| MiniPong Environment | `src/envs/minipong.py` | Custom Gymnasium env with deterministic physics |\n| DQN Agent | `src/agents/dqn_agent.py`, `src/rl/` | Classic DQN with replay, target net, epsilon-greedy |\n| Training Pipeline | `src/train/train_dqn.py` | Orchestrates training, eval, checkpointing |\n| Evaluation | `src/train/evaluate.py` | Fixed-seed policy evaluation |\n| Video Recording | `src/train/record_video.py` | Evaluation video capture |\n| Learning Verification | `src/train/verify_learning.py` | Statistical proof that learning occurred |\n| Dashboard | `src/dashboard/app.py` | Streamlit app for observability |\n| Montage | `src/train/make_montage.py` | Video progression compilation |\n\n## Makefile Targets (must implement)\n\nmake deps, make lint, make typecheck, make test, make docker-build, make docker-smoke, make env-smoke, make train-smoke, make eval-smoke, make verify-learning, make dashboard, make validate (lint + typecheck + test + docker-build + docker-smoke + env-smoke)\n\n## Docker\n\n- `Dockerfile.train` \u2014 CUDA default, allow python:3.12-slim fallback\n- `Dockerfile.demo` \u2014 python:3.12-slim\n\n## CI\n\nGitHub Actions \u2014 install deps, ruff, mypy, pytest, docker build, docker smoke. Must not require GPU.\n"
    },
    "src/agents/__init__.py": {
      "additions": 1,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/agents/__init__.py b/src/agents/__init__.py\nindex e1610a3..62617ff 100644\n--- a/src/agents/__init__.py\n+++ b/src/agents/__init__.py\n@@ -1,5 +1 @@\n-\"\"\"Play-agent implementations.\"\"\"\n-\n-from .dqn_agent import DQNAgent\n-\n-__all__ = [\"DQNAgent\"]\n+\"\"\"Agent package.\"\"\"\n",
      "raw": "\"\"\"Agent package.\"\"\"\n",
      "base": "\"\"\"Play-agent implementations.\"\"\"\n\nfrom .dqn_agent import DQNAgent\n\n__all__ = [\"DQNAgent\"]\n"
    },
    "src/agents/dqn_agent.py": {
      "additions": 62,
      "deletions": 15,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/agents/dqn_agent.py b/src/agents/dqn_agent.py\nindex aae12b9..fa88313 100644\n--- a/src/agents/dqn_agent.py\n+++ b/src/agents/dqn_agent.py\n@@ -1,27 +1,74 @@\n-\"\"\"DQN agent stub.\"\"\"\n+\"\"\"DQN agent implementation.\"\"\"\n \n from __future__ import annotations\n \n-from typing import Any\n+from dataclasses import dataclass\n \n+import numpy as np\n+import torch\n+import torch.nn.functional as F\n+from torch import optim\n \n-class DQNAgent:\n-    \"\"\"Placeholder DQN agent implementation.\"\"\"\n+from src.rl.networks import create_q_network\n+from src.rl.replay import ReplayBuffer, Transition\n \n-    def __init__(self, num_actions: int) -> None:\n-        self.num_actions = num_actions\n \n-    def act(self, observation: Any) -> int:\n-        \"\"\"Select an action given an observation.\"\"\"\n+@dataclass\n+class DQNConfig:\n+    gamma: float = 0.99\n+    lr: float = 1e-3\n+    batch_size: int = 32\n+\n+\n+class DQNAgent:\n+    def __init__(\n+        self,\n+        obs_shape: tuple[int, int, int],\n+        num_actions: int,\n+        replay: ReplayBuffer,\n+        config: DQNConfig,\n+    ) -> None:\n+        self.num_actions = num_actions\n+        self.replay = replay\n+        self.config = config\n+        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+        self.online = create_q_network(obs_shape, num_actions).to(self.device)\n+        self.target = create_q_network(obs_shape, num_actions).to(self.device)\n+        self.target.load_state_dict(self.online.state_dict())\n+        self.optimizer = optim.Adam(self.online.parameters(), lr=config.lr)\n \n-        raise NotImplementedError(\"Action selection not implemented yet.\")\n+    def act(self, observation: np.ndarray, epsilon: float) -> int:\n+        if np.random.rand() < epsilon:\n+            return int(np.random.randint(self.num_actions))\n+        with torch.no_grad():\n+            obs = torch.tensor(observation[None], dtype=torch.float32, device=self.device)\n+            q = self.online(obs)\n+            return int(torch.argmax(q, dim=1).item())\n \n-    def observe(self, transition: Any) -> None:\n-        \"\"\"Record a transition for learning.\"\"\"\n+    def observe(self, transition: Transition) -> None:\n+        self.replay.add(transition)\n \n-        raise NotImplementedError(\"Observation handling not implemented yet.\")\n+    def update(self) -> float:\n+        batch = self.replay.sample(self.config.batch_size)\n+        obs = torch.tensor(\n+            np.stack([t.obs for t in batch]), dtype=torch.float32, device=self.device\n+        )\n+        actions = torch.tensor([t.action for t in batch], dtype=torch.int64, device=self.device)\n+        rewards = torch.tensor([t.reward for t in batch], dtype=torch.float32, device=self.device)\n+        next_obs = torch.tensor(\n+            np.stack([t.next_obs for t in batch]), dtype=torch.float32, device=self.device\n+        )\n+        dones = torch.tensor([t.done for t in batch], dtype=torch.float32, device=self.device)\n \n-    def update(self) -> None:\n-        \"\"\"Run a learning update step.\"\"\"\n+        q = self.online(obs).gather(1, actions.unsqueeze(1)).squeeze(1)\n+        with torch.no_grad():\n+            q_next = self.target(next_obs).max(dim=1).values\n+            target = rewards + self.config.gamma * (1 - dones) * q_next\n+        loss = F.mse_loss(q, target)\n+        self.optimizer.zero_grad()\n+        loss.backward()\n+        self.optimizer.step()\n+        return float(loss.item())\n \n-        raise NotImplementedError(\"Update step not implemented yet.\")\n+    def sync_target(self) -> None:\n+        self.target.load_state_dict(self.online.state_dict())\n",
      "raw": "\"\"\"DQN agent implementation.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import optim\n\nfrom src.rl.networks import create_q_network\nfrom src.rl.replay import ReplayBuffer, Transition\n\n\n@dataclass\nclass DQNConfig:\n    gamma: float = 0.99\n    lr: float = 1e-3\n    batch_size: int = 32\n\n\nclass DQNAgent:\n    def __init__(\n        self,\n        obs_shape: tuple[int, int, int],\n        num_actions: int,\n        replay: ReplayBuffer,\n        config: DQNConfig,\n    ) -> None:\n        self.num_actions = num_actions\n        self.replay = replay\n        self.config = config\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.online = create_q_network(obs_shape, num_actions).to(self.device)\n        self.target = create_q_network(obs_shape, num_actions).to(self.device)\n        self.target.load_state_dict(self.online.state_dict())\n        self.optimizer = optim.Adam(self.online.parameters(), lr=config.lr)\n\n    def act(self, observation: np.ndarray, epsilon: float) -> int:\n        if np.random.rand() < epsilon:\n            return int(np.random.randint(self.num_actions))\n        with torch.no_grad():\n            obs = torch.tensor(observation[None], dtype=torch.float32, device=self.device)\n            q = self.online(obs)\n            return int(torch.argmax(q, dim=1).item())\n\n    def observe(self, transition: Transition) -> None:\n        self.replay.add(transition)\n\n    def update(self) -> float:\n        batch = self.replay.sample(self.config.batch_size)\n        obs = torch.tensor(\n            np.stack([t.obs for t in batch]), dtype=torch.float32, device=self.device\n        )\n        actions = torch.tensor([t.action for t in batch], dtype=torch.int64, device=self.device)\n        rewards = torch.tensor([t.reward for t in batch], dtype=torch.float32, device=self.device)\n        next_obs = torch.tensor(\n            np.stack([t.next_obs for t in batch]), dtype=torch.float32, device=self.device\n        )\n        dones = torch.tensor([t.done for t in batch], dtype=torch.float32, device=self.device)\n\n        q = self.online(obs).gather(1, actions.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            q_next = self.target(next_obs).max(dim=1).values\n            target = rewards + self.config.gamma * (1 - dones) * q_next\n        loss = F.mse_loss(q, target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return float(loss.item())\n\n    def sync_target(self) -> None:\n        self.target.load_state_dict(self.online.state_dict())\n",
      "base": "\"\"\"DQN agent stub.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any\n\n\nclass DQNAgent:\n    \"\"\"Placeholder DQN agent implementation.\"\"\"\n\n    def __init__(self, num_actions: int) -> None:\n        self.num_actions = num_actions\n\n    def act(self, observation: Any) -> int:\n        \"\"\"Select an action given an observation.\"\"\"\n\n        raise NotImplementedError(\"Action selection not implemented yet.\")\n\n    def observe(self, transition: Any) -> None:\n        \"\"\"Record a transition for learning.\"\"\"\n\n        raise NotImplementedError(\"Observation handling not implemented yet.\")\n\n    def update(self) -> None:\n        \"\"\"Run a learning update step.\"\"\"\n\n        raise NotImplementedError(\"Update step not implemented yet.\")\n"
    },
    "src/dashboard/app.py": {
      "additions": 60,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/src/dashboard/app.py b/src/dashboard/app.py\nnew file mode 100644\nindex 0000000..3e396ff\n--- /dev/null\n+++ b/src/dashboard/app.py\n@@ -0,0 +1,60 @@\n+from __future__ import annotations\n+\n+import json\n+from pathlib import Path\n+\n+import pandas as pd\n+import streamlit as st\n+\n+st.set_page_config(page_title=\"MiniPong RL Dashboard\", layout=\"wide\")\n+st.title(\"MiniPong RL Learning Dashboard\")\n+\n+base = Path(\"artifacts\")\n+runs = sorted([p.name for p in base.glob(\"*\") if p.is_dir()])\n+if not runs:\n+    st.warning(\"No runs found under artifacts/\")\n+    st.stop()\n+run_id = st.selectbox(\"Run\", runs)\n+run_dir = base / run_id\n+\n+log_path = run_dir / \"logs.jsonl\"\n+if log_path.exists():\n+    rows = [\n+        json.loads(line)\n+        for line in log_path.read_text(encoding=\"utf-8\").splitlines()\n+        if line.strip()\n+    ]\n+    df = pd.DataFrame(rows)\n+    st.subheader(\"Training metrics\")\n+    cols = st.columns(3)\n+    with cols[0]:\n+        if \"train/episode_return\" in df:\n+            st.line_chart(df[[\"step\", \"train/episode_return\"]].dropna().set_index(\"step\"))\n+    with cols[1]:\n+        if \"eval/mean_return\" in df:\n+            st.line_chart(df[[\"step\", \"eval/mean_return\"]].dropna().set_index(\"step\"))\n+    with cols[2]:\n+        if \"train/loss\" in df:\n+            st.line_chart(df[[\"step\", \"train/loss\"]].dropna().set_index(\"step\"))\n+    if \"train/epsilon\" in df:\n+        st.subheader(\"Epsilon\")\n+        st.line_chart(df[[\"step\", \"train/epsilon\"]].dropna().set_index(\"step\"))\n+\n+\n+st.subheader(\"Eval summaries\")\n+eval_files = sorted((run_dir / \"eval\").glob(\"metrics_*.json\"))\n+if eval_files:\n+    eval_rows = []\n+    for f in eval_files:\n+        d = json.loads(f.read_text(encoding=\"utf-8\"))\n+        d[\"file\"] = f.name\n+        eval_rows.append(d)\n+    edf = pd.DataFrame(eval_rows)\n+    st.dataframe(edf)\n+    if {\"mean_hits\", \"mean_return\"}.issubset(edf.columns):\n+        st.scatter_chart(edf[[\"mean_hits\", \"mean_return\"]])\n+\n+st.subheader(\"Videos\")\n+for v in sorted((run_dir / \"videos\").glob(\"*.mp4\")):\n+    st.write(v.name)\n+    st.video(str(v))\n",
      "raw": "from __future__ import annotations\n\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\nimport streamlit as st\n\nst.set_page_config(page_title=\"MiniPong RL Dashboard\", layout=\"wide\")\nst.title(\"MiniPong RL Learning Dashboard\")\n\nbase = Path(\"artifacts\")\nruns = sorted([p.name for p in base.glob(\"*\") if p.is_dir()])\nif not runs:\n    st.warning(\"No runs found under artifacts/\")\n    st.stop()\nrun_id = st.selectbox(\"Run\", runs)\nrun_dir = base / run_id\n\nlog_path = run_dir / \"logs.jsonl\"\nif log_path.exists():\n    rows = [\n        json.loads(line)\n        for line in log_path.read_text(encoding=\"utf-8\").splitlines()\n        if line.strip()\n    ]\n    df = pd.DataFrame(rows)\n    st.subheader(\"Training metrics\")\n    cols = st.columns(3)\n    with cols[0]:\n        if \"train/episode_return\" in df:\n            st.line_chart(df[[\"step\", \"train/episode_return\"]].dropna().set_index(\"step\"))\n    with cols[1]:\n        if \"eval/mean_return\" in df:\n            st.line_chart(df[[\"step\", \"eval/mean_return\"]].dropna().set_index(\"step\"))\n    with cols[2]:\n        if \"train/loss\" in df:\n            st.line_chart(df[[\"step\", \"train/loss\"]].dropna().set_index(\"step\"))\n    if \"train/epsilon\" in df:\n        st.subheader(\"Epsilon\")\n        st.line_chart(df[[\"step\", \"train/epsilon\"]].dropna().set_index(\"step\"))\n\n\nst.subheader(\"Eval summaries\")\neval_files = sorted((run_dir / \"eval\").glob(\"metrics_*.json\"))\nif eval_files:\n    eval_rows = []\n    for f in eval_files:\n        d = json.loads(f.read_text(encoding=\"utf-8\"))\n        d[\"file\"] = f.name\n        eval_rows.append(d)\n    edf = pd.DataFrame(eval_rows)\n    st.dataframe(edf)\n    if {\"mean_hits\", \"mean_return\"}.issubset(edf.columns):\n        st.scatter_chart(edf[[\"mean_hits\", \"mean_return\"]])\n\nst.subheader(\"Videos\")\nfor v in sorted((run_dir / \"videos\").glob(\"*.mp4\")):\n    st.write(v.name)\n    st.video(str(v))\n",
      "base": ""
    },
    "src/dashboard/pages/factory.py": {
      "additions": 274,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/src/dashboard/pages/factory.py b/src/dashboard/pages/factory.py\nnew file mode 100644\nindex 0000000..7b037f4\n--- /dev/null\n+++ b/src/dashboard/pages/factory.py\n@@ -0,0 +1,274 @@\n+\"\"\"Dark Factory \u2014 Satisfaction Dashboard.\n+\n+Provides visibility into the factory convergence loop:\n+- Current satisfaction score\n+- Per-scenario pass/fail breakdown\n+- Convergence trajectory across iterations\n+- Category-level aggregation\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import re\n+from pathlib import Path\n+\n+import pandas as pd\n+import streamlit as st\n+\n+st.set_page_config(\n+    page_title=\"Dark Factory \u2014 Satisfaction\",\n+    layout=\"wide\",\n+)\n+st.title(\"Dark Factory \u2014 Satisfaction Dashboard\")\n+\n+FACTORY_DIR = Path(\"artifacts/factory\")\n+SCENARIOS_DIR = Path(\"scenarios\")\n+\n+\n+# \u2500\u2500 Helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+def load_scenario_results() -> dict | None:\n+    \"\"\"Load the latest scenario results JSON.\"\"\"\n+    path = FACTORY_DIR / \"scenario_results.json\"\n+    if not path.exists():\n+        return None\n+    return json.loads(path.read_text(encoding=\"utf-8\"))\n+\n+\n+def load_iteration_count() -> int:\n+    \"\"\"Load the current iteration count.\"\"\"\n+    path = FACTORY_DIR / \"iteration_count.txt\"\n+    if not path.exists():\n+        return 0\n+    try:\n+        return int(path.read_text(encoding=\"utf-8\").strip())\n+    except ValueError:\n+        return 0\n+\n+\n+def load_feedback_history() -> list[dict]:\n+    \"\"\"Parse satisfaction scores from all feedback files.\"\"\"\n+    history = []\n+    for path in sorted(FACTORY_DIR.glob(\"feedback_iter_*.md\")):\n+        match = re.search(r\"feedback_iter_(\\d+)\\.md\", path.name)\n+        if not match:\n+            continue\n+        iteration = int(match.group(1))\n+        text = path.read_text(encoding=\"utf-8\")\n+\n+        # Extract satisfaction score from feedback\n+        # Format is: \"Satisfaction score: 80%\" or \"**Satisfaction score: 42%**\"\n+        score_match = re.search(\n+            r\"Satisfaction[^:]*:\\s*(\\d+\\.?\\d*)%\", text\n+        )\n+        score = float(score_match.group(1)) / 100 if score_match else 0\n+\n+        # Extract pass/fail counts \u2014 format: \"(5/12 scenarios passed)\"\n+        pass_match = re.search(\n+            r\"(\\d+)\\s*/\\s*(\\d+)\\s*scenario\", text\n+        )\n+        passed = int(pass_match.group(1)) if pass_match else 0\n+        total = int(pass_match.group(2)) if pass_match else 0\n+\n+        history.append({\n+            \"iteration\": iteration,\n+            \"satisfaction\": score,\n+            \"passed\": passed,\n+            \"total\": total,\n+        })\n+    return history\n+\n+\n+def count_scenarios_by_category() -> dict[str, int]:\n+    \"\"\"Count scenario files by category from the scenarios dir.\"\"\"\n+    cats: dict[str, int] = {}\n+    if not SCENARIOS_DIR.exists():\n+        return cats\n+    for path in SCENARIOS_DIR.glob(\"*.md\"):\n+        text = path.read_text(encoding=\"utf-8\")\n+        cat_match = re.search(\n+            r\"##\\s*Category\\s*\\n+\\s*(\\w+)\", text\n+        )\n+        cat = cat_match.group(1) if cat_match else \"unknown\"\n+        cats[cat] = cats.get(cat, 0) + 1\n+    return cats\n+\n+\n+# \u2500\u2500 Main Dashboard \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+results = load_scenario_results()\n+iteration = load_iteration_count()\n+history = load_feedback_history()\n+\n+if results is None and not history:\n+    st.info(\n+        \"No factory data yet. Run `make factory-local` \"\n+        \"or trigger the factory workflow to generate data.\"\n+    )\n+    st.stop()\n+\n+# \u2500\u2500 Top-Level Metrics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+st.subheader(\"Current State\")\n+\n+col1, col2, col3, col4 = st.columns(4)\n+\n+if results:\n+    score = results.get(\"satisfaction_score\", 0.0)\n+    passed = results.get(\"passed\", 0)\n+    total = results.get(\"total\", 0)\n+    failed = results.get(\"failed\", 0)\n+\n+    col1.metric(\n+        \"Satisfaction\",\n+        f\"{score:.0%}\",\n+        delta=(\n+            f\"+{score - history[-2]['satisfaction']:.0%}\"\n+            if len(history) >= 2\n+            else None\n+        ),\n+    )\n+    col2.metric(\"Passed\", f\"{passed}/{total}\")\n+    col3.metric(\"Failed\", str(failed))\n+    col4.metric(\"Iteration\", str(iteration))\n+\n+    gate1_failed = results.get(\"gate1_failed\", False)\n+    if gate1_failed:\n+        st.error(\n+            \"Gate 1 (lint/typecheck/test) failed \u2014 \"\n+            \"scenarios were not evaluated.\"\n+        )\n+else:\n+    col1.metric(\"Satisfaction\", \"N/A\")\n+    col2.metric(\"Passed\", \"N/A\")\n+    col3.metric(\"Failed\", \"N/A\")\n+    col4.metric(\"Iteration\", str(iteration))\n+\n+# \u2500\u2500 Convergence Trajectory \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+if history:\n+    st.subheader(\"Convergence Trajectory\")\n+\n+    df_hist = pd.DataFrame(history)\n+    df_hist = df_hist.set_index(\"iteration\")\n+\n+    chart_col1, chart_col2 = st.columns(2)\n+\n+    with chart_col1:\n+        st.line_chart(\n+            df_hist[[\"satisfaction\"]],\n+            y=\"satisfaction\",\n+            use_container_width=True,\n+        )\n+        st.caption(\"Satisfaction score per iteration\")\n+\n+    with chart_col2:\n+        st.bar_chart(\n+            df_hist[[\"passed\", \"total\"]],\n+            use_container_width=True,\n+        )\n+        st.caption(\"Scenarios passed vs total per iteration\")\n+\n+# \u2500\u2500 Per-Scenario Breakdown \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+if results and results.get(\"results\"):\n+    st.subheader(\"Scenario Breakdown\")\n+\n+    scenario_data = []\n+    for r in results[\"results\"]:\n+        scenario_data.append({\n+            \"Scenario\": r.get(\"name\", \"unknown\"),\n+            \"Category\": r.get(\"category\", \"unknown\"),\n+            \"Status\": \"\u2705 Pass\" if r.get(\"passed\") else \"\u274c Fail\",\n+            \"Duration (s)\": round(r.get(\"duration_seconds\", 0), 1),\n+            \"Error\": (\n+                r.get(\"error_summary\", \"\")[:100]\n+                if not r.get(\"passed\")\n+                else \"\"\n+            ),\n+        })\n+\n+    df_scenarios = pd.DataFrame(scenario_data)\n+\n+    # Category filter\n+    categories = [\"All\"] + sorted(\n+        df_scenarios[\"Category\"].unique().tolist()\n+    )\n+    selected_cat = st.selectbox(\"Filter by category\", categories)\n+    if selected_cat != \"All\":\n+        df_scenarios = df_scenarios[\n+            df_scenarios[\"Category\"] == selected_cat\n+        ]\n+\n+    # Color-code status\n+    st.dataframe(\n+        df_scenarios,\n+        use_container_width=True,\n+        hide_index=True,\n+    )\n+\n+    # \u2500\u2500 Category Aggregation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+    st.subheader(\"Category Summary\")\n+\n+    all_results = results.get(\"results\", [])\n+    cat_agg: dict[str, dict] = {}\n+    for r in all_results:\n+        cat = r.get(\"category\", \"unknown\")\n+        if cat not in cat_agg:\n+            cat_agg[cat] = {\"passed\": 0, \"total\": 0}\n+        cat_agg[cat][\"total\"] += 1\n+        if r.get(\"passed\"):\n+            cat_agg[cat][\"passed\"] += 1\n+\n+    cat_rows = []\n+    for cat, counts in sorted(cat_agg.items()):\n+        pct = (\n+            counts[\"passed\"] / counts[\"total\"]\n+            if counts[\"total\"] > 0\n+            else 0\n+        )\n+        cat_rows.append({\n+            \"Category\": cat,\n+            \"Passed\": counts[\"passed\"],\n+            \"Total\": counts[\"total\"],\n+            \"Score\": f\"{pct:.0%}\",\n+        })\n+\n+    st.dataframe(\n+        pd.DataFrame(cat_rows),\n+        use_container_width=True,\n+        hide_index=True,\n+    )\n+\n+# \u2500\u2500 Scenario Coverage \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+st.subheader(\"Scenario Coverage\")\n+\n+cat_counts = count_scenarios_by_category()\n+if cat_counts:\n+    cov_df = pd.DataFrame(\n+        [\n+            {\"Category\": k, \"Scenarios\": v}\n+            for k, v in sorted(cat_counts.items())\n+        ]\n+    )\n+    st.bar_chart(cov_df.set_index(\"Category\"), use_container_width=True)\n+    st.caption(\n+        f\"Total: {sum(cat_counts.values())} scenarios \"\n+        f\"across {len(cat_counts)} categories\"\n+    )\n+else:\n+    st.info(\"No scenario files found in /scenarios/\")\n+\n+# \u2500\u2500 Latest Feedback \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+feedback_files = sorted(FACTORY_DIR.glob(\"feedback_iter_*.md\"))\n+if feedback_files:\n+    st.subheader(\"Latest Feedback\")\n+    latest = feedback_files[-1]\n+    with st.expander(f\"\ud83d\udcc4 {latest.name}\", expanded=False):\n+        st.markdown(latest.read_text(encoding=\"utf-8\"))\n",
      "raw": "\"\"\"Dark Factory \u2014 Satisfaction Dashboard.\n\nProvides visibility into the factory convergence loop:\n- Current satisfaction score\n- Per-scenario pass/fail breakdown\n- Convergence trajectory across iterations\n- Category-level aggregation\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport re\nfrom pathlib import Path\n\nimport pandas as pd\nimport streamlit as st\n\nst.set_page_config(\n    page_title=\"Dark Factory \u2014 Satisfaction\",\n    layout=\"wide\",\n)\nst.title(\"Dark Factory \u2014 Satisfaction Dashboard\")\n\nFACTORY_DIR = Path(\"artifacts/factory\")\nSCENARIOS_DIR = Path(\"scenarios\")\n\n\n# \u2500\u2500 Helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ndef load_scenario_results() -> dict | None:\n    \"\"\"Load the latest scenario results JSON.\"\"\"\n    path = FACTORY_DIR / \"scenario_results.json\"\n    if not path.exists():\n        return None\n    return json.loads(path.read_text(encoding=\"utf-8\"))\n\n\ndef load_iteration_count() -> int:\n    \"\"\"Load the current iteration count.\"\"\"\n    path = FACTORY_DIR / \"iteration_count.txt\"\n    if not path.exists():\n        return 0\n    try:\n        return int(path.read_text(encoding=\"utf-8\").strip())\n    except ValueError:\n        return 0\n\n\ndef load_feedback_history() -> list[dict]:\n    \"\"\"Parse satisfaction scores from all feedback files.\"\"\"\n    history = []\n    for path in sorted(FACTORY_DIR.glob(\"feedback_iter_*.md\")):\n        match = re.search(r\"feedback_iter_(\\d+)\\.md\", path.name)\n        if not match:\n            continue\n        iteration = int(match.group(1))\n        text = path.read_text(encoding=\"utf-8\")\n\n        # Extract satisfaction score from feedback\n        # Format is: \"Satisfaction score: 80%\" or \"**Satisfaction score: 42%**\"\n        score_match = re.search(\n            r\"Satisfaction[^:]*:\\s*(\\d+\\.?\\d*)%\", text\n        )\n        score = float(score_match.group(1)) / 100 if score_match else 0\n\n        # Extract pass/fail counts \u2014 format: \"(5/12 scenarios passed)\"\n        pass_match = re.search(\n            r\"(\\d+)\\s*/\\s*(\\d+)\\s*scenario\", text\n        )\n        passed = int(pass_match.group(1)) if pass_match else 0\n        total = int(pass_match.group(2)) if pass_match else 0\n\n        history.append({\n            \"iteration\": iteration,\n            \"satisfaction\": score,\n            \"passed\": passed,\n            \"total\": total,\n        })\n    return history\n\n\ndef count_scenarios_by_category() -> dict[str, int]:\n    \"\"\"Count scenario files by category from the scenarios dir.\"\"\"\n    cats: dict[str, int] = {}\n    if not SCENARIOS_DIR.exists():\n        return cats\n    for path in SCENARIOS_DIR.glob(\"*.md\"):\n        text = path.read_text(encoding=\"utf-8\")\n        cat_match = re.search(\n            r\"##\\s*Category\\s*\\n+\\s*(\\w+)\", text\n        )\n        cat = cat_match.group(1) if cat_match else \"unknown\"\n        cats[cat] = cats.get(cat, 0) + 1\n    return cats\n\n\n# \u2500\u2500 Main Dashboard \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nresults = load_scenario_results()\niteration = load_iteration_count()\nhistory = load_feedback_history()\n\nif results is None and not history:\n    st.info(\n        \"No factory data yet. Run `make factory-local` \"\n        \"or trigger the factory workflow to generate data.\"\n    )\n    st.stop()\n\n# \u2500\u2500 Top-Level Metrics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nst.subheader(\"Current State\")\n\ncol1, col2, col3, col4 = st.columns(4)\n\nif results:\n    score = results.get(\"satisfaction_score\", 0.0)\n    passed = results.get(\"passed\", 0)\n    total = results.get(\"total\", 0)\n    failed = results.get(\"failed\", 0)\n\n    col1.metric(\n        \"Satisfaction\",\n        f\"{score:.0%}\",\n        delta=(\n            f\"+{score - history[-2]['satisfaction']:.0%}\"\n            if len(history) >= 2\n            else None\n        ),\n    )\n    col2.metric(\"Passed\", f\"{passed}/{total}\")\n    col3.metric(\"Failed\", str(failed))\n    col4.metric(\"Iteration\", str(iteration))\n\n    gate1_failed = results.get(\"gate1_failed\", False)\n    if gate1_failed:\n        st.error(\n            \"Gate 1 (lint/typecheck/test) failed \u2014 \"\n            \"scenarios were not evaluated.\"\n        )\nelse:\n    col1.metric(\"Satisfaction\", \"N/A\")\n    col2.metric(\"Passed\", \"N/A\")\n    col3.metric(\"Failed\", \"N/A\")\n    col4.metric(\"Iteration\", str(iteration))\n\n# \u2500\u2500 Convergence Trajectory \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nif history:\n    st.subheader(\"Convergence Trajectory\")\n\n    df_hist = pd.DataFrame(history)\n    df_hist = df_hist.set_index(\"iteration\")\n\n    chart_col1, chart_col2 = st.columns(2)\n\n    with chart_col1:\n        st.line_chart(\n            df_hist[[\"satisfaction\"]],\n            y=\"satisfaction\",\n            use_container_width=True,\n        )\n        st.caption(\"Satisfaction score per iteration\")\n\n    with chart_col2:\n        st.bar_chart(\n            df_hist[[\"passed\", \"total\"]],\n            use_container_width=True,\n        )\n        st.caption(\"Scenarios passed vs total per iteration\")\n\n# \u2500\u2500 Per-Scenario Breakdown \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nif results and results.get(\"results\"):\n    st.subheader(\"Scenario Breakdown\")\n\n    scenario_data = []\n    for r in results[\"results\"]:\n        scenario_data.append({\n            \"Scenario\": r.get(\"name\", \"unknown\"),\n            \"Category\": r.get(\"category\", \"unknown\"),\n            \"Status\": \"\u2705 Pass\" if r.get(\"passed\") else \"\u274c Fail\",\n            \"Duration (s)\": round(r.get(\"duration_seconds\", 0), 1),\n            \"Error\": (\n                r.get(\"error_summary\", \"\")[:100]\n                if not r.get(\"passed\")\n                else \"\"\n            ),\n        })\n\n    df_scenarios = pd.DataFrame(scenario_data)\n\n    # Category filter\n    categories = [\"All\"] + sorted(\n        df_scenarios[\"Category\"].unique().tolist()\n    )\n    selected_cat = st.selectbox(\"Filter by category\", categories)\n    if selected_cat != \"All\":\n        df_scenarios = df_scenarios[\n            df_scenarios[\"Category\"] == selected_cat\n        ]\n\n    # Color-code status\n    st.dataframe(\n        df_scenarios,\n        use_container_width=True,\n        hide_index=True,\n    )\n\n    # \u2500\u2500 Category Aggregation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n    st.subheader(\"Category Summary\")\n\n    all_results = results.get(\"results\", [])\n    cat_agg: dict[str, dict] = {}\n    for r in all_results:\n        cat = r.get(\"category\", \"unknown\")\n        if cat not in cat_agg:\n            cat_agg[cat] = {\"passed\": 0, \"total\": 0}\n        cat_agg[cat][\"total\"] += 1\n        if r.get(\"passed\"):\n            cat_agg[cat][\"passed\"] += 1\n\n    cat_rows = []\n    for cat, counts in sorted(cat_agg.items()):\n        pct = (\n            counts[\"passed\"] / counts[\"total\"]\n            if counts[\"total\"] > 0\n            else 0\n        )\n        cat_rows.append({\n            \"Category\": cat,\n            \"Passed\": counts[\"passed\"],\n            \"Total\": counts[\"total\"],\n            \"Score\": f\"{pct:.0%}\",\n        })\n\n    st.dataframe(\n        pd.DataFrame(cat_rows),\n        use_container_width=True,\n        hide_index=True,\n    )\n\n# \u2500\u2500 Scenario Coverage \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nst.subheader(\"Scenario Coverage\")\n\ncat_counts = count_scenarios_by_category()\nif cat_counts:\n    cov_df = pd.DataFrame(\n        [\n            {\"Category\": k, \"Scenarios\": v}\n            for k, v in sorted(cat_counts.items())\n        ]\n    )\n    st.bar_chart(cov_df.set_index(\"Category\"), use_container_width=True)\n    st.caption(\n        f\"Total: {sum(cat_counts.values())} scenarios \"\n        f\"across {len(cat_counts)} categories\"\n    )\nelse:\n    st.info(\"No scenario files found in /scenarios/\")\n\n# \u2500\u2500 Latest Feedback \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nfeedback_files = sorted(FACTORY_DIR.glob(\"feedback_iter_*.md\"))\nif feedback_files:\n    st.subheader(\"Latest Feedback\")\n    latest = feedback_files[-1]\n    with st.expander(f\"\ud83d\udcc4 {latest.name}\", expanded=False):\n        st.markdown(latest.read_text(encoding=\"utf-8\"))\n",
      "base": ""
    },
    "src/envs/__init__.py": {
      "additions": 3,
      "deletions": 4,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/envs/__init__.py b/src/envs/__init__.py\nindex 7a4248e..1b6e43d 100644\n--- a/src/envs/__init__.py\n+++ b/src/envs/__init__.py\n@@ -1,5 +1,4 @@\n-\"\"\"Environment interfaces and helpers.\"\"\"\n+from src.envs.minipong import MiniPongConfig, MiniPongEnv\n+from src.envs.registry import EnvRegistry, default_registry\n \n-from .registry import EnvRegistry, register_env\n-\n-__all__ = [\"EnvRegistry\", \"register_env\"]\n+__all__ = [\"MiniPongConfig\", \"MiniPongEnv\", \"EnvRegistry\", \"default_registry\"]\n",
      "raw": "from src.envs.minipong import MiniPongConfig, MiniPongEnv\nfrom src.envs.registry import EnvRegistry, default_registry\n\n__all__ = [\"MiniPongConfig\", \"MiniPongEnv\", \"EnvRegistry\", \"default_registry\"]\n",
      "base": "\"\"\"Environment interfaces and helpers.\"\"\"\n\nfrom .registry import EnvRegistry, register_env\n\n__all__ = [\"EnvRegistry\", \"register_env\"]\n"
    },
    "src/envs/minipong.py": {
      "additions": 169,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/src/envs/minipong.py b/src/envs/minipong.py\nnew file mode 100644\nindex 0000000..b7cdb13\n--- /dev/null\n+++ b/src/envs/minipong.py\n@@ -0,0 +1,169 @@\n+\"\"\"Open-source deterministic MiniPong environment.\"\"\"\n+\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+from typing import Any\n+\n+import gymnasium as gym\n+import numpy as np\n+from gymnasium import spaces\n+\n+\n+@dataclass\n+class MiniPongConfig:\n+    width: int = 84\n+    height: int = 84\n+    paddle_height: int = 16\n+    paddle_width: int = 3\n+    paddle_speed: int = 3\n+    ball_size: int = 3\n+    max_steps: int = 1200\n+    reward_shaping: bool = False\n+\n+\n+class MiniPongEnv(gym.Env[np.ndarray, int]):\n+    metadata = {\"render_modes\": [\"rgb_array\"], \"render_fps\": 30}\n+\n+    def __init__(\n+        self, render_mode: str | None = None, config: MiniPongConfig | None = None\n+    ) -> None:\n+        self.config = config or MiniPongConfig()\n+        self.render_mode = render_mode\n+        self.action_space = spaces.Discrete(3)\n+        self.observation_space = spaces.Box(\n+            low=0,\n+            high=255,\n+            shape=(self.config.height, self.config.width, 1),\n+            dtype=np.uint8,\n+        )\n+        self._rng = np.random.default_rng(0)\n+        self.steps = 0\n+        self.agent_score = 0\n+        self.opponent_score = 0\n+        self.hits = 0\n+        self.misses = 0\n+        self.rally_length = 0\n+        self.episode_reason = \"running\"\n+\n+        self.agent_y = 0.0\n+        self.opponent_y = 0.0\n+        self.ball_x = 0.0\n+        self.ball_y = 0.0\n+        self.ball_vx = 0.0\n+        self.ball_vy = 0.0\n+\n+    def reset(\n+        self, *, seed: int | None = None, options: dict[str, Any] | None = None\n+    ) -> tuple[np.ndarray, dict[str, Any]]:\n+        super().reset(seed=seed)\n+        if seed is not None:\n+            self._rng = np.random.default_rng(seed)\n+        self.steps = 0\n+        self.hits = 0\n+        self.misses = 0\n+        self.rally_length = 0\n+        self.episode_reason = \"running\"\n+\n+        self.agent_y = (self.config.height - self.config.paddle_height) / 2\n+        self.opponent_y = self.agent_y\n+        self.ball_x = self.config.width / 2\n+        self.ball_y = self.config.height / 2\n+        self.ball_vx = float(self._rng.choice([-2, 2]))\n+        self.ball_vy = float(self._rng.choice([-1, 1]))\n+        return self._obs(), self._info()\n+\n+    def step(self, action: int) -> tuple[np.ndarray, float, bool, bool, dict[str, Any]]:\n+        self.steps += 1\n+        self._move_agent(int(action))\n+        self._move_opponent()\n+        reward = 0.0\n+\n+        self.ball_x += self.ball_vx\n+        self.ball_y += self.ball_vy\n+\n+        if self.ball_y <= 0 or self.ball_y >= self.config.height - self.config.ball_size:\n+            self.ball_vy *= -1\n+            self.ball_y = np.clip(self.ball_y, 0, self.config.height - self.config.ball_size)\n+\n+        left_x = 4\n+        right_x = self.config.width - 4 - self.config.paddle_width\n+\n+        if self.ball_vx < 0 and self.ball_x <= left_x + self.config.paddle_width:\n+            if self.agent_y <= self.ball_y <= self.agent_y + self.config.paddle_height:\n+                self.ball_vx = abs(self.ball_vx)\n+                self.hits += 1\n+                self.rally_length += 1\n+                if self.config.reward_shaping:\n+                    reward += 0.01\n+            else:\n+                reward -= 1.0\n+                self.opponent_score += 1\n+                self.misses += 1\n+                self.episode_reason = \"agent_miss\"\n+                return self._obs(), reward, True, False, self._info()\n+\n+        if self.ball_vx > 0 and self.ball_x + self.config.ball_size >= right_x:\n+            if self.opponent_y <= self.ball_y <= self.opponent_y + self.config.paddle_height:\n+                self.ball_vx = -abs(self.ball_vx)\n+                self.rally_length += 1\n+            else:\n+                reward += 1.0\n+                self.agent_score += 1\n+                self.episode_reason = \"opponent_miss\"\n+                return self._obs(), reward, True, False, self._info()\n+\n+        truncated = self.steps >= self.config.max_steps\n+        if truncated:\n+            self.episode_reason = \"max_steps\"\n+        return self._obs(), reward, False, truncated, self._info()\n+\n+    def render(self) -> np.ndarray:\n+        gray = self._obs().squeeze(-1)\n+        rgb = np.repeat(gray[..., None], 3, axis=2)\n+        return rgb\n+\n+    def _move_agent(self, action: int) -> None:\n+        if action == 0:\n+            self.agent_y -= self.config.paddle_speed\n+        elif action == 1:\n+            self.agent_y += self.config.paddle_speed\n+        self.agent_y = float(\n+            np.clip(self.agent_y, 0, self.config.height - self.config.paddle_height)\n+        )\n+\n+    def _move_opponent(self) -> None:\n+        center = self.opponent_y + self.config.paddle_height / 2\n+        target = self.ball_y + self.config.ball_size / 2\n+        if target > center:\n+            self.opponent_y += self.config.paddle_speed * 0.9\n+        else:\n+            self.opponent_y -= self.config.paddle_speed * 0.9\n+        self.opponent_y = float(\n+            np.clip(self.opponent_y, 0, self.config.height - self.config.paddle_height)\n+        )\n+\n+    def _obs(self) -> np.ndarray:\n+        frame = np.zeros((self.config.height, self.config.width), dtype=np.uint8)\n+        left_x = 4\n+        right_x = self.config.width - 4 - self.config.paddle_width\n+        ay = int(self.agent_y)\n+        oy = int(self.opponent_y)\n+        bx = int(self.ball_x)\n+        by = int(self.ball_y)\n+        frame[ay : ay + self.config.paddle_height, left_x : left_x + self.config.paddle_width] = 255\n+        frame[oy : oy + self.config.paddle_height, right_x : right_x + self.config.paddle_width] = (\n+            255\n+        )\n+        frame[by : by + self.config.ball_size, bx : bx + self.config.ball_size] = 255\n+        return frame[..., None]\n+\n+    def _info(self) -> dict[str, Any]:\n+        return {\n+            \"rally_length\": self.rally_length,\n+            \"hits\": self.hits,\n+            \"misses\": self.misses,\n+            \"agent_score\": self.agent_score,\n+            \"opponent_score\": self.opponent_score,\n+            \"episode_reason\": self.episode_reason,\n+        }\n",
      "raw": "\"\"\"Open-source deterministic MiniPong environment.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport gymnasium as gym\nimport numpy as np\nfrom gymnasium import spaces\n\n\n@dataclass\nclass MiniPongConfig:\n    width: int = 84\n    height: int = 84\n    paddle_height: int = 16\n    paddle_width: int = 3\n    paddle_speed: int = 3\n    ball_size: int = 3\n    max_steps: int = 1200\n    reward_shaping: bool = False\n\n\nclass MiniPongEnv(gym.Env[np.ndarray, int]):\n    metadata = {\"render_modes\": [\"rgb_array\"], \"render_fps\": 30}\n\n    def __init__(\n        self, render_mode: str | None = None, config: MiniPongConfig | None = None\n    ) -> None:\n        self.config = config or MiniPongConfig()\n        self.render_mode = render_mode\n        self.action_space = spaces.Discrete(3)\n        self.observation_space = spaces.Box(\n            low=0,\n            high=255,\n            shape=(self.config.height, self.config.width, 1),\n            dtype=np.uint8,\n        )\n        self._rng = np.random.default_rng(0)\n        self.steps = 0\n        self.agent_score = 0\n        self.opponent_score = 0\n        self.hits = 0\n        self.misses = 0\n        self.rally_length = 0\n        self.episode_reason = \"running\"\n\n        self.agent_y = 0.0\n        self.opponent_y = 0.0\n        self.ball_x = 0.0\n        self.ball_y = 0.0\n        self.ball_vx = 0.0\n        self.ball_vy = 0.0\n\n    def reset(\n        self, *, seed: int | None = None, options: dict[str, Any] | None = None\n    ) -> tuple[np.ndarray, dict[str, Any]]:\n        super().reset(seed=seed)\n        if seed is not None:\n            self._rng = np.random.default_rng(seed)\n        self.steps = 0\n        self.hits = 0\n        self.misses = 0\n        self.rally_length = 0\n        self.episode_reason = \"running\"\n\n        self.agent_y = (self.config.height - self.config.paddle_height) / 2\n        self.opponent_y = self.agent_y\n        self.ball_x = self.config.width / 2\n        self.ball_y = self.config.height / 2\n        self.ball_vx = float(self._rng.choice([-2, 2]))\n        self.ball_vy = float(self._rng.choice([-1, 1]))\n        return self._obs(), self._info()\n\n    def step(self, action: int) -> tuple[np.ndarray, float, bool, bool, dict[str, Any]]:\n        self.steps += 1\n        self._move_agent(int(action))\n        self._move_opponent()\n        reward = 0.0\n\n        self.ball_x += self.ball_vx\n        self.ball_y += self.ball_vy\n\n        if self.ball_y <= 0 or self.ball_y >= self.config.height - self.config.ball_size:\n            self.ball_vy *= -1\n            self.ball_y = np.clip(self.ball_y, 0, self.config.height - self.config.ball_size)\n\n        left_x = 4\n        right_x = self.config.width - 4 - self.config.paddle_width\n\n        if self.ball_vx < 0 and self.ball_x <= left_x + self.config.paddle_width:\n            if self.agent_y <= self.ball_y <= self.agent_y + self.config.paddle_height:\n                self.ball_vx = abs(self.ball_vx)\n                self.hits += 1\n                self.rally_length += 1\n                if self.config.reward_shaping:\n                    reward += 0.01\n            else:\n                reward -= 1.0\n                self.opponent_score += 1\n                self.misses += 1\n                self.episode_reason = \"agent_miss\"\n                return self._obs(), reward, True, False, self._info()\n\n        if self.ball_vx > 0 and self.ball_x + self.config.ball_size >= right_x:\n            if self.opponent_y <= self.ball_y <= self.opponent_y + self.config.paddle_height:\n                self.ball_vx = -abs(self.ball_vx)\n                self.rally_length += 1\n            else:\n                reward += 1.0\n                self.agent_score += 1\n                self.episode_reason = \"opponent_miss\"\n                return self._obs(), reward, True, False, self._info()\n\n        truncated = self.steps >= self.config.max_steps\n        if truncated:\n            self.episode_reason = \"max_steps\"\n        return self._obs(), reward, False, truncated, self._info()\n\n    def render(self) -> np.ndarray:\n        gray = self._obs().squeeze(-1)\n        rgb = np.repeat(gray[..., None], 3, axis=2)\n        return rgb\n\n    def _move_agent(self, action: int) -> None:\n        if action == 0:\n            self.agent_y -= self.config.paddle_speed\n        elif action == 1:\n            self.agent_y += self.config.paddle_speed\n        self.agent_y = float(\n            np.clip(self.agent_y, 0, self.config.height - self.config.paddle_height)\n        )\n\n    def _move_opponent(self) -> None:\n        center = self.opponent_y + self.config.paddle_height / 2\n        target = self.ball_y + self.config.ball_size / 2\n        if target > center:\n            self.opponent_y += self.config.paddle_speed * 0.9\n        else:\n            self.opponent_y -= self.config.paddle_speed * 0.9\n        self.opponent_y = float(\n            np.clip(self.opponent_y, 0, self.config.height - self.config.paddle_height)\n        )\n\n    def _obs(self) -> np.ndarray:\n        frame = np.zeros((self.config.height, self.config.width), dtype=np.uint8)\n        left_x = 4\n        right_x = self.config.width - 4 - self.config.paddle_width\n        ay = int(self.agent_y)\n        oy = int(self.opponent_y)\n        bx = int(self.ball_x)\n        by = int(self.ball_y)\n        frame[ay : ay + self.config.paddle_height, left_x : left_x + self.config.paddle_width] = 255\n        frame[oy : oy + self.config.paddle_height, right_x : right_x + self.config.paddle_width] = (\n            255\n        )\n        frame[by : by + self.config.ball_size, bx : bx + self.config.ball_size] = 255\n        return frame[..., None]\n\n    def _info(self) -> dict[str, Any]:\n        return {\n            \"rally_length\": self.rally_length,\n            \"hits\": self.hits,\n            \"misses\": self.misses,\n            \"agent_score\": self.agent_score,\n            \"opponent_score\": self.opponent_score,\n            \"episode_reason\": self.episode_reason,\n        }\n",
      "base": ""
    },
    "src/envs/registry.py": {
      "additions": 13,
      "deletions": 16,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/envs/registry.py b/src/envs/registry.py\nindex 3e577dd..11c19d3 100644\n--- a/src/envs/registry.py\n+++ b/src/envs/registry.py\n@@ -1,33 +1,30 @@\n-\"\"\"Environment registry stubs.\"\"\"\n+\"\"\"Environment registry.\"\"\"\n \n from __future__ import annotations\n \n+from collections.abc import Callable\n from dataclasses import dataclass, field\n-from typing import Any, Callable, Dict\n+from typing import Any\n \n-EnvFactory = Callable[[], Any]\n+from src.envs.minipong import MiniPongConfig, MiniPongEnv\n+\n+EnvFactory = Callable[..., Any]\n \n \n @dataclass\n class EnvRegistry:\n-    \"\"\"In-memory registry for environment factories.\"\"\"\n-\n-    factories: Dict[str, EnvFactory] = field(default_factory=dict)\n+    factories: dict[str, EnvFactory] = field(default_factory=dict)\n \n     def register(self, name: str, factory: EnvFactory) -> None:\n-        \"\"\"Register a factory by name.\"\"\"\n-\n         self.factories[name] = factory\n \n-    def create(self, name: str) -> Any:\n-        \"\"\"Create an environment instance from the registry.\"\"\"\n-\n+    def create(self, name: str, **kwargs: Any) -> Any:\n         if name not in self.factories:\n             raise KeyError(f\"Unknown environment: {name}\")\n-        return self.factories[name]()\n-\n+        return self.factories[name](**kwargs)\n \n-def register_env(registry: EnvRegistry, name: str, factory: EnvFactory) -> None:\n-    \"\"\"Convenience helper to register an environment.\"\"\"\n \n-    registry.register(name, factory)\n+def default_registry() -> EnvRegistry:\n+    reg = EnvRegistry()\n+    reg.register(\"MiniPong-v0\", lambda **kwargs: MiniPongEnv(config=MiniPongConfig(), **kwargs))\n+    return reg\n",
      "raw": "\"\"\"Environment registry.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Callable\nfrom dataclasses import dataclass, field\nfrom typing import Any\n\nfrom src.envs.minipong import MiniPongConfig, MiniPongEnv\n\nEnvFactory = Callable[..., Any]\n\n\n@dataclass\nclass EnvRegistry:\n    factories: dict[str, EnvFactory] = field(default_factory=dict)\n\n    def register(self, name: str, factory: EnvFactory) -> None:\n        self.factories[name] = factory\n\n    def create(self, name: str, **kwargs: Any) -> Any:\n        if name not in self.factories:\n            raise KeyError(f\"Unknown environment: {name}\")\n        return self.factories[name](**kwargs)\n\n\ndef default_registry() -> EnvRegistry:\n    reg = EnvRegistry()\n    reg.register(\"MiniPong-v0\", lambda **kwargs: MiniPongEnv(config=MiniPongConfig(), **kwargs))\n    return reg\n",
      "base": "\"\"\"Environment registry stubs.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Dict\n\nEnvFactory = Callable[[], Any]\n\n\n@dataclass\nclass EnvRegistry:\n    \"\"\"In-memory registry for environment factories.\"\"\"\n\n    factories: Dict[str, EnvFactory] = field(default_factory=dict)\n\n    def register(self, name: str, factory: EnvFactory) -> None:\n        \"\"\"Register a factory by name.\"\"\"\n\n        self.factories[name] = factory\n\n    def create(self, name: str) -> Any:\n        \"\"\"Create an environment instance from the registry.\"\"\"\n\n        if name not in self.factories:\n            raise KeyError(f\"Unknown environment: {name}\")\n        return self.factories[name]()\n\n\ndef register_env(registry: EnvRegistry, name: str, factory: EnvFactory) -> None:\n    \"\"\"Convenience helper to register an environment.\"\"\"\n\n    registry.register(name, factory)\n"
    },
    "src/envs/wrappers.py": {
      "additions": 31,
      "deletions": 4,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/envs/wrappers.py b/src/envs/wrappers.py\nindex 2f0b1f0..0375345 100644\n--- a/src/envs/wrappers.py\n+++ b/src/envs/wrappers.py\n@@ -1,11 +1,38 @@\n-\"\"\"Environment wrapper stubs.\"\"\"\n+\"\"\"Environment wrappers for frame stacking.\"\"\"\n \n from __future__ import annotations\n \n+import collections\n from typing import Any\n \n+import gymnasium as gym\n+import numpy as np\n \n-def wrap_env(env: Any) -> Any:\n-    \"\"\"Apply standard wrappers to an environment.\"\"\"\n \n-    raise NotImplementedError(\"Environment wrappers are not implemented yet.\")\n+class FrameStackPixels(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n+    def __init__(self, env: gym.Env[np.ndarray, int], n_frames: int = 4) -> None:\n+        super().__init__(env)\n+        self.n_frames = n_frames\n+        self.frames: collections.deque[np.ndarray] = collections.deque(maxlen=n_frames)\n+        h, w, c = env.observation_space.shape\n+        self.observation_space = gym.spaces.Box(0, 255, shape=(h, w, c * n_frames), dtype=np.uint8)\n+\n+    def reset(self, **kwargs: Any) -> tuple[np.ndarray, dict[str, Any]]:\n+        obs, info = self.env.reset(**kwargs)\n+        for _ in range(self.n_frames):\n+            self.frames.append(obs)\n+        return self._get_obs(), info\n+\n+    def step(self, action: int) -> tuple[np.ndarray, float, bool, bool, dict[str, Any]]:\n+        obs, reward, terminated, truncated, info = self.env.step(action)\n+        self.frames.append(obs)\n+        return self._get_obs(), reward, terminated, truncated, info\n+\n+    def _get_obs(self) -> np.ndarray:\n+        return np.concatenate(list(self.frames), axis=2)\n+\n+\n+def wrap_env(env: Any, frame_stack: int = 1) -> Any:\n+    if frame_stack <= 1:\n+        return env\n+    return FrameStackPixels(env, n_frames=frame_stack)\n",
      "raw": "\"\"\"Environment wrappers for frame stacking.\"\"\"\n\nfrom __future__ import annotations\n\nimport collections\nfrom typing import Any\n\nimport gymnasium as gym\nimport numpy as np\n\n\nclass FrameStackPixels(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n    def __init__(self, env: gym.Env[np.ndarray, int], n_frames: int = 4) -> None:\n        super().__init__(env)\n        self.n_frames = n_frames\n        self.frames: collections.deque[np.ndarray] = collections.deque(maxlen=n_frames)\n        h, w, c = env.observation_space.shape\n        self.observation_space = gym.spaces.Box(0, 255, shape=(h, w, c * n_frames), dtype=np.uint8)\n\n    def reset(self, **kwargs: Any) -> tuple[np.ndarray, dict[str, Any]]:\n        obs, info = self.env.reset(**kwargs)\n        for _ in range(self.n_frames):\n            self.frames.append(obs)\n        return self._get_obs(), info\n\n    def step(self, action: int) -> tuple[np.ndarray, float, bool, bool, dict[str, Any]]:\n        obs, reward, terminated, truncated, info = self.env.step(action)\n        self.frames.append(obs)\n        return self._get_obs(), reward, terminated, truncated, info\n\n    def _get_obs(self) -> np.ndarray:\n        return np.concatenate(list(self.frames), axis=2)\n\n\ndef wrap_env(env: Any, frame_stack: int = 1) -> Any:\n    if frame_stack <= 1:\n        return env\n    return FrameStackPixels(env, n_frames=frame_stack)\n",
      "base": "\"\"\"Environment wrapper stubs.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any\n\n\ndef wrap_env(env: Any) -> Any:\n    \"\"\"Apply standard wrappers to an environment.\"\"\"\n\n    raise NotImplementedError(\"Environment wrappers are not implemented yet.\")\n"
    },
    "src/obs/__init__.py": {
      "additions": 1,
      "deletions": 6,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/obs/__init__.py b/src/obs/__init__.py\nindex bd00988..3a769d1 100644\n--- a/src/obs/__init__.py\n+++ b/src/obs/__init__.py\n@@ -1,6 +1 @@\n-\"\"\"Observability and metrics.\"\"\"\n-\n-from .logging import MetricsLogger\n-from .metrics import MetricSummary\n-\n-__all__ = [\"MetricsLogger\", \"MetricSummary\"]\n+\"\"\"Observability package.\"\"\"\n",
      "raw": "\"\"\"Observability package.\"\"\"\n",
      "base": "\"\"\"Observability and metrics.\"\"\"\n\nfrom .logging import MetricsLogger\nfrom .metrics import MetricSummary\n\n__all__ = [\"MetricsLogger\", \"MetricSummary\"]\n"
    },
    "src/obs/logging.py": {
      "additions": 22,
      "deletions": 6,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/obs/logging.py b/src/obs/logging.py\nindex 1d05a70..f081521 100644\n--- a/src/obs/logging.py\n+++ b/src/obs/logging.py\n@@ -1,14 +1,30 @@\n-\"\"\"Logging stubs.\"\"\"\n+\"\"\"Run logging utilities.\"\"\"\n \n from __future__ import annotations\n \n-from typing import Any, Mapping\n+import json\n+from pathlib import Path\n+from typing import Any\n+\n+from torch.utils.tensorboard import SummaryWriter\n \n \n class MetricsLogger:\n-    \"\"\"Placeholder metrics logger.\"\"\"\n+    def __init__(self, run_dir: Path) -> None:\n+        self.run_dir = run_dir\n+        self.run_dir.mkdir(parents=True, exist_ok=True)\n+        self.log_path = run_dir / \"logs.jsonl\"\n+        self.tb_dir = run_dir / \"tensorboard\"\n+        self.tb = SummaryWriter(str(self.tb_dir))\n \n-    def log_metrics(self, metrics: Mapping[str, Any]) -> None:\n-        \"\"\"Log a batch of metrics.\"\"\"\n+    def log_metrics(self, step: int, metrics: dict[str, Any]) -> None:\n+        line = {\"step\": step, **metrics}\n+        with self.log_path.open(\"a\", encoding=\"utf-8\") as f:\n+            f.write(json.dumps(line) + \"\\n\")\n+        for k, v in metrics.items():\n+            if isinstance(v, (int, float)):\n+                self.tb.add_scalar(k, v, step)\n \n-        raise NotImplementedError(\"Metrics logging not implemented yet.\")\n+    def close(self) -> None:\n+        self.tb.flush()\n+        self.tb.close()\n",
      "raw": "\"\"\"Run logging utilities.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom pathlib import Path\nfrom typing import Any\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass MetricsLogger:\n    def __init__(self, run_dir: Path) -> None:\n        self.run_dir = run_dir\n        self.run_dir.mkdir(parents=True, exist_ok=True)\n        self.log_path = run_dir / \"logs.jsonl\"\n        self.tb_dir = run_dir / \"tensorboard\"\n        self.tb = SummaryWriter(str(self.tb_dir))\n\n    def log_metrics(self, step: int, metrics: dict[str, Any]) -> None:\n        line = {\"step\": step, **metrics}\n        with self.log_path.open(\"a\", encoding=\"utf-8\") as f:\n            f.write(json.dumps(line) + \"\\n\")\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                self.tb.add_scalar(k, v, step)\n\n    def close(self) -> None:\n        self.tb.flush()\n        self.tb.close()\n",
      "base": "\"\"\"Logging stubs.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Mapping\n\n\nclass MetricsLogger:\n    \"\"\"Placeholder metrics logger.\"\"\"\n\n    def log_metrics(self, metrics: Mapping[str, Any]) -> None:\n        \"\"\"Log a batch of metrics.\"\"\"\n\n        raise NotImplementedError(\"Metrics logging not implemented yet.\")\n"
    },
    "src/obs/metrics.py": {
      "additions": 1,
      "deletions": 3,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/obs/metrics.py b/src/obs/metrics.py\nindex 8993e27..ce0d6c3 100644\n--- a/src/obs/metrics.py\n+++ b/src/obs/metrics.py\n@@ -1,4 +1,4 @@\n-\"\"\"Metrics stubs.\"\"\"\n+\"\"\"Metrics helpers.\"\"\"\n \n from __future__ import annotations\n \n@@ -7,7 +7,5 @@ from dataclasses import dataclass\n \n @dataclass(frozen=True)\n class MetricSummary:\n-    \"\"\"Summary for a metric stream.\"\"\"\n-\n     name: str\n     value: float\n",
      "raw": "\"\"\"Metrics helpers.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\n\n@dataclass(frozen=True)\nclass MetricSummary:\n    name: str\n    value: float\n",
      "base": "\"\"\"Metrics stubs.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\n\n@dataclass(frozen=True)\nclass MetricSummary:\n    \"\"\"Summary for a metric stream.\"\"\"\n\n    name: str\n    value: float\n"
    },
    "src/rl/__init__.py": {
      "additions": 1,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/rl/__init__.py b/src/rl/__init__.py\nindex b8ed1a7..a71d204 100644\n--- a/src/rl/__init__.py\n+++ b/src/rl/__init__.py\n@@ -1,5 +1 @@\n-\"\"\"RL primitives and utilities.\"\"\"\n-\n-from .replay import ReplayBuffer, Transition\n-\n-__all__ = [\"ReplayBuffer\", \"Transition\"]\n+\"\"\"RL core package.\"\"\"\n",
      "raw": "\"\"\"RL core package.\"\"\"\n",
      "base": "\"\"\"RL primitives and utilities.\"\"\"\n\nfrom .replay import ReplayBuffer, Transition\n\n__all__ = [\"ReplayBuffer\", \"Transition\"]\n"
    },
    "src/rl/networks.py": {
      "additions": 28,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/rl/networks.py b/src/rl/networks.py\nindex afd2c0b..a01b6d2 100644\n--- a/src/rl/networks.py\n+++ b/src/rl/networks.py\n@@ -1,11 +1,34 @@\n-\"\"\"Network stubs.\"\"\"\n+\"\"\"Q-network definitions.\"\"\"\n \n from __future__ import annotations\n \n-from typing import Any, Sequence\n+from collections.abc import Sequence\n \n+import torch\n+from torch import nn\n \n-def create_q_network(obs_shape: Sequence[int], num_actions: int) -> Any:\n-    \"\"\"Create a Q-network for the given observation shape and action space.\"\"\"\n \n-    raise NotImplementedError(\"Network creation not implemented yet.\")\n+class ConvQNetwork(nn.Module):\n+    def __init__(self, obs_shape: Sequence[int], num_actions: int) -> None:\n+        super().__init__()\n+        c = obs_shape[2]\n+        self.features = nn.Sequential(\n+            nn.Conv2d(c, 16, kernel_size=8, stride=4),\n+            nn.ReLU(),\n+            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n+            nn.ReLU(),\n+            nn.Flatten(),\n+        )\n+        with torch.no_grad():\n+            dummy = torch.zeros(1, c, obs_shape[0], obs_shape[1])\n+            n_flat = self.features(dummy).shape[1]\n+        self.head = nn.Sequential(nn.Linear(n_flat, 128), nn.ReLU(), nn.Linear(128, num_actions))\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        x = x / 255.0\n+        x = x.permute(0, 3, 1, 2)\n+        return self.head(self.features(x))\n+\n+\n+def create_q_network(obs_shape: Sequence[int], num_actions: int) -> nn.Module:\n+    return ConvQNetwork(obs_shape, num_actions)\n",
      "raw": "\"\"\"Q-network definitions.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Sequence\n\nimport torch\nfrom torch import nn\n\n\nclass ConvQNetwork(nn.Module):\n    def __init__(self, obs_shape: Sequence[int], num_actions: int) -> None:\n        super().__init__()\n        c = obs_shape[2]\n        self.features = nn.Sequential(\n            nn.Conv2d(c, 16, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n        with torch.no_grad():\n            dummy = torch.zeros(1, c, obs_shape[0], obs_shape[1])\n            n_flat = self.features(dummy).shape[1]\n        self.head = nn.Sequential(nn.Linear(n_flat, 128), nn.ReLU(), nn.Linear(128, num_actions))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x / 255.0\n        x = x.permute(0, 3, 1, 2)\n        return self.head(self.features(x))\n\n\ndef create_q_network(obs_shape: Sequence[int], num_actions: int) -> nn.Module:\n    return ConvQNetwork(obs_shape, num_actions)\n",
      "base": "\"\"\"Network stubs.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Sequence\n\n\ndef create_q_network(obs_shape: Sequence[int], num_actions: int) -> Any:\n    \"\"\"Create a Q-network for the given observation shape and action space.\"\"\"\n\n    raise NotImplementedError(\"Network creation not implemented yet.\")\n"
    },
    "src/rl/replay.py": {
      "additions": 19,
      "deletions": 17,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/rl/replay.py b/src/rl/replay.py\nindex e659485..f511e52 100644\n--- a/src/rl/replay.py\n+++ b/src/rl/replay.py\n@@ -1,35 +1,37 @@\n-\"\"\"Replay buffer stubs.\"\"\"\n+\"\"\"Replay buffer.\"\"\"\n \n from __future__ import annotations\n \n from dataclasses import dataclass\n-from typing import Any, Iterable, List\n+\n+import numpy as np\n \n \n @dataclass(frozen=True)\n class Transition:\n-    \"\"\"Single transition in the replay buffer.\"\"\"\n-\n-    obs: Any\n+    obs: np.ndarray\n     action: int\n     reward: float\n-    next_obs: Any\n+    next_obs: np.ndarray\n     done: bool\n \n \n class ReplayBuffer:\n-    \"\"\"Placeholder replay buffer.\"\"\"\n-\n     def __init__(self, capacity: int) -> None:\n         self.capacity = capacity\n-        self._storage: List[Transition] = []\n+        self._storage: list[Transition] = []\n+        self._index = 0\n \n-    def add(self, transition: Transition) -> None:\n-        \"\"\"Add a transition to the buffer.\"\"\"\n+    def __len__(self) -> int:\n+        return len(self._storage)\n \n-        raise NotImplementedError(\"Replay buffer add not implemented yet.\")\n-\n-    def sample(self, batch_size: int) -> Iterable[Transition]:\n-        \"\"\"Sample a batch of transitions.\"\"\"\n-\n-        raise NotImplementedError(\"Replay buffer sampling not implemented yet.\")\n+    def add(self, transition: Transition) -> None:\n+        if len(self._storage) < self.capacity:\n+            self._storage.append(transition)\n+        else:\n+            self._storage[self._index] = transition\n+        self._index = (self._index + 1) % self.capacity\n+\n+    def sample(self, batch_size: int) -> list[Transition]:\n+        idx = np.random.randint(0, len(self._storage), size=batch_size)\n+        return [self._storage[i] for i in idx]\n",
      "raw": "\"\"\"Replay buffer.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nimport numpy as np\n\n\n@dataclass(frozen=True)\nclass Transition:\n    obs: np.ndarray\n    action: int\n    reward: float\n    next_obs: np.ndarray\n    done: bool\n\n\nclass ReplayBuffer:\n    def __init__(self, capacity: int) -> None:\n        self.capacity = capacity\n        self._storage: list[Transition] = []\n        self._index = 0\n\n    def __len__(self) -> int:\n        return len(self._storage)\n\n    def add(self, transition: Transition) -> None:\n        if len(self._storage) < self.capacity:\n            self._storage.append(transition)\n        else:\n            self._storage[self._index] = transition\n        self._index = (self._index + 1) % self.capacity\n\n    def sample(self, batch_size: int) -> list[Transition]:\n        idx = np.random.randint(0, len(self._storage), size=batch_size)\n        return [self._storage[i] for i in idx]\n",
      "base": "\"\"\"Replay buffer stubs.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any, Iterable, List\n\n\n@dataclass(frozen=True)\nclass Transition:\n    \"\"\"Single transition in the replay buffer.\"\"\"\n\n    obs: Any\n    action: int\n    reward: float\n    next_obs: Any\n    done: bool\n\n\nclass ReplayBuffer:\n    \"\"\"Placeholder replay buffer.\"\"\"\n\n    def __init__(self, capacity: int) -> None:\n        self.capacity = capacity\n        self._storage: List[Transition] = []\n\n    def add(self, transition: Transition) -> None:\n        \"\"\"Add a transition to the buffer.\"\"\"\n\n        raise NotImplementedError(\"Replay buffer add not implemented yet.\")\n\n    def sample(self, batch_size: int) -> Iterable[Transition]:\n        \"\"\"Sample a batch of transitions.\"\"\"\n\n        raise NotImplementedError(\"Replay buffer sampling not implemented yet.\")\n"
    },
    "src/rl/schedules.py": {
      "additions": 7,
      "deletions": 7,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/rl/schedules.py b/src/rl/schedules.py\nindex a251d4e..cb5bbe5 100644\n--- a/src/rl/schedules.py\n+++ b/src/rl/schedules.py\n@@ -1,12 +1,12 @@\n-\"\"\"Schedule utilities.\"\"\"\n+\"\"\"Schedule functions.\"\"\"\n \n from __future__ import annotations\n \n \n def linear_schedule(step: int, start: float, end: float, duration: int) -> float:\n-    \"\"\"Linearly interpolate between start and end over duration steps.\"\"\"\n-\n-    if duration <= 0:\n-        raise ValueError(\"duration must be positive\")\n-    progress = min(max(step, 0), duration) / duration\n-    return start + (end - start) * progress\n+    if step <= 0:\n+        return start\n+    if step >= duration:\n+        return end\n+    frac = step / duration\n+    return start + frac * (end - start)\n",
      "raw": "\"\"\"Schedule functions.\"\"\"\n\nfrom __future__ import annotations\n\n\ndef linear_schedule(step: int, start: float, end: float, duration: int) -> float:\n    if step <= 0:\n        return start\n    if step >= duration:\n        return end\n    frac = step / duration\n    return start + frac * (end - start)\n",
      "base": "\"\"\"Schedule utilities.\"\"\"\n\nfrom __future__ import annotations\n\n\ndef linear_schedule(step: int, start: float, end: float, duration: int) -> float:\n    \"\"\"Linearly interpolate between start and end over duration steps.\"\"\"\n\n    if duration <= 0:\n        raise ValueError(\"duration must be positive\")\n    progress = min(max(step, 0), duration) / duration\n    return start + (end - start) * progress\n"
    },
    "src/train/__init__.py": {
      "additions": 1,
      "deletions": 1,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/train/__init__.py b/src/train/__init__.py\nindex 3980758..a4d0dfd 100644\n--- a/src/train/__init__.py\n+++ b/src/train/__init__.py\n@@ -1 +1 @@\n-\"\"\"Training entrypoints.\"\"\"\n+\"\"\"Training package.\"\"\"\n",
      "raw": "\"\"\"Training package.\"\"\"\n",
      "base": "\"\"\"Training entrypoints.\"\"\"\n"
    },
    "src/train/evaluate.py": {
      "additions": 78,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/train/evaluate.py b/src/train/evaluate.py\nindex 50e5e5f..6f48a9f 100644\n--- a/src/train/evaluate.py\n+++ b/src/train/evaluate.py\n@@ -1,11 +1,84 @@\n-\"\"\"Evaluation stub.\"\"\"\n+\"\"\"Evaluation helpers.\"\"\"\n \n from __future__ import annotations\n \n-from typing import Any, Mapping\n+import argparse\n+import json\n+from pathlib import Path\n+from typing import Any\n \n+import numpy as np\n+import torch\n \n-def evaluate(config: Mapping[str, Any]) -> None:\n-    \"\"\"Evaluate a trained agent using the provided configuration.\"\"\"\n+from src.agents.dqn_agent import DQNAgent, DQNConfig\n+from src.envs.minipong import MiniPongEnv\n+from src.envs.wrappers import wrap_env\n+from src.rl.replay import ReplayBuffer\n \n-    raise NotImplementedError(\"Evaluation not implemented yet.\")\n+\n+def evaluate_policy(\n+    checkpoint: Path | None,\n+    episodes: int,\n+    seeds: list[int],\n+    frame_stack: int,\n+    max_steps: int,\n+) -> dict[str, Any]:\n+    all_returns: list[float] = []\n+    all_hits: list[float] = []\n+    for seed in seeds:\n+        env = wrap_env(MiniPongEnv(), frame_stack=frame_stack)\n+        obs, _ = env.reset(seed=seed)\n+        replay = ReplayBuffer(capacity=10)\n+        agent = DQNAgent(obs.shape, env.action_space.n, replay, DQNConfig())\n+        if checkpoint is not None:\n+            data = torch.load(checkpoint, map_location=agent.device)\n+            agent.online.load_state_dict(data[\"model\"])\n+            agent.target.load_state_dict(data[\"model\"])\n+        for ep in range(episodes):\n+            obs, _ = env.reset(seed=seed + ep)\n+            done = False\n+            truncated = False\n+            ep_ret = 0.0\n+            steps = 0\n+            info: dict[str, Any] = {}\n+            while not done and not truncated and steps < max_steps:\n+                action = agent.act(obs, epsilon=0.0) if checkpoint else env.action_space.sample()\n+                obs, reward, done, truncated, info = env.step(action)\n+                ep_ret += reward\n+                steps += 1\n+            all_returns.append(ep_ret)\n+            all_hits.append(float(info.get(\"hits\", 0)))\n+    return {\n+        \"mean_return\": float(np.mean(all_returns)) if all_returns else 0.0,\n+        \"std_return\": float(np.std(all_returns)) if all_returns else 0.0,\n+        \"mean_hits\": float(np.mean(all_hits)) if all_hits else 0.0,\n+        \"episodes\": len(all_returns),\n+        \"seeds\": seeds,\n+    }\n+\n+\n+def main() -> None:\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--run-id\", required=True)\n+    parser.add_argument(\"--checkpoint\", default=\"\")\n+    parser.add_argument(\"--episodes\", type=int, default=5)\n+    parser.add_argument(\"--seeds\", nargs=\"+\", type=int, default=[0, 1])\n+    parser.add_argument(\"--frame-stack\", type=int, default=4)\n+    parser.add_argument(\"--max-steps\", type=int, default=1000)\n+    args = parser.parse_args()\n+\n+    run_dir = Path(\"artifacts\") / args.run_id\n+    run_dir.mkdir(parents=True, exist_ok=True)\n+    eval_dir = run_dir / \"eval\"\n+    eval_dir.mkdir(exist_ok=True)\n+    ckpt = Path(args.checkpoint) if args.checkpoint else None\n+    metrics = evaluate_policy(ckpt, args.episodes, args.seeds, args.frame_stack, args.max_steps)\n+    out_name = (\n+        \"metrics_random.json\" if ckpt is None else f\"metrics_{Path(args.checkpoint).stem}.json\"\n+    )\n+    (eval_dir / out_name).write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n+    print(json.dumps(metrics, indent=2))\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
      "raw": "\"\"\"Evaluation helpers.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nimport torch\n\nfrom src.agents.dqn_agent import DQNAgent, DQNConfig\nfrom src.envs.minipong import MiniPongEnv\nfrom src.envs.wrappers import wrap_env\nfrom src.rl.replay import ReplayBuffer\n\n\ndef evaluate_policy(\n    checkpoint: Path | None,\n    episodes: int,\n    seeds: list[int],\n    frame_stack: int,\n    max_steps: int,\n) -> dict[str, Any]:\n    all_returns: list[float] = []\n    all_hits: list[float] = []\n    for seed in seeds:\n        env = wrap_env(MiniPongEnv(), frame_stack=frame_stack)\n        obs, _ = env.reset(seed=seed)\n        replay = ReplayBuffer(capacity=10)\n        agent = DQNAgent(obs.shape, env.action_space.n, replay, DQNConfig())\n        if checkpoint is not None:\n            data = torch.load(checkpoint, map_location=agent.device)\n            agent.online.load_state_dict(data[\"model\"])\n            agent.target.load_state_dict(data[\"model\"])\n        for ep in range(episodes):\n            obs, _ = env.reset(seed=seed + ep)\n            done = False\n            truncated = False\n            ep_ret = 0.0\n            steps = 0\n            info: dict[str, Any] = {}\n            while not done and not truncated and steps < max_steps:\n                action = agent.act(obs, epsilon=0.0) if checkpoint else env.action_space.sample()\n                obs, reward, done, truncated, info = env.step(action)\n                ep_ret += reward\n                steps += 1\n            all_returns.append(ep_ret)\n            all_hits.append(float(info.get(\"hits\", 0)))\n    return {\n        \"mean_return\": float(np.mean(all_returns)) if all_returns else 0.0,\n        \"std_return\": float(np.std(all_returns)) if all_returns else 0.0,\n        \"mean_hits\": float(np.mean(all_hits)) if all_hits else 0.0,\n        \"episodes\": len(all_returns),\n        \"seeds\": seeds,\n    }\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--run-id\", required=True)\n    parser.add_argument(\"--checkpoint\", default=\"\")\n    parser.add_argument(\"--episodes\", type=int, default=5)\n    parser.add_argument(\"--seeds\", nargs=\"+\", type=int, default=[0, 1])\n    parser.add_argument(\"--frame-stack\", type=int, default=4)\n    parser.add_argument(\"--max-steps\", type=int, default=1000)\n    args = parser.parse_args()\n\n    run_dir = Path(\"artifacts\") / args.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    eval_dir = run_dir / \"eval\"\n    eval_dir.mkdir(exist_ok=True)\n    ckpt = Path(args.checkpoint) if args.checkpoint else None\n    metrics = evaluate_policy(ckpt, args.episodes, args.seeds, args.frame_stack, args.max_steps)\n    out_name = (\n        \"metrics_random.json\" if ckpt is None else f\"metrics_{Path(args.checkpoint).stem}.json\"\n    )\n    (eval_dir / out_name).write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n    print(json.dumps(metrics, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "base": "\"\"\"Evaluation stub.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Mapping\n\n\ndef evaluate(config: Mapping[str, Any]) -> None:\n    \"\"\"Evaluate a trained agent using the provided configuration.\"\"\"\n\n    raise NotImplementedError(\"Evaluation not implemented yet.\")\n"
    },
    "src/train/make_montage.py": {
      "additions": 34,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/src/train/make_montage.py b/src/train/make_montage.py\nnew file mode 100644\nindex 0000000..9aaf8fd\n--- /dev/null\n+++ b/src/train/make_montage.py\n@@ -0,0 +1,34 @@\n+\"\"\"Make demo index for videos.\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+from pathlib import Path\n+\n+\n+def make_index(run_id: str) -> Path:\n+    run_dir = Path(\"artifacts\") / run_id\n+    video_dir = run_dir / \"videos\"\n+    demo_dir = run_dir / \"demo\"\n+    demo_dir.mkdir(parents=True, exist_ok=True)\n+    videos = sorted(video_dir.glob(\"*.mp4\"))\n+    html = [\"<html><body><h1>MiniPong Progression</h1>\"]\n+    for v in videos:\n+        rel = f\"../videos/{v.name}\"\n+        html.append(f\"<h3>{v.name}</h3><video controls width='320' src='{rel}'></video>\")\n+    html.append(\"</body></html>\")\n+    out = demo_dir / \"index.html\"\n+    out.write_text(\"\\n\".join(html), encoding=\"utf-8\")\n+    return out\n+\n+\n+def main() -> None:\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--run_id\", required=True)\n+    args = parser.parse_args()\n+    out = make_index(args.run_id)\n+    print(out)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
      "raw": "\"\"\"Make demo index for videos.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nfrom pathlib import Path\n\n\ndef make_index(run_id: str) -> Path:\n    run_dir = Path(\"artifacts\") / run_id\n    video_dir = run_dir / \"videos\"\n    demo_dir = run_dir / \"demo\"\n    demo_dir.mkdir(parents=True, exist_ok=True)\n    videos = sorted(video_dir.glob(\"*.mp4\"))\n    html = [\"<html><body><h1>MiniPong Progression</h1>\"]\n    for v in videos:\n        rel = f\"../videos/{v.name}\"\n        html.append(f\"<h3>{v.name}</h3><video controls width='320' src='{rel}'></video>\")\n    html.append(\"</body></html>\")\n    out = demo_dir / \"index.html\"\n    out.write_text(\"\\n\".join(html), encoding=\"utf-8\")\n    return out\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--run_id\", required=True)\n    args = parser.parse_args()\n    out = make_index(args.run_id)\n    print(out)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "base": ""
    },
    "src/train/record_video.py": {
      "additions": 46,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/train/record_video.py b/src/train/record_video.py\nindex 7321fbf..8339ad2 100644\n--- a/src/train/record_video.py\n+++ b/src/train/record_video.py\n@@ -1,11 +1,52 @@\n-\"\"\"Video recording stub.\"\"\"\n+\"\"\"Record evaluation video.\"\"\"\n \n from __future__ import annotations\n \n-from typing import Any, Mapping\n+import argparse\n+from pathlib import Path\n \n+import imageio.v2 as imageio\n+import torch\n \n-def record_video(config: Mapping[str, Any]) -> None:\n-    \"\"\"Record gameplay video using the provided configuration.\"\"\"\n+from src.agents.dqn_agent import DQNAgent, DQNConfig\n+from src.envs.minipong import MiniPongEnv\n+from src.envs.wrappers import wrap_env\n+from src.rl.replay import ReplayBuffer\n \n-    raise NotImplementedError(\"Video recording not implemented yet.\")\n+\n+def record_video(\n+    checkpoint: Path | None, output_path: Path, seed: int = 0, max_steps: int = 1000\n+) -> None:\n+    env = wrap_env(MiniPongEnv(render_mode=\"rgb_array\"), frame_stack=4)\n+    obs, _ = env.reset(seed=seed)\n+    replay = ReplayBuffer(capacity=8)\n+    agent = DQNAgent(obs.shape, env.action_space.n, replay, DQNConfig())\n+    if checkpoint is not None:\n+        data = torch.load(checkpoint, map_location=agent.device)\n+        agent.online.load_state_dict(data[\"model\"])\n+    frames = []\n+    done = False\n+    trunc = False\n+    steps = 0\n+    while not done and not trunc and steps < max_steps:\n+        action = agent.act(obs, epsilon=0.0) if checkpoint else env.action_space.sample()\n+        obs, _, done, trunc, _ = env.step(action)\n+        frame = env.unwrapped.render()\n+        frames.append(frame)\n+        steps += 1\n+    output_path.parent.mkdir(parents=True, exist_ok=True)\n+    imageio.mimsave(output_path, frames, fps=30)\n+\n+\n+def main() -> None:\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--checkpoint\", default=\"\")\n+    parser.add_argument(\"--output\", required=True)\n+    parser.add_argument(\"--seed\", type=int, default=0)\n+    args = parser.parse_args()\n+    ckpt = Path(args.checkpoint) if args.checkpoint else None\n+    record_video(ckpt, Path(args.output), seed=args.seed)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
      "raw": "\"\"\"Record evaluation video.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nfrom pathlib import Path\n\nimport imageio.v2 as imageio\nimport torch\n\nfrom src.agents.dqn_agent import DQNAgent, DQNConfig\nfrom src.envs.minipong import MiniPongEnv\nfrom src.envs.wrappers import wrap_env\nfrom src.rl.replay import ReplayBuffer\n\n\ndef record_video(\n    checkpoint: Path | None, output_path: Path, seed: int = 0, max_steps: int = 1000\n) -> None:\n    env = wrap_env(MiniPongEnv(render_mode=\"rgb_array\"), frame_stack=4)\n    obs, _ = env.reset(seed=seed)\n    replay = ReplayBuffer(capacity=8)\n    agent = DQNAgent(obs.shape, env.action_space.n, replay, DQNConfig())\n    if checkpoint is not None:\n        data = torch.load(checkpoint, map_location=agent.device)\n        agent.online.load_state_dict(data[\"model\"])\n    frames = []\n    done = False\n    trunc = False\n    steps = 0\n    while not done and not trunc and steps < max_steps:\n        action = agent.act(obs, epsilon=0.0) if checkpoint else env.action_space.sample()\n        obs, _, done, trunc, _ = env.step(action)\n        frame = env.unwrapped.render()\n        frames.append(frame)\n        steps += 1\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    imageio.mimsave(output_path, frames, fps=30)\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--checkpoint\", default=\"\")\n    parser.add_argument(\"--output\", required=True)\n    parser.add_argument(\"--seed\", type=int, default=0)\n    args = parser.parse_args()\n    ckpt = Path(args.checkpoint) if args.checkpoint else None\n    record_video(ckpt, Path(args.output), seed=args.seed)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "base": "\"\"\"Video recording stub.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Mapping\n\n\ndef record_video(config: Mapping[str, Any]) -> None:\n    \"\"\"Record gameplay video using the provided configuration.\"\"\"\n\n    raise NotImplementedError(\"Video recording not implemented yet.\")\n"
    },
    "src/train/train_dqn.py": {
      "additions": 116,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/train/train_dqn.py b/src/train/train_dqn.py\nindex edbf70d..ff5390b 100644\n--- a/src/train/train_dqn.py\n+++ b/src/train/train_dqn.py\n@@ -1,11 +1,122 @@\n-\"\"\"Training loop stub for DQN.\"\"\"\n+\"\"\"Train DQN on MiniPong.\"\"\"\n \n from __future__ import annotations\n \n-from typing import Any, Mapping\n+import argparse\n+import json\n+from pathlib import Path\n \n+import yaml\n \n-def train(config: Mapping[str, Any]) -> None:\n-    \"\"\"Train a DQN agent using the provided configuration.\"\"\"\n+from src.agents.dqn_agent import DQNAgent, DQNConfig\n+from src.envs.minipong import MiniPongEnv\n+from src.envs.wrappers import wrap_env\n+from src.obs.logging import MetricsLogger\n+from src.rl.replay import ReplayBuffer, Transition\n+from src.rl.schedules import linear_schedule\n+from src.train.evaluate import evaluate_policy\n \n-    raise NotImplementedError(\"Training loop not implemented yet.\")\n+\n+def train(config: dict) -> str:\n+    run_id = config.get(\"run_id\", \"local_run\")\n+    run_dir = Path(\"artifacts\") / run_id\n+    (run_dir / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n+    (run_dir / \"eval\").mkdir(exist_ok=True)\n+    (run_dir / \"videos\").mkdir(exist_ok=True)\n+\n+    env = wrap_env(MiniPongEnv(), frame_stack=int(config[\"frame_stack\"]))\n+    obs, _ = env.reset(seed=int(config[\"seed\"]))\n+    replay = ReplayBuffer(capacity=int(config[\"replay_capacity\"]))\n+    agent = DQNAgent(\n+        obs.shape,\n+        env.action_space.n,\n+        replay,\n+        DQNConfig(\n+            lr=float(config[\"lr\"]),\n+            gamma=float(config[\"gamma\"]),\n+            batch_size=int(config[\"batch_size\"]),\n+        ),\n+    )\n+    logger = MetricsLogger(run_dir)\n+\n+    episode_return = 0.0\n+    total_steps = int(config[\"total_steps\"])\n+    for step in range(1, total_steps + 1):\n+        eps = linear_schedule(\n+            step,\n+            float(config[\"epsilon_start\"]),\n+            float(config[\"epsilon_end\"]),\n+            int(config[\"epsilon_decay_steps\"]),\n+        )\n+        action = agent.act(obs, epsilon=eps)\n+        next_obs, reward, terminated, truncated, info = env.step(action)\n+        done = terminated or truncated\n+        agent.observe(\n+            Transition(obs=obs, action=action, reward=reward, next_obs=next_obs, done=done)\n+        )\n+        episode_return += reward\n+        obs = next_obs\n+\n+        loss = None\n+        if step > int(config[\"replay_warmup_steps\"]) and len(replay) >= int(config[\"batch_size\"]):\n+            loss = agent.update()\n+        if step % int(config[\"target_update_period\"]) == 0:\n+            agent.sync_target()\n+\n+        if done:\n+            logger.log_metrics(\n+                step,\n+                {\n+                    \"train/episode_return\": episode_return,\n+                    \"train/hits\": info.get(\"hits\", 0),\n+                    \"train/epsilon\": eps,\n+                },\n+            )\n+            episode_return = 0.0\n+            obs, _ = env.reset(seed=int(config[\"seed\"]) + step)\n+\n+        if loss is not None and step % 10 == 0:\n+            logger.log_metrics(step, {\"train/loss\": loss, \"train/epsilon\": eps})\n+\n+        if step % int(config[\"eval_every_steps\"]) == 0:\n+            ckpt_path = run_dir / \"checkpoints\" / f\"step_{step}.pt\"\n+            import torch\n+\n+            torch.save({\"model\": agent.online.state_dict(), \"step\": step}, ckpt_path)\n+            metrics = evaluate_policy(\n+                ckpt_path,\n+                int(config[\"eval_episodes\"]),\n+                list(config[\"eval_seeds\"]),\n+                int(config[\"frame_stack\"]),\n+                int(config[\"max_episode_steps\"]),\n+            )\n+            (run_dir / \"eval\" / f\"metrics_step_{step}.json\").write_text(\n+                json.dumps(metrics, indent=2), encoding=\"utf-8\"\n+            )\n+            logger.log_metrics(\n+                step,\n+                {\n+                    \"eval/mean_return\": metrics[\"mean_return\"],\n+                    \"eval/mean_hits\": metrics[\"mean_hits\"],\n+                },\n+            )\n+\n+    logger.close()\n+    return run_id\n+\n+\n+def main() -> None:\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--config\", default=\"configs/dqn_minipong.yaml\")\n+    parser.add_argument(\"--run-id\", default=\"\")\n+    args = parser.parse_args()\n+\n+    config = yaml.safe_load(Path(args.config).read_text(encoding=\"utf-8\"))\n+    if args.run_id:\n+        config[\"run_id\"] = args.run_id\n+    run_id = train(config)\n+    print(run_id)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
      "raw": "\"\"\"Train DQN on MiniPong.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nfrom pathlib import Path\n\nimport yaml\n\nfrom src.agents.dqn_agent import DQNAgent, DQNConfig\nfrom src.envs.minipong import MiniPongEnv\nfrom src.envs.wrappers import wrap_env\nfrom src.obs.logging import MetricsLogger\nfrom src.rl.replay import ReplayBuffer, Transition\nfrom src.rl.schedules import linear_schedule\nfrom src.train.evaluate import evaluate_policy\n\n\ndef train(config: dict) -> str:\n    run_id = config.get(\"run_id\", \"local_run\")\n    run_dir = Path(\"artifacts\") / run_id\n    (run_dir / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n    (run_dir / \"eval\").mkdir(exist_ok=True)\n    (run_dir / \"videos\").mkdir(exist_ok=True)\n\n    env = wrap_env(MiniPongEnv(), frame_stack=int(config[\"frame_stack\"]))\n    obs, _ = env.reset(seed=int(config[\"seed\"]))\n    replay = ReplayBuffer(capacity=int(config[\"replay_capacity\"]))\n    agent = DQNAgent(\n        obs.shape,\n        env.action_space.n,\n        replay,\n        DQNConfig(\n            lr=float(config[\"lr\"]),\n            gamma=float(config[\"gamma\"]),\n            batch_size=int(config[\"batch_size\"]),\n        ),\n    )\n    logger = MetricsLogger(run_dir)\n\n    episode_return = 0.0\n    total_steps = int(config[\"total_steps\"])\n    for step in range(1, total_steps + 1):\n        eps = linear_schedule(\n            step,\n            float(config[\"epsilon_start\"]),\n            float(config[\"epsilon_end\"]),\n            int(config[\"epsilon_decay_steps\"]),\n        )\n        action = agent.act(obs, epsilon=eps)\n        next_obs, reward, terminated, truncated, info = env.step(action)\n        done = terminated or truncated\n        agent.observe(\n            Transition(obs=obs, action=action, reward=reward, next_obs=next_obs, done=done)\n        )\n        episode_return += reward\n        obs = next_obs\n\n        loss = None\n        if step > int(config[\"replay_warmup_steps\"]) and len(replay) >= int(config[\"batch_size\"]):\n            loss = agent.update()\n        if step % int(config[\"target_update_period\"]) == 0:\n            agent.sync_target()\n\n        if done:\n            logger.log_metrics(\n                step,\n                {\n                    \"train/episode_return\": episode_return,\n                    \"train/hits\": info.get(\"hits\", 0),\n                    \"train/epsilon\": eps,\n                },\n            )\n            episode_return = 0.0\n            obs, _ = env.reset(seed=int(config[\"seed\"]) + step)\n\n        if loss is not None and step % 10 == 0:\n            logger.log_metrics(step, {\"train/loss\": loss, \"train/epsilon\": eps})\n\n        if step % int(config[\"eval_every_steps\"]) == 0:\n            ckpt_path = run_dir / \"checkpoints\" / f\"step_{step}.pt\"\n            import torch\n\n            torch.save({\"model\": agent.online.state_dict(), \"step\": step}, ckpt_path)\n            metrics = evaluate_policy(\n                ckpt_path,\n                int(config[\"eval_episodes\"]),\n                list(config[\"eval_seeds\"]),\n                int(config[\"frame_stack\"]),\n                int(config[\"max_episode_steps\"]),\n            )\n            (run_dir / \"eval\" / f\"metrics_step_{step}.json\").write_text(\n                json.dumps(metrics, indent=2), encoding=\"utf-8\"\n            )\n            logger.log_metrics(\n                step,\n                {\n                    \"eval/mean_return\": metrics[\"mean_return\"],\n                    \"eval/mean_hits\": metrics[\"mean_hits\"],\n                },\n            )\n\n    logger.close()\n    return run_id\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", default=\"configs/dqn_minipong.yaml\")\n    parser.add_argument(\"--run-id\", default=\"\")\n    args = parser.parse_args()\n\n    config = yaml.safe_load(Path(args.config).read_text(encoding=\"utf-8\"))\n    if args.run_id:\n        config[\"run_id\"] = args.run_id\n    run_id = train(config)\n    print(run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "base": "\"\"\"Training loop stub for DQN.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Mapping\n\n\ndef train(config: Mapping[str, Any]) -> None:\n    \"\"\"Train a DQN agent using the provided configuration.\"\"\"\n\n    raise NotImplementedError(\"Training loop not implemented yet.\")\n"
    },
    "src/train/verify_learning.py": {
      "additions": 48,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/src/train/verify_learning.py b/src/train/verify_learning.py\nnew file mode 100644\nindex 0000000..978ebd8\n--- /dev/null\n+++ b/src/train/verify_learning.py\n@@ -0,0 +1,48 @@\n+\"\"\"Learning verification gate.\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+from pathlib import Path\n+\n+from src.train.evaluate import evaluate_policy\n+\n+\n+def verify_learning(run_id: str, min_return_gain: float, min_hits_gain: float) -> int:\n+    run_dir = Path(\"artifacts\") / run_id\n+    eval_dir = run_dir / \"eval\"\n+    checkpoints = sorted((run_dir / \"checkpoints\").glob(\"step_*.pt\"))\n+    if not checkpoints:\n+        print(\"No checkpoints found\")\n+        return 2\n+    last = checkpoints[-1]\n+    baseline = evaluate_policy(None, episodes=4, seeds=[11, 22], frame_stack=4, max_steps=600)\n+    trained = evaluate_policy(last, episodes=4, seeds=[11, 22], frame_stack=4, max_steps=600)\n+    pass_return = trained[\"mean_return\"] - baseline[\"mean_return\"] >= min_return_gain\n+    pass_hits = trained[\"mean_hits\"] - baseline[\"mean_hits\"] >= min_hits_gain\n+    summary = {\n+        \"baseline\": baseline,\n+        \"trained\": trained,\n+        \"pass_return\": pass_return,\n+        \"pass_hits\": pass_hits,\n+        \"passed\": pass_return and pass_hits,\n+        \"checkpoint\": str(last),\n+    }\n+    (eval_dir / \"learning_verification.json\").write_text(\n+        json.dumps(summary, indent=2), encoding=\"utf-8\"\n+    )\n+    return 0 if summary[\"passed\"] else 1\n+\n+\n+def main() -> None:\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--run-id\", required=True)\n+    parser.add_argument(\"--min-return-gain\", type=float, default=0.05)\n+    parser.add_argument(\"--min-hits-gain\", type=float, default=1.0)\n+    args = parser.parse_args()\n+    raise SystemExit(verify_learning(args.run_id, args.min_return_gain, args.min_hits_gain))\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n",
      "raw": "\"\"\"Learning verification gate.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nfrom pathlib import Path\n\nfrom src.train.evaluate import evaluate_policy\n\n\ndef verify_learning(run_id: str, min_return_gain: float, min_hits_gain: float) -> int:\n    run_dir = Path(\"artifacts\") / run_id\n    eval_dir = run_dir / \"eval\"\n    checkpoints = sorted((run_dir / \"checkpoints\").glob(\"step_*.pt\"))\n    if not checkpoints:\n        print(\"No checkpoints found\")\n        return 2\n    last = checkpoints[-1]\n    baseline = evaluate_policy(None, episodes=4, seeds=[11, 22], frame_stack=4, max_steps=600)\n    trained = evaluate_policy(last, episodes=4, seeds=[11, 22], frame_stack=4, max_steps=600)\n    pass_return = trained[\"mean_return\"] - baseline[\"mean_return\"] >= min_return_gain\n    pass_hits = trained[\"mean_hits\"] - baseline[\"mean_hits\"] >= min_hits_gain\n    summary = {\n        \"baseline\": baseline,\n        \"trained\": trained,\n        \"pass_return\": pass_return,\n        \"pass_hits\": pass_hits,\n        \"passed\": pass_return and pass_hits,\n        \"checkpoint\": str(last),\n    }\n    (eval_dir / \"learning_verification.json\").write_text(\n        json.dumps(summary, indent=2), encoding=\"utf-8\"\n    )\n    return 0 if summary[\"passed\"] else 1\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--run-id\", required=True)\n    parser.add_argument(\"--min-return-gain\", type=float, default=0.05)\n    parser.add_argument(\"--min-hits-gain\", type=float, default=1.0)\n    args = parser.parse_args()\n    raise SystemExit(verify_learning(args.run_id, args.min_return_gain, args.min_hits_gain))\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "base": ""
    },
    "tests/test_env_minipong_determinism.py": {
      "additions": 18,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/tests/test_env_minipong_determinism.py b/tests/test_env_minipong_determinism.py\nnew file mode 100644\nindex 0000000..fc35f09\n--- /dev/null\n+++ b/tests/test_env_minipong_determinism.py\n@@ -0,0 +1,18 @@\n+from __future__ import annotations\n+\n+from src.envs.minipong import MiniPongEnv\n+\n+\n+def test_deterministic_reset_and_step() -> None:\n+    env1 = MiniPongEnv()\n+    env2 = MiniPongEnv()\n+    o1, _ = env1.reset(seed=42)\n+    o2, _ = env2.reset(seed=42)\n+    assert (o1 == o2).all()\n+    for a in [2, 0, 1, 2, 2]:\n+        n1, r1, t1, tr1, _ = env1.step(a)\n+        n2, r2, t2, tr2, _ = env2.step(a)\n+        assert (n1 == n2).all()\n+        assert r1 == r2\n+        assert t1 == t2\n+        assert tr1 == tr2\n",
      "raw": "from __future__ import annotations\n\nfrom src.envs.minipong import MiniPongEnv\n\n\ndef test_deterministic_reset_and_step() -> None:\n    env1 = MiniPongEnv()\n    env2 = MiniPongEnv()\n    o1, _ = env1.reset(seed=42)\n    o2, _ = env2.reset(seed=42)\n    assert (o1 == o2).all()\n    for a in [2, 0, 1, 2, 2]:\n        n1, r1, t1, tr1, _ = env1.step(a)\n        n2, r2, t2, tr2, _ = env2.step(a)\n        assert (n1 == n2).all()\n        assert r1 == r2\n        assert t1 == t2\n        assert tr1 == tr2\n",
      "base": ""
    },
    "tests/test_env_minipong_smoke.py": {
      "additions": 11,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/tests/test_env_minipong_smoke.py b/tests/test_env_minipong_smoke.py\nnew file mode 100644\nindex 0000000..5fd1b0d\n--- /dev/null\n+++ b/tests/test_env_minipong_smoke.py\n@@ -0,0 +1,11 @@\n+from __future__ import annotations\n+\n+from src.envs.minipong import MiniPongEnv\n+\n+\n+def test_minipong_smoke() -> None:\n+    env = MiniPongEnv()\n+    obs, info = env.reset(seed=0)\n+    assert obs.shape == (84, 84, 1)\n+    assert obs.dtype.name == \"uint8\"\n+    assert \"hits\" in info\n",
      "raw": "from __future__ import annotations\n\nfrom src.envs.minipong import MiniPongEnv\n\n\ndef test_minipong_smoke() -> None:\n    env = MiniPongEnv()\n    obs, info = env.reset(seed=0)\n    assert obs.shape == (84, 84, 1)\n    assert obs.dtype.name == \"uint8\"\n    assert \"hits\" in info\n",
      "base": ""
    },
    "tests/test_env_wrappers.py": {
      "additions": 5,
      "deletions": 5,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/tests/test_env_wrappers.py b/tests/test_env_wrappers.py\nindex 7ff0850..0bbad78 100644\n--- a/tests/test_env_wrappers.py\n+++ b/tests/test_env_wrappers.py\n@@ -1,7 +1,6 @@\n from __future__ import annotations\n \n-import pytest\n-\n+from src.envs.minipong import MiniPongEnv\n from src.envs.wrappers import wrap_env\n from src.rl.schedules import linear_schedule\n \n@@ -11,6 +10,7 @@ def test_linear_schedule_bounds() -> None:\n     assert linear_schedule(step=10, start=1.0, end=0.0, duration=10) == 0.0\n \n \n-def test_wrap_env_placeholder() -> None:\n-    with pytest.raises(NotImplementedError):\n-        wrap_env(env=None)\n+def test_wrap_env_frame_stack() -> None:\n+    env = wrap_env(MiniPongEnv(), frame_stack=4)\n+    obs, _ = env.reset(seed=1)\n+    assert obs.shape[2] == 4\n",
      "raw": "from __future__ import annotations\n\nfrom src.envs.minipong import MiniPongEnv\nfrom src.envs.wrappers import wrap_env\nfrom src.rl.schedules import linear_schedule\n\n\ndef test_linear_schedule_bounds() -> None:\n    assert linear_schedule(step=-1, start=1.0, end=0.0, duration=10) == 1.0\n    assert linear_schedule(step=10, start=1.0, end=0.0, duration=10) == 0.0\n\n\ndef test_wrap_env_frame_stack() -> None:\n    env = wrap_env(MiniPongEnv(), frame_stack=4)\n    obs, _ = env.reset(seed=1)\n    assert obs.shape[2] == 4\n",
      "base": "from __future__ import annotations\n\nimport pytest\n\nfrom src.envs.wrappers import wrap_env\nfrom src.rl.schedules import linear_schedule\n\n\ndef test_linear_schedule_bounds() -> None:\n    assert linear_schedule(step=-1, start=1.0, end=0.0, duration=10) == 1.0\n    assert linear_schedule(step=10, start=1.0, end=0.0, duration=10) == 0.0\n\n\ndef test_wrap_env_placeholder() -> None:\n    with pytest.raises(NotImplementedError):\n        wrap_env(env=None)\n"
    },
    "tests/test_factory_compile_feedback.py": {
      "additions": 529,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/tests/test_factory_compile_feedback.py b/tests/test_factory_compile_feedback.py\nnew file mode 100644\nindex 0000000..f476e97\n--- /dev/null\n+++ b/tests/test_factory_compile_feedback.py\n@@ -0,0 +1,529 @@\n+\"\"\"Tests for the factory's own feedback compiler (scripts/compile_feedback.py).\n+\n+These test the factory's infrastructure, not the product code.\n+Every test exercises real code paths \u2014 no mocking of compile_feedback internals.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import sys\n+import textwrap\n+from pathlib import Path\n+\n+import pytest\n+\n+# Insert scripts/ into path so we can import the non-package modules.\n+sys.path.insert(0, str(Path(__file__).resolve().parent.parent / \"scripts\"))\n+from compile_feedback import (  # noqa: E402, I001\n+    compile_feedback,\n+    get_iteration_count,\n+    get_previous_feedback,\n+    infer_causes,\n+    load_ci_log,\n+    load_scenario_results,\n+)\n+\n+\n+# \u2500\u2500 Fixtures \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+@pytest.fixture()\n+def factory_dir(tmp_path: Path) -> Path:\n+    \"\"\"Create a temporary factory artifacts directory.\"\"\"\n+    d = tmp_path / \"factory\"\n+    d.mkdir()\n+    return d\n+\n+\n+def _make_results(\n+    factory_dir: Path,\n+    passed: int = 5,\n+    failed: int = 7,\n+    results: list[dict] | None = None,\n+) -> Path:\n+    \"\"\"Write a scenario_results.json to the factory dir.\"\"\"\n+    total = passed + failed\n+    score = passed / total if total > 0 else 0.0\n+    if results is None:\n+        results = []\n+        for i in range(passed):\n+            results.append({\n+                \"name\": f\"passing_{i}\",\n+                \"category\": \"test\",\n+                \"passed\": True,\n+                \"exit_code\": 0,\n+                \"stdout\": \"ok\",\n+                \"stderr\": \"\",\n+                \"duration_seconds\": 0.5,\n+            })\n+        for i in range(failed):\n+            results.append({\n+                \"name\": f\"failing_{i}\",\n+                \"category\": \"test\",\n+                \"passed\": False,\n+                \"exit_code\": 1,\n+                \"stdout\": \"\",\n+                \"stderr\": \"ModuleNotFoundError: No module named 'foo'\"\n+                if i % 2 == 0\n+                else \"AssertionError: expected True\",\n+                \"duration_seconds\": 1.0,\n+                \"error_summary\": \"ModuleNotFoundError\"\n+                if i % 2 == 0\n+                else \"AssertionError\",\n+            })\n+    data = {\n+        \"timestamp\": \"2026-02-22T00:00:00Z\",\n+        \"total\": total,\n+        \"passed\": passed,\n+        \"failed\": failed,\n+        \"skipped\": 0,\n+        \"satisfaction_score\": round(score, 4),\n+        \"results\": results,\n+    }\n+    path = factory_dir / \"scenario_results.json\"\n+    path.write_text(json.dumps(data, indent=2))\n+    return path\n+\n+\n+def _make_feedback(factory_dir: Path, iteration: int, score: float) -> Path:\n+    \"\"\"Write a feedback file for trajectory testing.\"\"\"\n+    content = textwrap.dedent(f\"\"\"\\\n+        # Factory Feedback \u2014 Iteration {iteration}\n+\n+        ## Summary\n+        - **Satisfaction score: {score:.0%}** (x/y scenarios passed)\n+        - Passed: x | Failed: y | Total: z\n+\n+        ## Likely Root Causes\n+        1. Some cause.\n+    \"\"\")\n+    path = factory_dir / f\"feedback_iter_{iteration}.md\"\n+    path.write_text(content)\n+    return path\n+\n+\n+# \u2500\u2500 load_scenario_results tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+class TestLoadScenarioResults:\n+    \"\"\"Tests for loading scenario results JSON.\"\"\"\n+\n+    def test_returns_none_when_file_missing(\n+        self, factory_dir: Path\n+    ) -> None:\n+        result = load_scenario_results(factory_dir / \"nonexistent.json\")\n+        assert result is None\n+\n+    def test_loads_valid_json(self, factory_dir: Path) -> None:\n+        _make_results(factory_dir, passed=3, failed=2)\n+        result = load_scenario_results(\n+            factory_dir / \"scenario_results.json\"\n+        )\n+        assert result is not None\n+        assert result[\"total\"] == 5\n+        assert result[\"passed\"] == 3\n+        assert result[\"failed\"] == 2\n+\n+    def test_preserves_satisfaction_score(\n+        self, factory_dir: Path\n+    ) -> None:\n+        _make_results(factory_dir, passed=8, failed=4)\n+        result = load_scenario_results(\n+            factory_dir / \"scenario_results.json\"\n+        )\n+        assert result is not None\n+        # 8/12 = 0.6667\n+        assert 0.66 < result[\"satisfaction_score\"] < 0.67\n+\n+\n+# \u2500\u2500 load_ci_log tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+class TestLoadCiLog:\n+    \"\"\"Tests for loading CI output logs.\"\"\"\n+\n+    def test_returns_placeholder_when_missing(\n+        self, factory_dir: Path\n+    ) -> None:\n+        result = load_ci_log(factory_dir / \"ci_output.log\")\n+        assert \"no ci log\" in result.lower()\n+\n+    def test_loads_full_log(self, factory_dir: Path) -> None:\n+        log_path = factory_dir / \"ci_output.log\"\n+        log_path.write_text(\"line1\\nline2\\nline3\")\n+        result = load_ci_log(log_path)\n+        assert \"line1\" in result\n+        assert \"line3\" in result\n+\n+    def test_truncates_long_logs(self, factory_dir: Path) -> None:\n+        log_path = factory_dir / \"ci_output.log\"\n+        # Write a log longer than 10000 chars\n+        log_path.write_text(\"A\" * 5000 + \"MIDDLE\" + \"Z\" * 5001)\n+        result = load_ci_log(log_path)\n+        assert \"truncated\" in result.lower()\n+        # Should preserve start and end\n+        assert result.startswith(\"A\")\n+        assert result.rstrip().endswith(\"Z\")\n+\n+\n+# \u2500\u2500 get_iteration_count tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+class TestGetIterationCount:\n+    \"\"\"Tests for reading iteration count.\"\"\"\n+\n+    def test_returns_zero_when_no_file(self, factory_dir: Path) -> None:\n+        count = get_iteration_count(factory_dir)\n+        assert count == 0\n+\n+    def test_reads_integer_from_file(self, factory_dir: Path) -> None:\n+        (factory_dir / \"iteration_count.txt\").write_text(\"5\\n\")\n+        count = get_iteration_count(factory_dir)\n+        assert count == 5\n+\n+    def test_handles_invalid_content(self, factory_dir: Path) -> None:\n+        (factory_dir / \"iteration_count.txt\").write_text(\"not a number\\n\")\n+        count = get_iteration_count(factory_dir)\n+        assert count == 0\n+\n+\n+# \u2500\u2500 get_previous_feedback tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+class TestGetPreviousFeedback:\n+    \"\"\"Tests for loading feedback history.\"\"\"\n+\n+    def test_returns_empty_when_no_feedback(\n+        self, factory_dir: Path\n+    ) -> None:\n+        result = get_previous_feedback(factory_dir)\n+        assert result == []\n+\n+    def test_loads_ordered_feedback(self, factory_dir: Path) -> None:\n+        _make_feedback(factory_dir, 1, 0.25)\n+        _make_feedback(factory_dir, 2, 0.50)\n+        _make_feedback(factory_dir, 3, 0.75)\n+        result = get_previous_feedback(factory_dir)\n+        assert len(result) == 3\n+        assert result[0][0] == 1  # iteration number\n+        assert result[2][0] == 3\n+\n+    def test_extracts_summary_section(self, factory_dir: Path) -> None:\n+        _make_feedback(factory_dir, 1, 0.42)\n+        result = get_previous_feedback(factory_dir)\n+        assert len(result) == 1\n+        # Summary should contain the satisfaction score\n+        assert \"42%\" in result[0][1]\n+\n+\n+# \u2500\u2500 infer_causes tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+class TestInferCauses:\n+    \"\"\"Tests for root cause inference from error patterns.\"\"\"\n+\n+    def test_detects_import_errors(self) -> None:\n+        results = {\n+            \"results\": [\n+                {\n+                    \"name\": \"broken\",\n+                    \"passed\": False,\n+                    \"stderr\": \"ModuleNotFoundError: No module named 'foo'\",\n+                    \"stdout\": \"\",\n+                }\n+            ]\n+        }\n+        causes = infer_causes(results)\n+        assert any(\"import\" in c.lower() for c in causes)\n+\n+    def test_detects_assertion_errors(self) -> None:\n+        results = {\n+            \"results\": [\n+                {\n+                    \"name\": \"assertion_fail\",\n+                    \"passed\": False,\n+                    \"stderr\": \"AssertionError: expected True\",\n+                    \"stdout\": \"\",\n+                }\n+            ]\n+        }\n+        causes = infer_causes(results)\n+        assert any(\"assertion\" in c.lower() for c in causes)\n+\n+    def test_detects_timeouts(self) -> None:\n+        results = {\n+            \"results\": [\n+                {\n+                    \"name\": \"slow\",\n+                    \"passed\": False,\n+                    \"stderr\": \"TIMEOUT: exceeded 60s\",\n+                    \"stdout\": \"\",\n+                }\n+            ]\n+        }\n+        causes = infer_causes(results)\n+        assert any(\"timeout\" in c.lower() for c in causes)\n+\n+    def test_detects_file_not_found(self) -> None:\n+        results = {\n+            \"results\": [\n+                {\n+                    \"name\": \"missing\",\n+                    \"passed\": False,\n+                    \"stderr\": \"FileNotFoundError: [Errno 2] No such file\",\n+                    \"stdout\": \"\",\n+                }\n+            ]\n+        }\n+        causes = infer_causes(results)\n+        assert any(\"missing file\" in c.lower() for c in causes)\n+\n+    def test_returns_fallback_for_unknown_patterns(self) -> None:\n+        results = {\n+            \"results\": [\n+                {\n+                    \"name\": \"weird\",\n+                    \"passed\": False,\n+                    \"stderr\": \"SegmentationFault: core dumped\",\n+                    \"stdout\": \"\",\n+                }\n+            ]\n+        }\n+        causes = infer_causes(results)\n+        assert any(\"no clear pattern\" in c.lower() for c in causes)\n+\n+    def test_ignores_passing_scenarios(self) -> None:\n+        results = {\n+            \"results\": [\n+                {\n+                    \"name\": \"good\",\n+                    \"passed\": True,\n+                    \"stderr\": \"ModuleNotFoundError in warning\",\n+                    \"stdout\": \"\",\n+                }\n+            ]\n+        }\n+        causes = infer_causes(results)\n+        # Should not flag the passing scenario\n+        assert not any(\"good\" in c for c in causes)\n+\n+    def test_multiple_error_types(self) -> None:\n+        results = {\n+            \"results\": [\n+                {\n+                    \"name\": \"import_fail\",\n+                    \"passed\": False,\n+                    \"stderr\": \"ModuleNotFoundError: no module 'x'\",\n+                    \"stdout\": \"\",\n+                },\n+                {\n+                    \"name\": \"assert_fail\",\n+                    \"passed\": False,\n+                    \"stderr\": \"AssertionError: bad\",\n+                    \"stdout\": \"\",\n+                },\n+            ]\n+        }\n+        causes = infer_causes(results)\n+        # Should detect both patterns\n+        assert any(\"import\" in c.lower() for c in causes)\n+        assert any(\"assertion\" in c.lower() for c in causes)\n+\n+\n+# \u2500\u2500 compile_feedback tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+class TestCompileFeedback:\n+    \"\"\"Tests for the main feedback compilation function.\"\"\"\n+\n+    def test_produces_markdown_with_header(self) -> None:\n+        feedback = compile_feedback(\n+            results=None,\n+            ci_log=\"\",\n+            iteration=1,\n+            previous_feedback=[],\n+        )\n+        assert \"# Factory Feedback \u2014 Iteration 1\" in feedback\n+\n+    def test_includes_satisfaction_score(self) -> None:\n+        results = {\n+            \"total\": 10,\n+            \"passed\": 7,\n+            \"failed\": 3,\n+            \"satisfaction_score\": 0.7,\n+            \"results\": [],\n+        }\n+        feedback = compile_feedback(\n+            results=results,\n+            ci_log=\"\",\n+            iteration=1,\n+            previous_feedback=[],\n+        )\n+        assert \"70%\" in feedback\n+        assert \"7/10\" in feedback\n+\n+    def test_includes_convergence_trajectory(self) -> None:\n+        previous = [\n+            (1, \"Score: 25%\"),\n+            (2, \"Score: 50%\"),\n+        ]\n+        feedback = compile_feedback(\n+            results=None,\n+            ci_log=\"\",\n+            iteration=3,\n+            previous_feedback=previous,\n+        )\n+        assert \"Convergence Trajectory\" in feedback\n+        assert \"Score: 25%\" in feedback\n+        assert \"Score: 50%\" in feedback\n+\n+    def test_includes_ci_log_when_present(self) -> None:\n+        feedback = compile_feedback(\n+            results=None,\n+            ci_log=\"ruff check failed with 5 errors\",\n+            iteration=1,\n+            previous_feedback=[],\n+        )\n+        assert \"ruff check failed\" in feedback\n+\n+    def test_excludes_ci_log_placeholder(self) -> None:\n+        feedback = compile_feedback(\n+            results=None,\n+            ci_log=\"(no CI log available)\",\n+            iteration=1,\n+            previous_feedback=[],\n+        )\n+        assert \"CI Log Output\" not in feedback\n+\n+    def test_includes_failed_scenario_details(self) -> None:\n+        results = {\n+            \"total\": 2,\n+            \"passed\": 1,\n+            \"failed\": 1,\n+            \"satisfaction_score\": 0.5,\n+            \"results\": [\n+                {\n+                    \"name\": \"good_one\",\n+                    \"category\": \"test\",\n+                    \"passed\": True,\n+                    \"exit_code\": 0,\n+                    \"stdout\": \"\",\n+                    \"stderr\": \"\",\n+                    \"duration_seconds\": 0.1,\n+                },\n+                {\n+                    \"name\": \"bad_one\",\n+                    \"category\": \"test\",\n+                    \"passed\": False,\n+                    \"exit_code\": 1,\n+                    \"stdout\": \"\",\n+                    \"stderr\": \"ImportError: no module named 'x'\",\n+                    \"duration_seconds\": 0.2,\n+                    \"error_summary\": \"ImportError\",\n+                },\n+            ],\n+        }\n+        feedback = compile_feedback(\n+            results=results,\n+            ci_log=\"\",\n+            iteration=1,\n+            previous_feedback=[],\n+        )\n+        # Should include failed scenario details\n+        assert \"bad_one\" in feedback\n+        assert \"ImportError\" in feedback\n+        # Should NOT list passing scenarios in failure details\n+        assert feedback.count(\"good_one\") == 0 or \"Failed\" not in feedback.split(\"good_one\")[0]\n+\n+    def test_includes_instructions_section(self) -> None:\n+        feedback = compile_feedback(\n+            results=None,\n+            ci_log=\"\",\n+            iteration=1,\n+            previous_feedback=[],\n+        )\n+        assert \"Instructions for Coding Agent\" in feedback\n+        assert \"Import errors\" in feedback\n+\n+    def test_handles_none_results_gracefully(self) -> None:\n+        feedback = compile_feedback(\n+            results=None,\n+            ci_log=\"\",\n+            iteration=1,\n+            previous_feedback=[],\n+        )\n+        assert \"No scenario results available\" in feedback\n+\n+    def test_output_is_valid_markdown(self) -> None:\n+        \"\"\"Basic structural check \u2014 headings use ## format.\"\"\"\n+        results = {\n+            \"total\": 1,\n+            \"passed\": 0,\n+            \"failed\": 1,\n+            \"satisfaction_score\": 0.0,\n+            \"results\": [\n+                {\n+                    \"name\": \"fail\",\n+                    \"category\": \"test\",\n+                    \"passed\": False,\n+                    \"exit_code\": 1,\n+                    \"stdout\": \"out\",\n+                    \"stderr\": \"err\",\n+                    \"duration_seconds\": 1.0,\n+                    \"error_summary\": \"err\",\n+                },\n+            ],\n+        }\n+        feedback = compile_feedback(\n+            results=results,\n+            ci_log=\"some log\",\n+            iteration=3,\n+            previous_feedback=[(1, \"iter 1\"), (2, \"iter 2\")],\n+        )\n+        # Should have proper heading structure\n+        assert feedback.startswith(\"# Factory Feedback\")\n+        assert \"## Summary\" in feedback\n+        assert \"## Likely Root Causes\" in feedback\n+        assert \"## Failed Scenarios\" in feedback\n+\n+\n+# \u2500\u2500 Integration test \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+class TestCompileFeedbackIntegration:\n+    \"\"\"End-to-end test of the feedback compilation pipeline.\"\"\"\n+\n+    def test_full_pipeline(self, factory_dir: Path) -> None:\n+        \"\"\"Write results + feedback files, compile, verify output.\"\"\"\n+        # Set up iteration 1\n+        _make_results(factory_dir, passed=3, failed=9)\n+        _make_feedback(factory_dir, 1, 0.25)\n+\n+        # Load inputs (real function calls, not mocks)\n+        results = load_scenario_results(\n+            factory_dir / \"scenario_results.json\"\n+        )\n+        ci_log = load_ci_log(factory_dir / \"ci_output.log\")\n+        previous = get_previous_feedback(factory_dir)\n+\n+        # Compile\n+        feedback = compile_feedback(\n+            results=results,\n+            ci_log=ci_log,\n+            iteration=2,\n+            previous_feedback=previous,\n+        )\n+\n+        # Verify all sections are present\n+        assert \"Iteration 2\" in feedback\n+        assert \"25%\" in feedback  # satisfaction score from results\n+        assert \"Convergence Trajectory\" in feedback\n+        assert \"Likely Root Causes\" in feedback\n+        assert \"Instructions for Coding Agent\" in feedback\n+\n+        # Verify the output can be written and re-read\n+        output_path = factory_dir / \"feedback_iter_2.md\"\n+        output_path.write_text(feedback)\n+        reloaded = output_path.read_text()\n+        assert reloaded == feedback\n",
      "raw": "\"\"\"Tests for the factory's own feedback compiler (scripts/compile_feedback.py).\n\nThese test the factory's infrastructure, not the product code.\nEvery test exercises real code paths \u2014 no mocking of compile_feedback internals.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport sys\nimport textwrap\nfrom pathlib import Path\n\nimport pytest\n\n# Insert scripts/ into path so we can import the non-package modules.\nsys.path.insert(0, str(Path(__file__).resolve().parent.parent / \"scripts\"))\nfrom compile_feedback import (  # noqa: E402, I001\n    compile_feedback,\n    get_iteration_count,\n    get_previous_feedback,\n    infer_causes,\n    load_ci_log,\n    load_scenario_results,\n)\n\n\n# \u2500\u2500 Fixtures \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\n@pytest.fixture()\ndef factory_dir(tmp_path: Path) -> Path:\n    \"\"\"Create a temporary factory artifacts directory.\"\"\"\n    d = tmp_path / \"factory\"\n    d.mkdir()\n    return d\n\n\ndef _make_results(\n    factory_dir: Path,\n    passed: int = 5,\n    failed: int = 7,\n    results: list[dict] | None = None,\n) -> Path:\n    \"\"\"Write a scenario_results.json to the factory dir.\"\"\"\n    total = passed + failed\n    score = passed / total if total > 0 else 0.0\n    if results is None:\n        results = []\n        for i in range(passed):\n            results.append({\n                \"name\": f\"passing_{i}\",\n                \"category\": \"test\",\n                \"passed\": True,\n                \"exit_code\": 0,\n                \"stdout\": \"ok\",\n                \"stderr\": \"\",\n                \"duration_seconds\": 0.5,\n            })\n        for i in range(failed):\n            results.append({\n                \"name\": f\"failing_{i}\",\n                \"category\": \"test\",\n                \"passed\": False,\n                \"exit_code\": 1,\n                \"stdout\": \"\",\n                \"stderr\": \"ModuleNotFoundError: No module named 'foo'\"\n                if i % 2 == 0\n                else \"AssertionError: expected True\",\n                \"duration_seconds\": 1.0,\n                \"error_summary\": \"ModuleNotFoundError\"\n                if i % 2 == 0\n                else \"AssertionError\",\n            })\n    data = {\n        \"timestamp\": \"2026-02-22T00:00:00Z\",\n        \"total\": total,\n        \"passed\": passed,\n        \"failed\": failed,\n        \"skipped\": 0,\n        \"satisfaction_score\": round(score, 4),\n        \"results\": results,\n    }\n    path = factory_dir / \"scenario_results.json\"\n    path.write_text(json.dumps(data, indent=2))\n    return path\n\n\ndef _make_feedback(factory_dir: Path, iteration: int, score: float) -> Path:\n    \"\"\"Write a feedback file for trajectory testing.\"\"\"\n    content = textwrap.dedent(f\"\"\"\\\n        # Factory Feedback \u2014 Iteration {iteration}\n\n        ## Summary\n        - **Satisfaction score: {score:.0%}** (x/y scenarios passed)\n        - Passed: x | Failed: y | Total: z\n\n        ## Likely Root Causes\n        1. Some cause.\n    \"\"\")\n    path = factory_dir / f\"feedback_iter_{iteration}.md\"\n    path.write_text(content)\n    return path\n\n\n# \u2500\u2500 load_scenario_results tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nclass TestLoadScenarioResults:\n    \"\"\"Tests for loading scenario results JSON.\"\"\"\n\n    def test_returns_none_when_file_missing(\n        self, factory_dir: Path\n    ) -> None:\n        result = load_scenario_results(factory_dir / \"nonexistent.json\")\n        assert result is None\n\n    def test_loads_valid_json(self, factory_dir: Path) -> None:\n        _make_results(factory_dir, passed=3, failed=2)\n        result = load_scenario_results(\n            factory_dir / \"scenario_results.json\"\n        )\n        assert result is not None\n        assert result[\"total\"] == 5\n        assert result[\"passed\"] == 3\n        assert result[\"failed\"] == 2\n\n    def test_preserves_satisfaction_score(\n        self, factory_dir: Path\n    ) -> None:\n        _make_results(factory_dir, passed=8, failed=4)\n        result = load_scenario_results(\n            factory_dir / \"scenario_results.json\"\n        )\n        assert result is not None\n        # 8/12 = 0.6667\n        assert 0.66 < result[\"satisfaction_score\"] < 0.67\n\n\n# \u2500\u2500 load_ci_log tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nclass TestLoadCiLog:\n    \"\"\"Tests for loading CI output logs.\"\"\"\n\n    def test_returns_placeholder_when_missing(\n        self, factory_dir: Path\n    ) -> None:\n        result = load_ci_log(factory_dir / \"ci_output.log\")\n        assert \"no ci log\" in result.lower()\n\n    def test_loads_full_log(self, factory_dir: Path) -> None:\n        log_path = factory_dir / \"ci_output.log\"\n        log_path.write_text(\"line1\\nline2\\nline3\")\n        result = load_ci_log(log_path)\n        assert \"line1\" in result\n        assert \"line3\" in result\n\n    def test_truncates_long_logs(self, factory_dir: Path) -> None:\n        log_path = factory_dir / \"ci_output.log\"\n        # Write a log longer than 10000 chars\n        log_path.write_text(\"A\" * 5000 + \"MIDDLE\" + \"Z\" * 5001)\n        result = load_ci_log(log_path)\n        assert \"truncated\" in result.lower()\n        # Should preserve start and end\n        assert result.startswith(\"A\")\n        assert result.rstrip().endswith(\"Z\")\n\n\n# \u2500\u2500 get_iteration_count tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nclass TestGetIterationCount:\n    \"\"\"Tests for reading iteration count.\"\"\"\n\n    def test_returns_zero_when_no_file(self, factory_dir: Path) -> None:\n        count = get_iteration_count(factory_dir)\n        assert count == 0\n\n    def test_reads_integer_from_file(self, factory_dir: Path) -> None:\n        (factory_dir / \"iteration_count.txt\").write_text(\"5\\n\")\n        count = get_iteration_count(factory_dir)\n        assert count == 5\n\n    def test_handles_invalid_content(self, factory_dir: Path) -> None:\n        (factory_dir / \"iteration_count.txt\").write_text(\"not a number\\n\")\n        count = get_iteration_count(factory_dir)\n        assert count == 0\n\n\n# \u2500\u2500 get_previous_feedback tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nclass TestGetPreviousFeedback:\n    \"\"\"Tests for loading feedback history.\"\"\"\n\n    def test_returns_empty_when_no_feedback(\n        self, factory_dir: Path\n    ) -> None:\n        result = get_previous_feedback(factory_dir)\n        assert result == []\n\n    def test_loads_ordered_feedback(self, factory_dir: Path) -> None:\n        _make_feedback(factory_dir, 1, 0.25)\n        _make_feedback(factory_dir, 2, 0.50)\n        _make_feedback(factory_dir, 3, 0.75)\n        result = get_previous_feedback(factory_dir)\n        assert len(result) == 3\n        assert result[0][0] == 1  # iteration number\n        assert result[2][0] == 3\n\n    def test_extracts_summary_section(self, factory_dir: Path) -> None:\n        _make_feedback(factory_dir, 1, 0.42)\n        result = get_previous_feedback(factory_dir)\n        assert len(result) == 1\n        # Summary should contain the satisfaction score\n        assert \"42%\" in result[0][1]\n\n\n# \u2500\u2500 infer_causes tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nclass TestInferCauses:\n    \"\"\"Tests for root cause inference from error patterns.\"\"\"\n\n    def test_detects_import_errors(self) -> None:\n        results = {\n            \"results\": [\n                {\n                    \"name\": \"broken\",\n                    \"passed\": False,\n                    \"stderr\": \"ModuleNotFoundError: No module named 'foo'\",\n                    \"stdout\": \"\",\n                }\n            ]\n        }\n        causes = infer_causes(results)\n        assert any(\"import\" in c.lower() for c in causes)\n\n    def test_detects_assertion_errors(self) -> None:\n        results = {\n            \"results\": [\n                {\n                    \"name\": \"assertion_fail\",\n                    \"passed\": False,\n                    \"stderr\": \"AssertionError: expected True\",\n                    \"stdout\": \"\",\n                }\n            ]\n        }\n        causes = infer_causes(results)\n        assert any(\"assertion\" in c.lower() for c in causes)\n\n    def test_detects_timeouts(self) -> None:\n        results = {\n            \"results\": [\n                {\n                    \"name\": \"slow\",\n                    \"passed\": False,\n                    \"stderr\": \"TIMEOUT: exceeded 60s\",\n                    \"stdout\": \"\",\n                }\n            ]\n        }\n        causes = infer_causes(results)\n        assert any(\"timeout\" in c.lower() for c in causes)\n\n    def test_detects_file_not_found(self) -> None:\n        results = {\n            \"results\": [\n                {\n                    \"name\": \"missing\",\n                    \"passed\": False,\n                    \"stderr\": \"FileNotFoundError: [Errno 2] No such file\",\n                    \"stdout\": \"\",\n                }\n            ]\n        }\n        causes = infer_causes(results)\n        assert any(\"missing file\" in c.lower() for c in causes)\n\n    def test_returns_fallback_for_unknown_patterns(self) -> None:\n        results = {\n            \"results\": [\n                {\n                    \"name\": \"weird\",\n                    \"passed\": False,\n                    \"stderr\": \"SegmentationFault: core dumped\",\n                    \"stdout\": \"\",\n                }\n            ]\n        }\n        causes = infer_causes(results)\n        assert any(\"no clear pattern\" in c.lower() for c in causes)\n\n    def test_ignores_passing_scenarios(self) -> None:\n        results = {\n            \"results\": [\n                {\n                    \"name\": \"good\",\n                    \"passed\": True,\n                    \"stderr\": \"ModuleNotFoundError in warning\",\n                    \"stdout\": \"\",\n                }\n            ]\n        }\n        causes = infer_causes(results)\n        # Should not flag the passing scenario\n        assert not any(\"good\" in c for c in causes)\n\n    def test_multiple_error_types(self) -> None:\n        results = {\n            \"results\": [\n                {\n                    \"name\": \"import_fail\",\n                    \"passed\": False,\n                    \"stderr\": \"ModuleNotFoundError: no module 'x'\",\n                    \"stdout\": \"\",\n                },\n                {\n                    \"name\": \"assert_fail\",\n                    \"passed\": False,\n                    \"stderr\": \"AssertionError: bad\",\n                    \"stdout\": \"\",\n                },\n            ]\n        }\n        causes = infer_causes(results)\n        # Should detect both patterns\n        assert any(\"import\" in c.lower() for c in causes)\n        assert any(\"assertion\" in c.lower() for c in causes)\n\n\n# \u2500\u2500 compile_feedback tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nclass TestCompileFeedback:\n    \"\"\"Tests for the main feedback compilation function.\"\"\"\n\n    def test_produces_markdown_with_header(self) -> None:\n        feedback = compile_feedback(\n            results=None,\n            ci_log=\"\",\n            iteration=1,\n            previous_feedback=[],\n        )\n        assert \"# Factory Feedback \u2014 Iteration 1\" in feedback\n\n    def test_includes_satisfaction_score(self) -> None:\n        results = {\n            \"total\": 10,\n            \"passed\": 7,\n            \"failed\": 3,\n            \"satisfaction_score\": 0.7,\n            \"results\": [],\n        }\n        feedback = compile_feedback(\n            results=results,\n            ci_log=\"\",\n            iteration=1,\n            previous_feedback=[],\n        )\n        assert \"70%\" in feedback\n        assert \"7/10\" in feedback\n\n    def test_includes_convergence_trajectory(self) -> None:\n        previous = [\n            (1, \"Score: 25%\"),\n            (2, \"Score: 50%\"),\n        ]\n        feedback = compile_feedback(\n            results=None,\n            ci_log=\"\",\n            iteration=3,\n            previous_feedback=previous,\n        )\n        assert \"Convergence Trajectory\" in feedback\n        assert \"Score: 25%\" in feedback\n        assert \"Score: 50%\" in feedback\n\n    def test_includes_ci_log_when_present(self) -> None:\n        feedback = compile_feedback(\n            results=None,\n            ci_log=\"ruff check failed with 5 errors\",\n            iteration=1,\n            previous_feedback=[],\n        )\n        assert \"ruff check failed\" in feedback\n\n    def test_excludes_ci_log_placeholder(self) -> None:\n        feedback = compile_feedback(\n            results=None,\n            ci_log=\"(no CI log available)\",\n            iteration=1,\n            previous_feedback=[],\n        )\n        assert \"CI Log Output\" not in feedback\n\n    def test_includes_failed_scenario_details(self) -> None:\n        results = {\n            \"total\": 2,\n            \"passed\": 1,\n            \"failed\": 1,\n            \"satisfaction_score\": 0.5,\n            \"results\": [\n                {\n                    \"name\": \"good_one\",\n                    \"category\": \"test\",\n                    \"passed\": True,\n                    \"exit_code\": 0,\n                    \"stdout\": \"\",\n                    \"stderr\": \"\",\n                    \"duration_seconds\": 0.1,\n                },\n                {\n                    \"name\": \"bad_one\",\n                    \"category\": \"test\",\n                    \"passed\": False,\n                    \"exit_code\": 1,\n                    \"stdout\": \"\",\n                    \"stderr\": \"ImportError: no module named 'x'\",\n                    \"duration_seconds\": 0.2,\n                    \"error_summary\": \"ImportError\",\n                },\n            ],\n        }\n        feedback = compile_feedback(\n            results=results,\n            ci_log=\"\",\n            iteration=1,\n            previous_feedback=[],\n        )\n        # Should include failed scenario details\n        assert \"bad_one\" in feedback\n        assert \"ImportError\" in feedback\n        # Should NOT list passing scenarios in failure details\n        assert feedback.count(\"good_one\") == 0 or \"Failed\" not in feedback.split(\"good_one\")[0]\n\n    def test_includes_instructions_section(self) -> None:\n        feedback = compile_feedback(\n            results=None,\n            ci_log=\"\",\n            iteration=1,\n            previous_feedback=[],\n        )\n        assert \"Instructions for Coding Agent\" in feedback\n        assert \"Import errors\" in feedback\n\n    def test_handles_none_results_gracefully(self) -> None:\n        feedback = compile_feedback(\n            results=None,\n            ci_log=\"\",\n            iteration=1,\n            previous_feedback=[],\n        )\n        assert \"No scenario results available\" in feedback\n\n    def test_output_is_valid_markdown(self) -> None:\n        \"\"\"Basic structural check \u2014 headings use ## format.\"\"\"\n        results = {\n            \"total\": 1,\n            \"passed\": 0,\n            \"failed\": 1,\n            \"satisfaction_score\": 0.0,\n            \"results\": [\n                {\n                    \"name\": \"fail\",\n                    \"category\": \"test\",\n                    \"passed\": False,\n                    \"exit_code\": 1,\n                    \"stdout\": \"out\",\n                    \"stderr\": \"err\",\n                    \"duration_seconds\": 1.0,\n                    \"error_summary\": \"err\",\n                },\n            ],\n        }\n        feedback = compile_feedback(\n            results=results,\n            ci_log=\"some log\",\n            iteration=3,\n            previous_feedback=[(1, \"iter 1\"), (2, \"iter 2\")],\n        )\n        # Should have proper heading structure\n        assert feedback.startswith(\"# Factory Feedback\")\n        assert \"## Summary\" in feedback\n        assert \"## Likely Root Causes\" in feedback\n        assert \"## Failed Scenarios\" in feedback\n\n\n# \u2500\u2500 Integration test \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nclass TestCompileFeedbackIntegration:\n    \"\"\"End-to-end test of the feedback compilation pipeline.\"\"\"\n\n    def test_full_pipeline(self, factory_dir: Path) -> None:\n        \"\"\"Write results + feedback files, compile, verify output.\"\"\"\n        # Set up iteration 1\n        _make_results(factory_dir, passed=3, failed=9)\n        _make_feedback(factory_dir, 1, 0.25)\n\n        # Load inputs (real function calls, not mocks)\n        results = load_scenario_results(\n            factory_dir / \"scenario_results.json\"\n        )\n        ci_log = load_ci_log(factory_dir / \"ci_output.log\")\n        previous = get_previous_feedback(factory_dir)\n\n        # Compile\n        feedback = compile_feedback(\n            results=results,\n            ci_log=ci_log,\n            iteration=2,\n            previous_feedback=previous,\n        )\n\n        # Verify all sections are present\n        assert \"Iteration 2\" in feedback\n        assert \"25%\" in feedback  # satisfaction score from results\n        assert \"Convergence Trajectory\" in feedback\n        assert \"Likely Root Causes\" in feedback\n        assert \"Instructions for Coding Agent\" in feedback\n\n        # Verify the output can be written and re-read\n        output_path = factory_dir / \"feedback_iter_2.md\"\n        output_path.write_text(feedback)\n        reloaded = output_path.read_text()\n        assert reloaded == feedback\n",
      "base": ""
    },
    "tests/test_factory_run_scenarios.py": {
      "additions": 368,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/tests/test_factory_run_scenarios.py b/tests/test_factory_run_scenarios.py\nnew file mode 100644\nindex 0000000..f561aa9\n--- /dev/null\n+++ b/tests/test_factory_run_scenarios.py\n@@ -0,0 +1,368 @@\n+\"\"\"Tests for the factory's own scenario runner (scripts/run_scenarios.py).\n+\n+These test the factory's infrastructure, not the product code.\n+Every test exercises real code paths \u2014 no mocking of run_scenarios internals.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+import sys\n+import textwrap\n+from pathlib import Path\n+\n+import pytest\n+\n+# Insert scripts/ into path so we can import the non-package modules.\n+sys.path.insert(0, str(Path(__file__).resolve().parent.parent / \"scripts\"))\n+from run_scenarios import (  # noqa: E402, I001\n+    Scenario,\n+    ScenarioReport,\n+    ScenarioResult,\n+    parse_scenario,\n+    run_scenario,\n+)\n+\n+\n+# \u2500\u2500 Fixtures \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+@pytest.fixture()\n+def scenario_dir(tmp_path: Path) -> Path:\n+    \"\"\"Create a temporary scenarios directory with test scenarios.\"\"\"\n+    d = tmp_path / \"scenarios\"\n+    d.mkdir()\n+    return d\n+\n+\n+def _write_scenario(\n+    d: Path,\n+    name: str,\n+    category: str,\n+    eval_command: str,\n+    pass_criteria: str = \"Exits with code 0\",\n+) -> Path:\n+    \"\"\"Write a scenario markdown file with given parameters.\"\"\"\n+    content = textwrap.dedent(f\"\"\"\\\n+        # Scenario: {name}\n+\n+        ## Category\n+        {category}\n+\n+        ## Preconditions\n+        - None\n+\n+        ## Behavioral Expectation\n+        Test expectation for {name}.\n+\n+        ## Evaluation Method\n+        ```bash\n+        {eval_command}\n+        ```\n+\n+        ## Pass Criteria\n+        {pass_criteria}\n+\n+        ## Evidence Required\n+        - stdout\n+    \"\"\")\n+    path = d / f\"{name.lower().replace(' ', '_')}.md\"\n+    path.write_text(content)\n+    return path\n+\n+\n+# \u2500\u2500 parse_scenario tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+class TestParseScenario:\n+    \"\"\"Tests for scenario markdown parsing.\"\"\"\n+\n+    def test_parses_name_from_h1(self, scenario_dir: Path) -> None:\n+        path = _write_scenario(scenario_dir, \"Foo Bar Test\", \"environment\", \"echo hi\")\n+        scenario = parse_scenario(path)\n+        assert scenario.name == \"Foo Bar Test\"\n+\n+    def test_parses_category(self, scenario_dir: Path) -> None:\n+        path = _write_scenario(scenario_dir, \"Cat Test\", \"training\", \"echo hi\")\n+        scenario = parse_scenario(path)\n+        assert scenario.category == \"training\"\n+\n+    def test_parses_evaluation_method_from_code_block(\n+        self, scenario_dir: Path\n+    ) -> None:\n+        path = _write_scenario(\n+            scenario_dir,\n+            \"Eval Test\",\n+            \"environment\",\n+            'python -c \"print(42)\"',\n+        )\n+        scenario = parse_scenario(path)\n+        assert 'python -c \"print(42)\"' in scenario.evaluation_method\n+\n+    def test_parses_preconditions_as_list(self, scenario_dir: Path) -> None:\n+        content = textwrap.dedent(\"\"\"\\\n+            # Scenario: Multi Precond\n+\n+            ## Category\n+            environment\n+\n+            ## Preconditions\n+            - First condition\n+            - Second condition\n+            - Third condition\n+\n+            ## Behavioral Expectation\n+            Something.\n+\n+            ## Evaluation Method\n+            ```bash\n+            echo ok\n+            ```\n+\n+            ## Pass Criteria\n+            Exits 0.\n+\n+            ## Evidence Required\n+            - stdout\n+        \"\"\")\n+        path = scenario_dir / \"multi_precond.md\"\n+        path.write_text(content)\n+        scenario = parse_scenario(path)\n+        assert len(scenario.preconditions) == 3\n+        assert \"First condition\" in scenario.preconditions[0]\n+\n+    def test_parses_pass_criteria(self, scenario_dir: Path) -> None:\n+        path = _write_scenario(\n+            scenario_dir, \"PC Test\", \"integration\", \"echo ok\", \"Script exits with 0\"\n+        )\n+        scenario = parse_scenario(path)\n+        assert \"exits with 0\" in scenario.pass_criteria.lower()\n+\n+    def test_handles_missing_sections_gracefully(\n+        self, scenario_dir: Path\n+    ) -> None:\n+        \"\"\"A minimal scenario file should parse without crashing.\"\"\"\n+        content = \"# Scenario: Minimal\\n\\n## Category\\ntest\\n\"\n+        path = scenario_dir / \"minimal.md\"\n+        path.write_text(content)\n+        scenario = parse_scenario(path)\n+        assert scenario.name == \"Minimal\"\n+        assert scenario.category == \"test\"\n+        assert scenario.evaluation_method == \"\"\n+\n+    def test_returns_scenario_dataclass(self, scenario_dir: Path) -> None:\n+        path = _write_scenario(scenario_dir, \"Type Test\", \"environment\", \"echo 1\")\n+        result = parse_scenario(path)\n+        assert isinstance(result, Scenario)\n+        assert result.file_path == str(path)\n+\n+\n+# \u2500\u2500 run_scenario tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+class TestRunScenario:\n+    \"\"\"Tests for scenario execution.\n+\n+    These run REAL bash commands \u2014 not mocked subprocess calls.\n+    \"\"\"\n+\n+    def test_passing_scenario_returns_passed_true(self, tmp_path: Path) -> None:\n+        scenario = Scenario(\n+            name=\"Passing\",\n+            file_path=\"test.md\",\n+            category=\"test\",\n+            preconditions=[],\n+            behavioral_expectation=\"\",\n+            evaluation_method=\"exit 0\",\n+            pass_criteria=\"\",\n+            evidence_required=[],\n+        )\n+        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n+        assert result.passed is True\n+        assert result.exit_code == 0\n+\n+    def test_failing_scenario_returns_passed_false(self, tmp_path: Path) -> None:\n+        scenario = Scenario(\n+            name=\"Failing\",\n+            file_path=\"test.md\",\n+            category=\"test\",\n+            preconditions=[],\n+            behavioral_expectation=\"\",\n+            evaluation_method=\"exit 1\",\n+            pass_criteria=\"\",\n+            evidence_required=[],\n+        )\n+        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n+        assert result.passed is False\n+        assert result.exit_code == 1\n+\n+    def test_timeout_scenario_returns_timeout_error(\n+        self, tmp_path: Path\n+    ) -> None:\n+        scenario = Scenario(\n+            name=\"Slow\",\n+            file_path=\"test.md\",\n+            category=\"test\",\n+            preconditions=[],\n+            behavioral_expectation=\"\",\n+            evaluation_method=\"sleep 30\",\n+            pass_criteria=\"\",\n+            evidence_required=[],\n+        )\n+        result = run_scenario(scenario, timeout=1, repo_root=tmp_path)\n+        assert result.passed is False\n+        assert result.exit_code == -1\n+        assert \"TIMEOUT\" in result.stderr\n+\n+    def test_captures_stdout(self, tmp_path: Path) -> None:\n+        scenario = Scenario(\n+            name=\"Output\",\n+            file_path=\"test.md\",\n+            category=\"test\",\n+            preconditions=[],\n+            behavioral_expectation=\"\",\n+            evaluation_method='echo \"hello factory\"',\n+            pass_criteria=\"\",\n+            evidence_required=[],\n+        )\n+        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n+        assert \"hello factory\" in result.stdout\n+\n+    def test_captures_stderr(self, tmp_path: Path) -> None:\n+        scenario = Scenario(\n+            name=\"Stderr\",\n+            file_path=\"test.md\",\n+            category=\"test\",\n+            preconditions=[],\n+            behavioral_expectation=\"\",\n+            evaluation_method='echo \"err msg\" >&2; exit 1',\n+            pass_criteria=\"\",\n+            evidence_required=[],\n+        )\n+        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n+        assert \"err msg\" in result.stderr\n+\n+    def test_error_summary_extracts_error_lines(\n+        self, tmp_path: Path\n+    ) -> None:\n+        scenario = Scenario(\n+            name=\"ErrorExtract\",\n+            file_path=\"test.md\",\n+            category=\"test\",\n+            preconditions=[],\n+            behavioral_expectation=\"\",\n+            evaluation_method=(\n+                'echo \"line 1\" >&2; '\n+                'echo \"AssertionError: bad value\" >&2; '\n+                \"exit 1\"\n+            ),\n+            pass_criteria=\"\",\n+            evidence_required=[],\n+        )\n+        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n+        assert \"AssertionError\" in result.error_summary\n+\n+    def test_duration_is_positive(self, tmp_path: Path) -> None:\n+        scenario = Scenario(\n+            name=\"Duration\",\n+            file_path=\"test.md\",\n+            category=\"test\",\n+            preconditions=[],\n+            behavioral_expectation=\"\",\n+            evaluation_method=\"sleep 0.1\",\n+            pass_criteria=\"\",\n+            evidence_required=[],\n+        )\n+        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n+        assert result.duration_seconds > 0\n+\n+    def test_result_is_scenarioresult_dataclass(\n+        self, tmp_path: Path\n+    ) -> None:\n+        scenario = Scenario(\n+            name=\"Type\",\n+            file_path=\"test.md\",\n+            category=\"test\",\n+            preconditions=[],\n+            behavioral_expectation=\"\",\n+            evaluation_method=\"exit 0\",\n+            pass_criteria=\"\",\n+            evidence_required=[],\n+        )\n+        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n+        assert isinstance(result, ScenarioResult)\n+\n+    def test_cwd_is_repo_root(self, tmp_path: Path) -> None:\n+        \"\"\"The scenario runs in the repo root, not some temp dir.\"\"\"\n+        scenario = Scenario(\n+            name=\"CWD\",\n+            file_path=\"test.md\",\n+            category=\"test\",\n+            preconditions=[],\n+            behavioral_expectation=\"\",\n+            evaluation_method=\"pwd\",\n+            pass_criteria=\"\",\n+            evidence_required=[],\n+        )\n+        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n+        # pwd output should match the repo root we passed in\n+        assert str(tmp_path) in result.stdout\n+\n+    def test_pythonpath_is_set(self, tmp_path: Path) -> None:\n+        \"\"\"PYTHONPATH should include repo root for imports.\"\"\"\n+        scenario = Scenario(\n+            name=\"PythonPath\",\n+            file_path=\"test.md\",\n+            category=\"test\",\n+            preconditions=[],\n+            behavioral_expectation=\"\",\n+            evaluation_method=\"echo $PYTHONPATH\",\n+            pass_criteria=\"\",\n+            evidence_required=[],\n+        )\n+        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n+        assert str(tmp_path) in result.stdout\n+\n+\n+# \u2500\u2500 Dataclass tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n+\n+\n+class TestDataclasses:\n+    \"\"\"Verify that dataclasses serialize correctly for JSON output.\"\"\"\n+\n+    def test_scenario_report_serializes_to_json(self) -> None:\n+        from dataclasses import asdict\n+\n+        report = ScenarioReport(\n+            timestamp=\"2026-02-22T00:00:00Z\",\n+            total=2,\n+            passed=1,\n+            failed=1,\n+            skipped=0,\n+            satisfaction_score=0.5,\n+            results=[],\n+        )\n+        data = asdict(report)\n+        json_str = json.dumps(data)\n+        parsed = json.loads(json_str)\n+        assert parsed[\"satisfaction_score\"] == 0.5\n+        assert parsed[\"total\"] == 2\n+\n+    def test_scenario_result_serializes_to_json(self) -> None:\n+        from dataclasses import asdict\n+\n+        result = ScenarioResult(\n+            name=\"Test\",\n+            file_path=\"test.md\",\n+            category=\"test\",\n+            passed=True,\n+            exit_code=0,\n+            stdout=\"ok\",\n+            stderr=\"\",\n+            duration_seconds=1.5,\n+        )\n+        data = asdict(result)\n+        json_str = json.dumps(data)\n+        parsed = json.loads(json_str)\n+        assert parsed[\"passed\"] is True\n+        assert parsed[\"duration_seconds\"] == 1.5\n",
      "raw": "\"\"\"Tests for the factory's own scenario runner (scripts/run_scenarios.py).\n\nThese test the factory's infrastructure, not the product code.\nEvery test exercises real code paths \u2014 no mocking of run_scenarios internals.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport sys\nimport textwrap\nfrom pathlib import Path\n\nimport pytest\n\n# Insert scripts/ into path so we can import the non-package modules.\nsys.path.insert(0, str(Path(__file__).resolve().parent.parent / \"scripts\"))\nfrom run_scenarios import (  # noqa: E402, I001\n    Scenario,\n    ScenarioReport,\n    ScenarioResult,\n    parse_scenario,\n    run_scenario,\n)\n\n\n# \u2500\u2500 Fixtures \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\n@pytest.fixture()\ndef scenario_dir(tmp_path: Path) -> Path:\n    \"\"\"Create a temporary scenarios directory with test scenarios.\"\"\"\n    d = tmp_path / \"scenarios\"\n    d.mkdir()\n    return d\n\n\ndef _write_scenario(\n    d: Path,\n    name: str,\n    category: str,\n    eval_command: str,\n    pass_criteria: str = \"Exits with code 0\",\n) -> Path:\n    \"\"\"Write a scenario markdown file with given parameters.\"\"\"\n    content = textwrap.dedent(f\"\"\"\\\n        # Scenario: {name}\n\n        ## Category\n        {category}\n\n        ## Preconditions\n        - None\n\n        ## Behavioral Expectation\n        Test expectation for {name}.\n\n        ## Evaluation Method\n        ```bash\n        {eval_command}\n        ```\n\n        ## Pass Criteria\n        {pass_criteria}\n\n        ## Evidence Required\n        - stdout\n    \"\"\")\n    path = d / f\"{name.lower().replace(' ', '_')}.md\"\n    path.write_text(content)\n    return path\n\n\n# \u2500\u2500 parse_scenario tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nclass TestParseScenario:\n    \"\"\"Tests for scenario markdown parsing.\"\"\"\n\n    def test_parses_name_from_h1(self, scenario_dir: Path) -> None:\n        path = _write_scenario(scenario_dir, \"Foo Bar Test\", \"environment\", \"echo hi\")\n        scenario = parse_scenario(path)\n        assert scenario.name == \"Foo Bar Test\"\n\n    def test_parses_category(self, scenario_dir: Path) -> None:\n        path = _write_scenario(scenario_dir, \"Cat Test\", \"training\", \"echo hi\")\n        scenario = parse_scenario(path)\n        assert scenario.category == \"training\"\n\n    def test_parses_evaluation_method_from_code_block(\n        self, scenario_dir: Path\n    ) -> None:\n        path = _write_scenario(\n            scenario_dir,\n            \"Eval Test\",\n            \"environment\",\n            'python -c \"print(42)\"',\n        )\n        scenario = parse_scenario(path)\n        assert 'python -c \"print(42)\"' in scenario.evaluation_method\n\n    def test_parses_preconditions_as_list(self, scenario_dir: Path) -> None:\n        content = textwrap.dedent(\"\"\"\\\n            # Scenario: Multi Precond\n\n            ## Category\n            environment\n\n            ## Preconditions\n            - First condition\n            - Second condition\n            - Third condition\n\n            ## Behavioral Expectation\n            Something.\n\n            ## Evaluation Method\n            ```bash\n            echo ok\n            ```\n\n            ## Pass Criteria\n            Exits 0.\n\n            ## Evidence Required\n            - stdout\n        \"\"\")\n        path = scenario_dir / \"multi_precond.md\"\n        path.write_text(content)\n        scenario = parse_scenario(path)\n        assert len(scenario.preconditions) == 3\n        assert \"First condition\" in scenario.preconditions[0]\n\n    def test_parses_pass_criteria(self, scenario_dir: Path) -> None:\n        path = _write_scenario(\n            scenario_dir, \"PC Test\", \"integration\", \"echo ok\", \"Script exits with 0\"\n        )\n        scenario = parse_scenario(path)\n        assert \"exits with 0\" in scenario.pass_criteria.lower()\n\n    def test_handles_missing_sections_gracefully(\n        self, scenario_dir: Path\n    ) -> None:\n        \"\"\"A minimal scenario file should parse without crashing.\"\"\"\n        content = \"# Scenario: Minimal\\n\\n## Category\\ntest\\n\"\n        path = scenario_dir / \"minimal.md\"\n        path.write_text(content)\n        scenario = parse_scenario(path)\n        assert scenario.name == \"Minimal\"\n        assert scenario.category == \"test\"\n        assert scenario.evaluation_method == \"\"\n\n    def test_returns_scenario_dataclass(self, scenario_dir: Path) -> None:\n        path = _write_scenario(scenario_dir, \"Type Test\", \"environment\", \"echo 1\")\n        result = parse_scenario(path)\n        assert isinstance(result, Scenario)\n        assert result.file_path == str(path)\n\n\n# \u2500\u2500 run_scenario tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nclass TestRunScenario:\n    \"\"\"Tests for scenario execution.\n\n    These run REAL bash commands \u2014 not mocked subprocess calls.\n    \"\"\"\n\n    def test_passing_scenario_returns_passed_true(self, tmp_path: Path) -> None:\n        scenario = Scenario(\n            name=\"Passing\",\n            file_path=\"test.md\",\n            category=\"test\",\n            preconditions=[],\n            behavioral_expectation=\"\",\n            evaluation_method=\"exit 0\",\n            pass_criteria=\"\",\n            evidence_required=[],\n        )\n        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n        assert result.passed is True\n        assert result.exit_code == 0\n\n    def test_failing_scenario_returns_passed_false(self, tmp_path: Path) -> None:\n        scenario = Scenario(\n            name=\"Failing\",\n            file_path=\"test.md\",\n            category=\"test\",\n            preconditions=[],\n            behavioral_expectation=\"\",\n            evaluation_method=\"exit 1\",\n            pass_criteria=\"\",\n            evidence_required=[],\n        )\n        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n        assert result.passed is False\n        assert result.exit_code == 1\n\n    def test_timeout_scenario_returns_timeout_error(\n        self, tmp_path: Path\n    ) -> None:\n        scenario = Scenario(\n            name=\"Slow\",\n            file_path=\"test.md\",\n            category=\"test\",\n            preconditions=[],\n            behavioral_expectation=\"\",\n            evaluation_method=\"sleep 30\",\n            pass_criteria=\"\",\n            evidence_required=[],\n        )\n        result = run_scenario(scenario, timeout=1, repo_root=tmp_path)\n        assert result.passed is False\n        assert result.exit_code == -1\n        assert \"TIMEOUT\" in result.stderr\n\n    def test_captures_stdout(self, tmp_path: Path) -> None:\n        scenario = Scenario(\n            name=\"Output\",\n            file_path=\"test.md\",\n            category=\"test\",\n            preconditions=[],\n            behavioral_expectation=\"\",\n            evaluation_method='echo \"hello factory\"',\n            pass_criteria=\"\",\n            evidence_required=[],\n        )\n        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n        assert \"hello factory\" in result.stdout\n\n    def test_captures_stderr(self, tmp_path: Path) -> None:\n        scenario = Scenario(\n            name=\"Stderr\",\n            file_path=\"test.md\",\n            category=\"test\",\n            preconditions=[],\n            behavioral_expectation=\"\",\n            evaluation_method='echo \"err msg\" >&2; exit 1',\n            pass_criteria=\"\",\n            evidence_required=[],\n        )\n        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n        assert \"err msg\" in result.stderr\n\n    def test_error_summary_extracts_error_lines(\n        self, tmp_path: Path\n    ) -> None:\n        scenario = Scenario(\n            name=\"ErrorExtract\",\n            file_path=\"test.md\",\n            category=\"test\",\n            preconditions=[],\n            behavioral_expectation=\"\",\n            evaluation_method=(\n                'echo \"line 1\" >&2; '\n                'echo \"AssertionError: bad value\" >&2; '\n                \"exit 1\"\n            ),\n            pass_criteria=\"\",\n            evidence_required=[],\n        )\n        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n        assert \"AssertionError\" in result.error_summary\n\n    def test_duration_is_positive(self, tmp_path: Path) -> None:\n        scenario = Scenario(\n            name=\"Duration\",\n            file_path=\"test.md\",\n            category=\"test\",\n            preconditions=[],\n            behavioral_expectation=\"\",\n            evaluation_method=\"sleep 0.1\",\n            pass_criteria=\"\",\n            evidence_required=[],\n        )\n        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n        assert result.duration_seconds > 0\n\n    def test_result_is_scenarioresult_dataclass(\n        self, tmp_path: Path\n    ) -> None:\n        scenario = Scenario(\n            name=\"Type\",\n            file_path=\"test.md\",\n            category=\"test\",\n            preconditions=[],\n            behavioral_expectation=\"\",\n            evaluation_method=\"exit 0\",\n            pass_criteria=\"\",\n            evidence_required=[],\n        )\n        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n        assert isinstance(result, ScenarioResult)\n\n    def test_cwd_is_repo_root(self, tmp_path: Path) -> None:\n        \"\"\"The scenario runs in the repo root, not some temp dir.\"\"\"\n        scenario = Scenario(\n            name=\"CWD\",\n            file_path=\"test.md\",\n            category=\"test\",\n            preconditions=[],\n            behavioral_expectation=\"\",\n            evaluation_method=\"pwd\",\n            pass_criteria=\"\",\n            evidence_required=[],\n        )\n        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n        # pwd output should match the repo root we passed in\n        assert str(tmp_path) in result.stdout\n\n    def test_pythonpath_is_set(self, tmp_path: Path) -> None:\n        \"\"\"PYTHONPATH should include repo root for imports.\"\"\"\n        scenario = Scenario(\n            name=\"PythonPath\",\n            file_path=\"test.md\",\n            category=\"test\",\n            preconditions=[],\n            behavioral_expectation=\"\",\n            evaluation_method=\"echo $PYTHONPATH\",\n            pass_criteria=\"\",\n            evidence_required=[],\n        )\n        result = run_scenario(scenario, timeout=10, repo_root=tmp_path)\n        assert str(tmp_path) in result.stdout\n\n\n# \u2500\u2500 Dataclass tests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nclass TestDataclasses:\n    \"\"\"Verify that dataclasses serialize correctly for JSON output.\"\"\"\n\n    def test_scenario_report_serializes_to_json(self) -> None:\n        from dataclasses import asdict\n\n        report = ScenarioReport(\n            timestamp=\"2026-02-22T00:00:00Z\",\n            total=2,\n            passed=1,\n            failed=1,\n            skipped=0,\n            satisfaction_score=0.5,\n            results=[],\n        )\n        data = asdict(report)\n        json_str = json.dumps(data)\n        parsed = json.loads(json_str)\n        assert parsed[\"satisfaction_score\"] == 0.5\n        assert parsed[\"total\"] == 2\n\n    def test_scenario_result_serializes_to_json(self) -> None:\n        from dataclasses import asdict\n\n        result = ScenarioResult(\n            name=\"Test\",\n            file_path=\"test.md\",\n            category=\"test\",\n            passed=True,\n            exit_code=0,\n            stdout=\"ok\",\n            stderr=\"\",\n            duration_seconds=1.5,\n        )\n        data = asdict(result)\n        json_str = json.dumps(data)\n        parsed = json.loads(json_str)\n        assert parsed[\"passed\"] is True\n        assert parsed[\"duration_seconds\"] == 1.5\n",
      "base": ""
    },
    "tests/test_replay_buffer_behavior.py": {
      "additions": 15,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/tests/test_replay_buffer_behavior.py b/tests/test_replay_buffer_behavior.py\nnew file mode 100644\nindex 0000000..5a2592d\n--- /dev/null\n+++ b/tests/test_replay_buffer_behavior.py\n@@ -0,0 +1,15 @@\n+from __future__ import annotations\n+\n+import numpy as np\n+\n+from src.rl.replay import ReplayBuffer, Transition\n+\n+\n+def test_replay_capacity_and_sample() -> None:\n+    rb = ReplayBuffer(capacity=3)\n+    obs = np.zeros((2, 2, 1), dtype=np.uint8)\n+    for i in range(5):\n+        rb.add(Transition(obs=obs + i, action=0, reward=0.0, next_obs=obs + i, done=False))\n+    assert len(rb) == 3\n+    sample = rb.sample(2)\n+    assert len(sample) == 2\n",
      "raw": "from __future__ import annotations\n\nimport numpy as np\n\nfrom src.rl.replay import ReplayBuffer, Transition\n\n\ndef test_replay_capacity_and_sample() -> None:\n    rb = ReplayBuffer(capacity=3)\n    obs = np.zeros((2, 2, 1), dtype=np.uint8)\n    for i in range(5):\n        rb.add(Transition(obs=obs + i, action=0, reward=0.0, next_obs=obs + i, done=False))\n    assert len(rb) == 3\n    sample = rb.sample(2)\n    assert len(sample) == 2\n",
      "base": ""
    },
    "tests/test_training_smoke_cpu.py": {
      "additions": 30,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/tests/test_training_smoke_cpu.py b/tests/test_training_smoke_cpu.py\nnew file mode 100644\nindex 0000000..f4ea9eb\n--- /dev/null\n+++ b/tests/test_training_smoke_cpu.py\n@@ -0,0 +1,30 @@\n+from __future__ import annotations\n+\n+from pathlib import Path\n+\n+from src.train.train_dqn import train\n+\n+\n+def test_training_smoke_cpu() -> None:\n+    run_id = \"pytest_smoke\"\n+    config = {\n+        \"run_id\": run_id,\n+        \"seed\": 1,\n+        \"frame_stack\": 2,\n+        \"total_steps\": 100,\n+        \"max_episode_steps\": 200,\n+        \"replay_capacity\": 1000,\n+        \"replay_warmup_steps\": 10,\n+        \"batch_size\": 8,\n+        \"gamma\": 0.99,\n+        \"lr\": 0.001,\n+        \"epsilon_start\": 1.0,\n+        \"epsilon_end\": 0.1,\n+        \"epsilon_decay_steps\": 80,\n+        \"target_update_period\": 20,\n+        \"eval_every_steps\": 50,\n+        \"eval_episodes\": 2,\n+        \"eval_seeds\": [3, 4],\n+    }\n+    train(config)\n+    assert (Path(\"artifacts\") / run_id / \"checkpoints\").exists()\n",
      "raw": "from __future__ import annotations\n\nfrom pathlib import Path\n\nfrom src.train.train_dqn import train\n\n\ndef test_training_smoke_cpu() -> None:\n    run_id = \"pytest_smoke\"\n    config = {\n        \"run_id\": run_id,\n        \"seed\": 1,\n        \"frame_stack\": 2,\n        \"total_steps\": 100,\n        \"max_episode_steps\": 200,\n        \"replay_capacity\": 1000,\n        \"replay_warmup_steps\": 10,\n        \"batch_size\": 8,\n        \"gamma\": 0.99,\n        \"lr\": 0.001,\n        \"epsilon_start\": 1.0,\n        \"epsilon_end\": 0.1,\n        \"epsilon_decay_steps\": 80,\n        \"target_update_period\": 20,\n        \"eval_every_steps\": 50,\n        \"eval_episodes\": 2,\n        \"eval_seeds\": [3, 4],\n    }\n    train(config)\n    assert (Path(\"artifacts\") / run_id / \"checkpoints\").exists()\n",
      "base": ""
    },
    "tests/test_whitepapers_verify_manifest.py": {
      "additions": 10,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/tests/test_whitepapers_verify_manifest.py b/tests/test_whitepapers_verify_manifest.py\nnew file mode 100644\nindex 0000000..f86667a\n--- /dev/null\n+++ b/tests/test_whitepapers_verify_manifest.py\n@@ -0,0 +1,10 @@\n+from __future__ import annotations\n+\n+import json\n+from pathlib import Path\n+\n+\n+def test_whitepaper_manifest_schema() -> None:\n+    manifest = json.loads(Path(\"docs/whitepapers/manifest.json\").read_text(encoding=\"utf-8\"))\n+    assert \"papers\" in manifest\n+    assert isinstance(manifest[\"papers\"], list)\n",
      "raw": "from __future__ import annotations\n\nimport json\nfrom pathlib import Path\n\n\ndef test_whitepaper_manifest_schema() -> None:\n    manifest = json.loads(Path(\"docs/whitepapers/manifest.json\").read_text(encoding=\"utf-8\"))\n    assert \"papers\" in manifest\n    assert isinstance(manifest[\"papers\"], list)\n",
      "base": ""
    }
  }
}