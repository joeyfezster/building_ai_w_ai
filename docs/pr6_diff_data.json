{
  "pr": 6,
  "base_branch": "main",
  "head_branch": "df-crank-v01-eval-videos",
  "head_sha": "bce032a",
  "head_sha_full": "bce032aa149f125641214933666551aa3a59380d",
  "total_files": 2,
  "total_additions": 16,
  "total_deletions": 2,
  "files": {
    "src/train/record_video.py": {
      "additions": 6,
      "deletions": 2,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/train/record_video.py b/src/train/record_video.py\nindex 8339ad2..1b4d97f 100644\n--- a/src/train/record_video.py\n+++ b/src/train/record_video.py\n@@ -15,9 +15,13 @@ from src.rl.replay import ReplayBuffer\n \n \n def record_video(\n-    checkpoint: Path | None, output_path: Path, seed: int = 0, max_steps: int = 1000\n+    checkpoint: Path | None,\n+    output_path: Path,\n+    seed: int = 0,\n+    max_steps: int = 1000,\n+    frame_stack: int = 4,\n ) -> None:\n-    env = wrap_env(MiniPongEnv(render_mode=\"rgb_array\"), frame_stack=4)\n+    env = wrap_env(MiniPongEnv(render_mode=\"rgb_array\"), frame_stack=frame_stack)\n     obs, _ = env.reset(seed=seed)\n     replay = ReplayBuffer(capacity=8)\n     agent = DQNAgent(obs.shape, env.action_space.n, replay, DQNConfig())\n",
      "raw": "\"\"\"Record evaluation video.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nfrom pathlib import Path\n\nimport imageio.v2 as imageio\nimport torch\n\nfrom src.agents.dqn_agent import DQNAgent, DQNConfig\nfrom src.envs.minipong import MiniPongEnv\nfrom src.envs.wrappers import wrap_env\nfrom src.rl.replay import ReplayBuffer\n\n\ndef record_video(\n    checkpoint: Path | None,\n    output_path: Path,\n    seed: int = 0,\n    max_steps: int = 1000,\n    frame_stack: int = 4,\n) -> None:\n    env = wrap_env(MiniPongEnv(render_mode=\"rgb_array\"), frame_stack=frame_stack)\n    obs, _ = env.reset(seed=seed)\n    replay = ReplayBuffer(capacity=8)\n    agent = DQNAgent(obs.shape, env.action_space.n, replay, DQNConfig())\n    if checkpoint is not None:\n        data = torch.load(checkpoint, map_location=agent.device)\n        agent.online.load_state_dict(data[\"model\"])\n    frames = []\n    done = False\n    trunc = False\n    steps = 0\n    while not done and not trunc and steps < max_steps:\n        action = agent.act(obs, epsilon=0.0) if checkpoint else env.action_space.sample()\n        obs, _, done, trunc, _ = env.step(action)\n        frame = env.unwrapped.render()\n        frames.append(frame)\n        steps += 1\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    imageio.mimsave(output_path, frames, fps=30)\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--checkpoint\", default=\"\")\n    parser.add_argument(\"--output\", required=True)\n    parser.add_argument(\"--seed\", type=int, default=0)\n    args = parser.parse_args()\n    ckpt = Path(args.checkpoint) if args.checkpoint else None\n    record_video(ckpt, Path(args.output), seed=args.seed)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "base": "\"\"\"Record evaluation video.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nfrom pathlib import Path\n\nimport imageio.v2 as imageio\nimport torch\n\nfrom src.agents.dqn_agent import DQNAgent, DQNConfig\nfrom src.envs.minipong import MiniPongEnv\nfrom src.envs.wrappers import wrap_env\nfrom src.rl.replay import ReplayBuffer\n\n\ndef record_video(\n    checkpoint: Path | None, output_path: Path, seed: int = 0, max_steps: int = 1000\n) -> None:\n    env = wrap_env(MiniPongEnv(render_mode=\"rgb_array\"), frame_stack=4)\n    obs, _ = env.reset(seed=seed)\n    replay = ReplayBuffer(capacity=8)\n    agent = DQNAgent(obs.shape, env.action_space.n, replay, DQNConfig())\n    if checkpoint is not None:\n        data = torch.load(checkpoint, map_location=agent.device)\n        agent.online.load_state_dict(data[\"model\"])\n    frames = []\n    done = False\n    trunc = False\n    steps = 0\n    while not done and not trunc and steps < max_steps:\n        action = agent.act(obs, epsilon=0.0) if checkpoint else env.action_space.sample()\n        obs, _, done, trunc, _ = env.step(action)\n        frame = env.unwrapped.render()\n        frames.append(frame)\n        steps += 1\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    imageio.mimsave(output_path, frames, fps=30)\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--checkpoint\", default=\"\")\n    parser.add_argument(\"--output\", required=True)\n    parser.add_argument(\"--seed\", type=int, default=0)\n    args = parser.parse_args()\n    ckpt = Path(args.checkpoint) if args.checkpoint else None\n    record_video(ckpt, Path(args.output), seed=args.seed)\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    "src/train/train_dqn.py": {
      "additions": 10,
      "deletions": 0,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/src/train/train_dqn.py b/src/train/train_dqn.py\nindex ff5390b..86b8807 100644\n--- a/src/train/train_dqn.py\n+++ b/src/train/train_dqn.py\n@@ -15,6 +15,7 @@ from src.obs.logging import MetricsLogger\n from src.rl.replay import ReplayBuffer, Transition\n from src.rl.schedules import linear_schedule\n from src.train.evaluate import evaluate_policy\n+from src.train.record_video import record_video\n \n \n def train(config: dict) -> str:\n@@ -100,6 +101,15 @@ def train(config: dict) -> str:\n                     \"eval/mean_hits\": metrics[\"mean_hits\"],\n                 },\n             )\n+            eval_seeds = config.get(\"eval_seeds\", [])\n+            if eval_seeds:\n+                video_path = run_dir / \"videos\" / f\"eval_step_{step}.mp4\"\n+                record_video(\n+                    ckpt_path,\n+                    video_path,\n+                    seed=int(eval_seeds[0]),\n+                    frame_stack=int(config[\"frame_stack\"]),\n+                )\n \n     logger.close()\n     return run_id\n",
      "raw": "\"\"\"Train DQN on MiniPong.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nfrom pathlib import Path\n\nimport yaml\n\nfrom src.agents.dqn_agent import DQNAgent, DQNConfig\nfrom src.envs.minipong import MiniPongEnv\nfrom src.envs.wrappers import wrap_env\nfrom src.obs.logging import MetricsLogger\nfrom src.rl.replay import ReplayBuffer, Transition\nfrom src.rl.schedules import linear_schedule\nfrom src.train.evaluate import evaluate_policy\nfrom src.train.record_video import record_video\n\n\ndef train(config: dict) -> str:\n    run_id = config.get(\"run_id\", \"local_run\")\n    run_dir = Path(\"artifacts\") / run_id\n    (run_dir / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n    (run_dir / \"eval\").mkdir(exist_ok=True)\n    (run_dir / \"videos\").mkdir(exist_ok=True)\n\n    env = wrap_env(MiniPongEnv(), frame_stack=int(config[\"frame_stack\"]))\n    obs, _ = env.reset(seed=int(config[\"seed\"]))\n    replay = ReplayBuffer(capacity=int(config[\"replay_capacity\"]))\n    agent = DQNAgent(\n        obs.shape,\n        env.action_space.n,\n        replay,\n        DQNConfig(\n            lr=float(config[\"lr\"]),\n            gamma=float(config[\"gamma\"]),\n            batch_size=int(config[\"batch_size\"]),\n        ),\n    )\n    logger = MetricsLogger(run_dir)\n\n    episode_return = 0.0\n    total_steps = int(config[\"total_steps\"])\n    for step in range(1, total_steps + 1):\n        eps = linear_schedule(\n            step,\n            float(config[\"epsilon_start\"]),\n            float(config[\"epsilon_end\"]),\n            int(config[\"epsilon_decay_steps\"]),\n        )\n        action = agent.act(obs, epsilon=eps)\n        next_obs, reward, terminated, truncated, info = env.step(action)\n        done = terminated or truncated\n        agent.observe(\n            Transition(obs=obs, action=action, reward=reward, next_obs=next_obs, done=done)\n        )\n        episode_return += reward\n        obs = next_obs\n\n        loss = None\n        if step > int(config[\"replay_warmup_steps\"]) and len(replay) >= int(config[\"batch_size\"]):\n            loss = agent.update()\n        if step % int(config[\"target_update_period\"]) == 0:\n            agent.sync_target()\n\n        if done:\n            logger.log_metrics(\n                step,\n                {\n                    \"train/episode_return\": episode_return,\n                    \"train/hits\": info.get(\"hits\", 0),\n                    \"train/epsilon\": eps,\n                },\n            )\n            episode_return = 0.0\n            obs, _ = env.reset(seed=int(config[\"seed\"]) + step)\n\n        if loss is not None and step % 10 == 0:\n            logger.log_metrics(step, {\"train/loss\": loss, \"train/epsilon\": eps})\n\n        if step % int(config[\"eval_every_steps\"]) == 0:\n            ckpt_path = run_dir / \"checkpoints\" / f\"step_{step}.pt\"\n            import torch\n\n            torch.save({\"model\": agent.online.state_dict(), \"step\": step}, ckpt_path)\n            metrics = evaluate_policy(\n                ckpt_path,\n                int(config[\"eval_episodes\"]),\n                list(config[\"eval_seeds\"]),\n                int(config[\"frame_stack\"]),\n                int(config[\"max_episode_steps\"]),\n            )\n            (run_dir / \"eval\" / f\"metrics_step_{step}.json\").write_text(\n                json.dumps(metrics, indent=2), encoding=\"utf-8\"\n            )\n            logger.log_metrics(\n                step,\n                {\n                    \"eval/mean_return\": metrics[\"mean_return\"],\n                    \"eval/mean_hits\": metrics[\"mean_hits\"],\n                },\n            )\n            eval_seeds = config.get(\"eval_seeds\", [])\n            if eval_seeds:\n                video_path = run_dir / \"videos\" / f\"eval_step_{step}.mp4\"\n                record_video(\n                    ckpt_path,\n                    video_path,\n                    seed=int(eval_seeds[0]),\n                    frame_stack=int(config[\"frame_stack\"]),\n                )\n\n    logger.close()\n    return run_id\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", default=\"configs/dqn_minipong.yaml\")\n    parser.add_argument(\"--run-id\", default=\"\")\n    args = parser.parse_args()\n\n    config = yaml.safe_load(Path(args.config).read_text(encoding=\"utf-8\"))\n    if args.run_id:\n        config[\"run_id\"] = args.run_id\n    run_id = train(config)\n    print(run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "base": "\"\"\"Train DQN on MiniPong.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nfrom pathlib import Path\n\nimport yaml\n\nfrom src.agents.dqn_agent import DQNAgent, DQNConfig\nfrom src.envs.minipong import MiniPongEnv\nfrom src.envs.wrappers import wrap_env\nfrom src.obs.logging import MetricsLogger\nfrom src.rl.replay import ReplayBuffer, Transition\nfrom src.rl.schedules import linear_schedule\nfrom src.train.evaluate import evaluate_policy\n\n\ndef train(config: dict) -> str:\n    run_id = config.get(\"run_id\", \"local_run\")\n    run_dir = Path(\"artifacts\") / run_id\n    (run_dir / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n    (run_dir / \"eval\").mkdir(exist_ok=True)\n    (run_dir / \"videos\").mkdir(exist_ok=True)\n\n    env = wrap_env(MiniPongEnv(), frame_stack=int(config[\"frame_stack\"]))\n    obs, _ = env.reset(seed=int(config[\"seed\"]))\n    replay = ReplayBuffer(capacity=int(config[\"replay_capacity\"]))\n    agent = DQNAgent(\n        obs.shape,\n        env.action_space.n,\n        replay,\n        DQNConfig(\n            lr=float(config[\"lr\"]),\n            gamma=float(config[\"gamma\"]),\n            batch_size=int(config[\"batch_size\"]),\n        ),\n    )\n    logger = MetricsLogger(run_dir)\n\n    episode_return = 0.0\n    total_steps = int(config[\"total_steps\"])\n    for step in range(1, total_steps + 1):\n        eps = linear_schedule(\n            step,\n            float(config[\"epsilon_start\"]),\n            float(config[\"epsilon_end\"]),\n            int(config[\"epsilon_decay_steps\"]),\n        )\n        action = agent.act(obs, epsilon=eps)\n        next_obs, reward, terminated, truncated, info = env.step(action)\n        done = terminated or truncated\n        agent.observe(\n            Transition(obs=obs, action=action, reward=reward, next_obs=next_obs, done=done)\n        )\n        episode_return += reward\n        obs = next_obs\n\n        loss = None\n        if step > int(config[\"replay_warmup_steps\"]) and len(replay) >= int(config[\"batch_size\"]):\n            loss = agent.update()\n        if step % int(config[\"target_update_period\"]) == 0:\n            agent.sync_target()\n\n        if done:\n            logger.log_metrics(\n                step,\n                {\n                    \"train/episode_return\": episode_return,\n                    \"train/hits\": info.get(\"hits\", 0),\n                    \"train/epsilon\": eps,\n                },\n            )\n            episode_return = 0.0\n            obs, _ = env.reset(seed=int(config[\"seed\"]) + step)\n\n        if loss is not None and step % 10 == 0:\n            logger.log_metrics(step, {\"train/loss\": loss, \"train/epsilon\": eps})\n\n        if step % int(config[\"eval_every_steps\"]) == 0:\n            ckpt_path = run_dir / \"checkpoints\" / f\"step_{step}.pt\"\n            import torch\n\n            torch.save({\"model\": agent.online.state_dict(), \"step\": step}, ckpt_path)\n            metrics = evaluate_policy(\n                ckpt_path,\n                int(config[\"eval_episodes\"]),\n                list(config[\"eval_seeds\"]),\n                int(config[\"frame_stack\"]),\n                int(config[\"max_episode_steps\"]),\n            )\n            (run_dir / \"eval\" / f\"metrics_step_{step}.json\").write_text(\n                json.dumps(metrics, indent=2), encoding=\"utf-8\"\n            )\n            logger.log_metrics(\n                step,\n                {\n                    \"eval/mean_return\": metrics[\"mean_return\"],\n                    \"eval/mean_hits\": metrics[\"mean_hits\"],\n                },\n            )\n\n    logger.close()\n    return run_id\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", default=\"configs/dqn_minipong.yaml\")\n    parser.add_argument(\"--run-id\", default=\"\")\n    args = parser.parse_args()\n\n    config = yaml.safe_load(Path(args.config).read_text(encoding=\"utf-8\"))\n    if args.run_id:\n        config[\"run_id\"] = args.run_id\n    run_id = train(config)\n    print(run_id)\n\n\nif __name__ == \"__main__\":\n    main()\n"
    }
  }
}