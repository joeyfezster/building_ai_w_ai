{
  "pr": 10,
  "base_branch": "main",
  "head_branch": "worktree-decisions-persistence",
  "head_sha": "babed4d",
  "head_sha_full": "babed4d70da20f23867b9436595dc6fdf49112d2",
  "total_files": 6,
  "total_additions": 340,
  "total_deletions": 3,
  "files": {
    ".claude/skills/factory-orchestrate/SKILL.md": {
      "additions": 29,
      "deletions": 0,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/.claude/skills/factory-orchestrate/SKILL.md b/.claude/skills/factory-orchestrate/SKILL.md\nindex 1c6b664..449069d 100644\n--- a/.claude/skills/factory-orchestrate/SKILL.md\n+++ b/.claude/skills/factory-orchestrate/SKILL.md\n@@ -200,6 +200,33 @@ The review pack gives the project lead:\n \n The review pack is the artifact that communicates factory status to the human. Without it, the accept/merge gate is a rubber stamp.\n \n+### Step 13: Post-Merge Persistence\n+\n+After the project lead merges the PR:\n+\n+1. **Persist decisions** to the cumulative log:\n+   ```bash\n+   python scripts/persist_decisions.py --pr {PR_NUMBER}\n+   ```\n+   The script extracts decisions from the review pack HTML (or from the JSON intermediate if available via `--data`) and appends them to `docs/decisions/decision_log.json`. It is idempotent \u2014 safe to run multiple times.\n+\n+2. **Create post-merge issues** (if applicable):\n+   ```bash\n+   python scripts/create_postmerge_issues.py --pr {PR_NUMBER}\n+   ```\n+\n+3. **Commit and push** the updated decision log:\n+   ```bash\n+   git add docs/decisions/decision_log.json\n+   git commit -m \"decisions: persist PR #{PR_NUMBER} decisions\"\n+   git push\n+   ```\n+\n+4. **Delete Codex's remote branch** (cleanup):\n+   ```bash\n+   git push origin --delete codex-{branch}\n+   ```\n+\n ### Stall Protocol\n \n If after 3+ iterations:\n@@ -215,6 +242,8 @@ If after 3+ iterations:\n - **NFR checks script**: `scripts/nfr_checks.py` (Gate 0 tool agents + Gate 2)\n - **Test quality scanner**: `scripts/check_test_quality.py` (Gate 0 tool agent)\n - **PR review pack skill**: `.claude/skills/pr-review-pack/SKILL.md` (Step 12)\n+- **Decision log**: `docs/decisions/decision_log.json` (Step 13, cumulative archive)\n+- **Decision persistence**: `scripts/persist_decisions.py` (Step 13)\n - **Specs**: `specs/*.md`\n - **Factory docs**: `docs/dark_factory.md`\n - **Factory architecture**: `docs/factory_architecture.html`\n",
      "raw": "---\nname: factory-orchestrate\ndescription: Run the dark factory convergence loop. Use when the user says \"run a factory crank\", \"start the factory\", \"orchestrate a crank\", or similar. Orchestrates Codex via browser, manages holdout isolation, runs validation gates, and performs LLM-as-judge evaluation.\nallowed-tools: Bash, Read, Write, Glob, Grep, Edit\n---\n\n# Dark Factory Orchestration \u2014 Claude Code as Orchestrator\n\nYou are the factory orchestrator. You run the convergence loop that turns specs into working software through iterative AI coding + validation.\n\n## Prerequisites\n\n- Chrome is logged into Codex (ChatGPT Plus account)\n- Repository: `joeyfezster/building_ai_w_ai`\n- Branch to base work on: confirm with user or default to `factory/v1`\n- Satisfaction threshold: confirm with user or default to 80%\n- Max iterations: confirm with user or default to 5\n\n## The Loop\n\nFor each iteration:\n\n### Step 1: Create Factory Branch\n```bash\n# First crank \u2014 create from base branch\ngit checkout -b df-crank-v01-{descriptor} {base_branch}\ngit push -u origin df-crank-v01-{descriptor}\n```\n\nBranch naming: `df-crank-vXX-{descriptor}` where XX is the crank version.\n\n### Step 2: Strip Holdout\n```bash\npython scripts/strip_holdout.py\ngit push\n```\n\nThis deterministically removes `/scenarios/` and comments out scenario Makefile targets. Codex literally cannot see evaluation criteria.\n\nVerify: `ls scenarios/` should fail (directory gone).\n\n### Step 3: Invoke Codex via Browser\n\nOpen the Codex UI in Chrome. Provide:\n- **Repository**: `joeyfezster/building_ai_w_ai`\n- **Base branch**: `df-crank-vXX-{descriptor}` (the stripped branch)\n- **Prompt**: Contents of `.github/codex/prompts/factory_fix.md` + the latest feedback file (`artifacts/factory/feedback_iter_N.md`)\n- **Versions**: 1\n\nCodex will create its own branch (named `codex-...`). Wait for it to finish.\n\n### Step 4: Gate 0 \u2014 Adversarial Code Review (Agent Team)\n\nBefore merging Codex's changes, run a **full adversarial review via agent teams**. This is the first line of defense \u2014 there is no point sending code to CI or later gates if Gate 0 finds critical issues.\n\n1. Fetch Codex's branch: `git fetch origin`\n2. Get the diff: `git diff df-crank-vXX...origin/codex-{branch}`\n\n3. **Spawn the Gate 0 agent team.** Create a team and launch these agents in parallel:\n\n   **Tool agents** (deterministic, Bash-capable \u2014 run ALL simultaneously):\n   | Agent | What It Runs | What It Catches |\n   |-------|-------------|-----------------|\n   | `ruff-agent` | `python scripts/nfr_checks.py --check code_quality` | Lint violations, style, import issues |\n   | `radon-agent` | `python scripts/nfr_checks.py --check complexity` | Cyclomatic complexity > threshold |\n   | `vulture-agent` | `python scripts/nfr_checks.py --check dead_code` | Unreachable code, unused functions |\n   | `bandit-agent` | `python scripts/nfr_checks.py --check security` | Security vulnerabilities |\n   | `test-quality-agent` | `python scripts/check_test_quality.py` | Vacuous tests, stub assertions, mock abuse |\n\n   **Semantic reviewer** (LLM-based, runs in parallel with tool agents):\n   | Agent | What It Reads | What It Catches |\n   |-------|--------------|-----------------|\n   | `adversarial-reviewer` | The diff + `.github/codex/prompts/adversarial_review.md` + `docs/code_quality_standards.md` + `/specs/*.md` | Gaming, architectural dishonesty, spec violations, integration gaps, subtle patterns the tools miss |\n\n4. **Aggregate findings.** Collect all agent outputs. Each finding has a severity: CRITICAL, WARNING, or NIT.\n\n5. **Fail-fast rule:** If **any agent** reports a CRITICAL finding, Gate 0 fails. Do NOT merge. Compile all findings (from all agents) as feedback and loop back to Step 3 with specific remediation instructions.\n\n**If clean or WARNING-only across all agents**: Proceed to Step 5. WARNING findings are tracked \u2014 they feed into the LLM-as-judge evaluation in Step 10.\n\n**Why agent teams, not a single reviewer:** The tool agents catch cheap, obvious violations in seconds (dead code, complexity, security). The semantic reviewer catches subtle gaming and architectural dishonesty. Running them in parallel means Gate 0 is both fast AND thorough. A single reviewer doing everything sequentially is slower and more likely to miss things.\n\n### Step 5: Merge Codex Changes\n```bash\ngit merge origin/codex-{branch} --no-ff -m \"factory: merge codex iteration N\"\n```\n\n### Step 5b: Check CI Results\n\nAfter pushing, check CI results. CI runs Gates 1-3 on every push to factory/** branches. Use CI results as early signal before running gates locally.\n\n```bash\n# Wait for CI to complete (typically 2-5 minutes)\ngh run list --branch df-crank-vXX --limit 3\n\n# Check the PR's checks (if PR exists)\ngh pr checks <PR_NUMBER>\n```\n\n**Known `gh pr checks` behavior:**\n- `gh pr checks` shows checks associated with the PR's **merge ref**, not the branch HEAD directly\n- If a bot (GITHUB_TOKEN) pushes a commit that becomes PR HEAD, GitHub does NOT re-trigger CI workflows (prevents infinite loops). The PR may show \"0 checks\" even though CI ran fine on the previous commit.\n- Workaround: If you see stale/missing checks, use `gh run list --branch <branch>` instead \u2014 this shows actual workflow runs regardless of the merge ref.\n- Commits pushed by workflows using `GITHUB_TOKEN` do not trigger other workflows. This is a GitHub safety measure.\n- If CI results conflict with your local gate results, investigate \u2014 don't just ignore the discrepancy.\n\n**If CI fails**: Use the github.com's copilot's 'explain errors' as initial reference, check logs to validate and make up your own mind, compile actionable feedback (yourself), and loop back to Step 3.\n\n### Step 6: Restore Holdout\n```bash\npython scripts/restore_holdout.py\ngit add scenarios/ Makefile\ngit commit -m \"factory: restore holdout scenarios for evaluation\"\n```\n\n### Step 7: Gate 1 \u2014 Deterministic Validation\n```bash\nmake lint && make typecheck && make test\n```\n\n\"test\" = *should be* the FULL pytest suite, including any tests Codex wrote (already reviewed in Gate 0).\n\n**If fail**: Compile feedback (use this script as aid, but make sure you intervene if the feedback doesn't make sense: `python scripts/compile_feedback.py --iteration N`), loop to Step 3.\n\n### Step 8: Gate 2 \u2014 Non-Functional Requirements\n```bash\nmake nfr-check\n```\n\nThis runs all implemented NFR checks (code quality, complexity, dead code, security).\n\nGate 2 is **non-blocking** but findings are tracked and feed into:\n- The feedback for the next Codex iteration\n- Your LLM-as-judge evaluation in Step 10\n\n### Step 9: Gate 3 \u2014 Behavioral Scenarios\n```bash\npython scripts/run_scenarios.py --timeout 180\n```\n\nProduces `artifacts/factory/scenario_results.json` with satisfaction score.\n\n### Step 10: LLM-as-Judge \u2014 Holistic Evaluation\n\nYou ARE the judge. Don't just check `satisfaction_score >= threshold`. Reason through:\n\n1. **Satisfaction trajectory**: Is the score improving across iterations? Plateaued? Regressing?\n2. **Failure patterns**: Are the same scenarios failing repeatedly? Different ones each time?\n3. **Fix quality**: Do Codex's changes look like real solutions or gaming attempts? (Gate 0 caught the obvious ones, but look for subtle patterns across iterations)\n4. **Gate 2 NFR findings**: Even though non-blocking, are there concerning patterns? Growing complexity? Dropping coverage?\n5. **Systemic issues**: Is there something the score doesn't capture? An architectural problem that will cause future failures?\n6. **Documentation currency**: Did this iteration's changes affect documented behavior? Check: Are specs in `/specs/` still accurate? Does the README reflect current state? Are factory docs (`dark_factory.md`, `code_quality_standards.md`) still correct? Stale documentation is technical debt \u2014 flag it in feedback if needed.\n\n**If satisfied**: Proceed to Step 11.\n**If not satisfied**: Compile feedback with your holistic assessment, loop to Step 3.\n\n### Step 11: Create PR (Accept/Merge Gate)\n```bash\ngh pr create \\\n  --title \"[Factory] df-crank-vXX converged at {score}%\" \\\n  --body \"$(cat <<'EOF'\n## Dark Factory \u2014 Converged\n\n**Satisfaction score: {score}%**\n**Iterations: {N}**\n**Gate 2 NFR status: {summary}**\n\n### Accept/Merge Gate\nThis PR was produced by the dark factory convergence loop, orchestrated by Claude Code.\n\n**Before merging, verify:**\n- [ ] Satisfaction score meets your quality bar\n- [ ] Review latest feedback for residual warnings\n- [ ] Gate 2 NFR findings are acceptable\n- [ ] No unexpected files or dependencies introduced\n\n**To merge:** Approve and merge. The factory branch can then be deleted.\n**To reject:** Close this PR and either adjust scenarios/specs or trigger another crank.\nEOF\n)\" \\\n  --label factory-converged --label accept-merge-gate\n```\n\n### Step 12: Generate PR Review Pack\n\nAfter creating the PR, invoke the `/pr-review-pack` skill to generate the interactive HTML review pack. This is how the human project lead reviews the factory's output \u2014 they review the report, not the code.\n\nThe review pack gives the project lead:\n- Architecture diagram showing which zones were touched\n- Adversarial findings (from Gate 0 agent team) graded by file\n- CI performance with health classification\n- Key decisions with zone-level traceability\n- Convergence result (gate-by-gate status)\n- Post-merge items with code snippets and failure/success scenarios\n- Factory history (iteration timeline, gate findings per iteration)\n\n```\n/pr-review-pack {PR_NUMBER}\n```\n\nThe review pack is the artifact that communicates factory status to the human. Without it, the accept/merge gate is a rubber stamp.\n\n### Step 13: Post-Merge Persistence\n\nAfter the project lead merges the PR:\n\n1. **Persist decisions** to the cumulative log:\n   ```bash\n   python scripts/persist_decisions.py --pr {PR_NUMBER}\n   ```\n   The script extracts decisions from the review pack HTML (or from the JSON intermediate if available via `--data`) and appends them to `docs/decisions/decision_log.json`. It is idempotent \u2014 safe to run multiple times.\n\n2. **Create post-merge issues** (if applicable):\n   ```bash\n   python scripts/create_postmerge_issues.py --pr {PR_NUMBER}\n   ```\n\n3. **Commit and push** the updated decision log:\n   ```bash\n   git add docs/decisions/decision_log.json\n   git commit -m \"decisions: persist PR #{PR_NUMBER} decisions\"\n   git push\n   ```\n\n4. **Delete Codex's remote branch** (cleanup):\n   ```bash\n   git push origin --delete codex-{branch}\n   ```\n\n### Stall Protocol\n\nIf after 3+ iterations:\n- Same scenario fails with same error \u2192 the spec or scenario may need adjustment. Escalate to the project lead.\n- Score oscillates without converging \u2192 architectural issue. Escalate.\n- Gate 0 keeps finding critical issues \u2192 attractor needs stronger constraints. Update `factory_fix.md`.\n\n## Reference Files\n\n- **Attractor prompt**: `.github/codex/prompts/factory_fix.md`\n- **Adversarial review**: `.github/codex/prompts/adversarial_review.md`\n- **Code quality standards**: `docs/code_quality_standards.md`\n- **NFR checks script**: `scripts/nfr_checks.py` (Gate 0 tool agents + Gate 2)\n- **Test quality scanner**: `scripts/check_test_quality.py` (Gate 0 tool agent)\n- **PR review pack skill**: `.claude/skills/pr-review-pack/SKILL.md` (Step 12)\n- **Decision log**: `docs/decisions/decision_log.json` (Step 13, cumulative archive)\n- **Decision persistence**: `scripts/persist_decisions.py` (Step 13)\n- **Specs**: `specs/*.md`\n- **Factory docs**: `docs/dark_factory.md`\n- **Factory architecture**: `docs/factory_architecture.html`\n\n## Operational Knowledge\n\n### Layered Defense Against Gaming\nThe factory's quality defense is layered \u2014 no single gate is sufficient:\n1. **Gate 0 tool agents** (agent team, parallel) \u2014 deterministic checks via vulture, radon, bandit, ruff, and `check_test_quality.py`. Catches dead code, complexity, security issues, lint violations, and obvious vacuous test patterns. Fast, cheap, runs in seconds. Risk: regex/AST-level analysis can be fooled by sophisticated gaming.\n2. **Gate 0 adversarial reviewer** (agent team, parallel with tool agents) \u2014 LLM-based judgment catches subtle gaming, architectural dishonesty, spec violations, and patterns the static tools miss. Reads the full diff against code quality standards and specs.\n3. **Gate 3 holdout scenarios** \u2014 behavioral evaluation against criteria the attractor never sees (ground truth). If the code actually works, gaming doesn't matter.\n\nThe tool agents and semantic reviewer run in parallel at Gate 0 as an agent team. Any CRITICAL finding from any agent stops the pipeline before merge. No single layer is sufficient. Tool agents catch the cheap stuff fast, the adversarial reviewer catches the clever stuff, and holdout scenarios verify actual behavior.\n\n### Iteration \u2192 Commit Model\nEach factory iteration produces ONE commit from Codex (via merge). This provides a clean diff for adversarial review and clear rollback boundaries. The commit message must include the iteration number for traceability.\n\n### CI vs. Orchestrator Roles\nCI (factory.yaml) runs validation-only on every push \u2014 Gates 1, 2, 3 + feedback compilation. CI does NOT drive the convergence loop. Claude Code drives the loop via this skill. CI results are INPUT to orchestration decisions, not orchestration themselves.\n\n**Current CI structure:**\n- `factory-self-test (push)` \u2014 factory script validation\n- `factory-self-test (PR)` \u2014 PR-specific factory validation\n- `factory-loop` \u2014 fallback convergence via Codex API (only with OPENAI_API_KEY)\n- `validate (push)` \u2014 product code validation\n- `validate (PR)` \u2014 PR-specific product validation\n\n**Future consolidation note:** Once the factory runs regularly, `factory-loop` and `validate` could be consolidated into a single workflow with better separation. Current overlap provides coverage redundancy during proof-of-concept.\n\n### NFR Gate Architecture\nGate 2 runs deterministic tool-based checks. Each check follows the pattern:\n1. Run external tool (ruff, radon, vulture, bandit)\n2. Parse output (JSON preferred, text fallback)\n3. Map findings to severity (CRITICAL/WARNING/NIT/INFO)\n4. Return structured findings\n\nAdding a new check: write `check_<name>(repo_root: Path) -> list[NFRFinding]`, register in `NFR_CHECKS` dict. The factory picks it up automatically.\n\n**Important:** All JSON parsing must include fallback handling for decode errors. Silent `pass` on JSONDecodeError hides tool failures \u2014 always emit at least a WARNING finding.\n\n### Gate 2 and LLM-Based Review\nGate 2 should stay deterministic (tool-based). LLM-based review belongs in Gate 0 (adversarial review) and Step 10 (LLM-as-judge). Mixing deterministic and non-deterministic findings in the same gate creates confusion about what's reliable vs. advisory. If LLM-based checks are added, label findings as \"advisory\" and never let them block convergence alone.\n\n### Holdout Stripping Scope\n`strip_holdout.py` removes `/scenarios/` and Makefile scenario targets. It also strips review pack artifacts (`docs/pr_review_pack.html`, `docs/pr_diff_data.json`) \u2014 the attractor has no business seeing adversarial review findings from previous iterations. The attractor's information boundary is: specs + feedback + its own code. Nothing else.\n\n### Git Hooks vs. CI Enforcement\n`.githooks/pre-commit` runs ruff + mypy on staged Python files \u2014 a local speed bump. It is NOT enforced on clean clone; developers must run `make install-hooks`. CI is the enforcement layer. The hook catches issues before they hit CI, saving iteration time. Both exist because they serve different failure modes.\n",
      "base": "---\nname: factory-orchestrate\ndescription: Run the dark factory convergence loop. Use when the user says \"run a factory crank\", \"start the factory\", \"orchestrate a crank\", or similar. Orchestrates Codex via browser, manages holdout isolation, runs validation gates, and performs LLM-as-judge evaluation.\nallowed-tools: Bash, Read, Write, Glob, Grep, Edit\n---\n\n# Dark Factory Orchestration \u2014 Claude Code as Orchestrator\n\nYou are the factory orchestrator. You run the convergence loop that turns specs into working software through iterative AI coding + validation.\n\n## Prerequisites\n\n- Chrome is logged into Codex (ChatGPT Plus account)\n- Repository: `joeyfezster/building_ai_w_ai`\n- Branch to base work on: confirm with user or default to `factory/v1`\n- Satisfaction threshold: confirm with user or default to 80%\n- Max iterations: confirm with user or default to 5\n\n## The Loop\n\nFor each iteration:\n\n### Step 1: Create Factory Branch\n```bash\n# First crank \u2014 create from base branch\ngit checkout -b df-crank-v01-{descriptor} {base_branch}\ngit push -u origin df-crank-v01-{descriptor}\n```\n\nBranch naming: `df-crank-vXX-{descriptor}` where XX is the crank version.\n\n### Step 2: Strip Holdout\n```bash\npython scripts/strip_holdout.py\ngit push\n```\n\nThis deterministically removes `/scenarios/` and comments out scenario Makefile targets. Codex literally cannot see evaluation criteria.\n\nVerify: `ls scenarios/` should fail (directory gone).\n\n### Step 3: Invoke Codex via Browser\n\nOpen the Codex UI in Chrome. Provide:\n- **Repository**: `joeyfezster/building_ai_w_ai`\n- **Base branch**: `df-crank-vXX-{descriptor}` (the stripped branch)\n- **Prompt**: Contents of `.github/codex/prompts/factory_fix.md` + the latest feedback file (`artifacts/factory/feedback_iter_N.md`)\n- **Versions**: 1\n\nCodex will create its own branch (named `codex-...`). Wait for it to finish.\n\n### Step 4: Gate 0 \u2014 Adversarial Code Review (Agent Team)\n\nBefore merging Codex's changes, run a **full adversarial review via agent teams**. This is the first line of defense \u2014 there is no point sending code to CI or later gates if Gate 0 finds critical issues.\n\n1. Fetch Codex's branch: `git fetch origin`\n2. Get the diff: `git diff df-crank-vXX...origin/codex-{branch}`\n\n3. **Spawn the Gate 0 agent team.** Create a team and launch these agents in parallel:\n\n   **Tool agents** (deterministic, Bash-capable \u2014 run ALL simultaneously):\n   | Agent | What It Runs | What It Catches |\n   |-------|-------------|-----------------|\n   | `ruff-agent` | `python scripts/nfr_checks.py --check code_quality` | Lint violations, style, import issues |\n   | `radon-agent` | `python scripts/nfr_checks.py --check complexity` | Cyclomatic complexity > threshold |\n   | `vulture-agent` | `python scripts/nfr_checks.py --check dead_code` | Unreachable code, unused functions |\n   | `bandit-agent` | `python scripts/nfr_checks.py --check security` | Security vulnerabilities |\n   | `test-quality-agent` | `python scripts/check_test_quality.py` | Vacuous tests, stub assertions, mock abuse |\n\n   **Semantic reviewer** (LLM-based, runs in parallel with tool agents):\n   | Agent | What It Reads | What It Catches |\n   |-------|--------------|-----------------|\n   | `adversarial-reviewer` | The diff + `.github/codex/prompts/adversarial_review.md` + `docs/code_quality_standards.md` + `/specs/*.md` | Gaming, architectural dishonesty, spec violations, integration gaps, subtle patterns the tools miss |\n\n4. **Aggregate findings.** Collect all agent outputs. Each finding has a severity: CRITICAL, WARNING, or NIT.\n\n5. **Fail-fast rule:** If **any agent** reports a CRITICAL finding, Gate 0 fails. Do NOT merge. Compile all findings (from all agents) as feedback and loop back to Step 3 with specific remediation instructions.\n\n**If clean or WARNING-only across all agents**: Proceed to Step 5. WARNING findings are tracked \u2014 they feed into the LLM-as-judge evaluation in Step 10.\n\n**Why agent teams, not a single reviewer:** The tool agents catch cheap, obvious violations in seconds (dead code, complexity, security). The semantic reviewer catches subtle gaming and architectural dishonesty. Running them in parallel means Gate 0 is both fast AND thorough. A single reviewer doing everything sequentially is slower and more likely to miss things.\n\n### Step 5: Merge Codex Changes\n```bash\ngit merge origin/codex-{branch} --no-ff -m \"factory: merge codex iteration N\"\n```\n\n### Step 5b: Check CI Results\n\nAfter pushing, check CI results. CI runs Gates 1-3 on every push to factory/** branches. Use CI results as early signal before running gates locally.\n\n```bash\n# Wait for CI to complete (typically 2-5 minutes)\ngh run list --branch df-crank-vXX --limit 3\n\n# Check the PR's checks (if PR exists)\ngh pr checks <PR_NUMBER>\n```\n\n**Known `gh pr checks` behavior:**\n- `gh pr checks` shows checks associated with the PR's **merge ref**, not the branch HEAD directly\n- If a bot (GITHUB_TOKEN) pushes a commit that becomes PR HEAD, GitHub does NOT re-trigger CI workflows (prevents infinite loops). The PR may show \"0 checks\" even though CI ran fine on the previous commit.\n- Workaround: If you see stale/missing checks, use `gh run list --branch <branch>` instead \u2014 this shows actual workflow runs regardless of the merge ref.\n- Commits pushed by workflows using `GITHUB_TOKEN` do not trigger other workflows. This is a GitHub safety measure.\n- If CI results conflict with your local gate results, investigate \u2014 don't just ignore the discrepancy.\n\n**If CI fails**: Use the github.com's copilot's 'explain errors' as initial reference, check logs to validate and make up your own mind, compile actionable feedback (yourself), and loop back to Step 3.\n\n### Step 6: Restore Holdout\n```bash\npython scripts/restore_holdout.py\ngit add scenarios/ Makefile\ngit commit -m \"factory: restore holdout scenarios for evaluation\"\n```\n\n### Step 7: Gate 1 \u2014 Deterministic Validation\n```bash\nmake lint && make typecheck && make test\n```\n\n\"test\" = *should be* the FULL pytest suite, including any tests Codex wrote (already reviewed in Gate 0).\n\n**If fail**: Compile feedback (use this script as aid, but make sure you intervene if the feedback doesn't make sense: `python scripts/compile_feedback.py --iteration N`), loop to Step 3.\n\n### Step 8: Gate 2 \u2014 Non-Functional Requirements\n```bash\nmake nfr-check\n```\n\nThis runs all implemented NFR checks (code quality, complexity, dead code, security).\n\nGate 2 is **non-blocking** but findings are tracked and feed into:\n- The feedback for the next Codex iteration\n- Your LLM-as-judge evaluation in Step 10\n\n### Step 9: Gate 3 \u2014 Behavioral Scenarios\n```bash\npython scripts/run_scenarios.py --timeout 180\n```\n\nProduces `artifacts/factory/scenario_results.json` with satisfaction score.\n\n### Step 10: LLM-as-Judge \u2014 Holistic Evaluation\n\nYou ARE the judge. Don't just check `satisfaction_score >= threshold`. Reason through:\n\n1. **Satisfaction trajectory**: Is the score improving across iterations? Plateaued? Regressing?\n2. **Failure patterns**: Are the same scenarios failing repeatedly? Different ones each time?\n3. **Fix quality**: Do Codex's changes look like real solutions or gaming attempts? (Gate 0 caught the obvious ones, but look for subtle patterns across iterations)\n4. **Gate 2 NFR findings**: Even though non-blocking, are there concerning patterns? Growing complexity? Dropping coverage?\n5. **Systemic issues**: Is there something the score doesn't capture? An architectural problem that will cause future failures?\n6. **Documentation currency**: Did this iteration's changes affect documented behavior? Check: Are specs in `/specs/` still accurate? Does the README reflect current state? Are factory docs (`dark_factory.md`, `code_quality_standards.md`) still correct? Stale documentation is technical debt \u2014 flag it in feedback if needed.\n\n**If satisfied**: Proceed to Step 11.\n**If not satisfied**: Compile feedback with your holistic assessment, loop to Step 3.\n\n### Step 11: Create PR (Accept/Merge Gate)\n```bash\ngh pr create \\\n  --title \"[Factory] df-crank-vXX converged at {score}%\" \\\n  --body \"$(cat <<'EOF'\n## Dark Factory \u2014 Converged\n\n**Satisfaction score: {score}%**\n**Iterations: {N}**\n**Gate 2 NFR status: {summary}**\n\n### Accept/Merge Gate\nThis PR was produced by the dark factory convergence loop, orchestrated by Claude Code.\n\n**Before merging, verify:**\n- [ ] Satisfaction score meets your quality bar\n- [ ] Review latest feedback for residual warnings\n- [ ] Gate 2 NFR findings are acceptable\n- [ ] No unexpected files or dependencies introduced\n\n**To merge:** Approve and merge. The factory branch can then be deleted.\n**To reject:** Close this PR and either adjust scenarios/specs or trigger another crank.\nEOF\n)\" \\\n  --label factory-converged --label accept-merge-gate\n```\n\n### Step 12: Generate PR Review Pack\n\nAfter creating the PR, invoke the `/pr-review-pack` skill to generate the interactive HTML review pack. This is how the human project lead reviews the factory's output \u2014 they review the report, not the code.\n\nThe review pack gives the project lead:\n- Architecture diagram showing which zones were touched\n- Adversarial findings (from Gate 0 agent team) graded by file\n- CI performance with health classification\n- Key decisions with zone-level traceability\n- Convergence result (gate-by-gate status)\n- Post-merge items with code snippets and failure/success scenarios\n- Factory history (iteration timeline, gate findings per iteration)\n\n```\n/pr-review-pack {PR_NUMBER}\n```\n\nThe review pack is the artifact that communicates factory status to the human. Without it, the accept/merge gate is a rubber stamp.\n\n### Stall Protocol\n\nIf after 3+ iterations:\n- Same scenario fails with same error \u2192 the spec or scenario may need adjustment. Escalate to the project lead.\n- Score oscillates without converging \u2192 architectural issue. Escalate.\n- Gate 0 keeps finding critical issues \u2192 attractor needs stronger constraints. Update `factory_fix.md`.\n\n## Reference Files\n\n- **Attractor prompt**: `.github/codex/prompts/factory_fix.md`\n- **Adversarial review**: `.github/codex/prompts/adversarial_review.md`\n- **Code quality standards**: `docs/code_quality_standards.md`\n- **NFR checks script**: `scripts/nfr_checks.py` (Gate 0 tool agents + Gate 2)\n- **Test quality scanner**: `scripts/check_test_quality.py` (Gate 0 tool agent)\n- **PR review pack skill**: `.claude/skills/pr-review-pack/SKILL.md` (Step 12)\n- **Specs**: `specs/*.md`\n- **Factory docs**: `docs/dark_factory.md`\n- **Factory architecture**: `docs/factory_architecture.html`\n\n## Operational Knowledge\n\n### Layered Defense Against Gaming\nThe factory's quality defense is layered \u2014 no single gate is sufficient:\n1. **Gate 0 tool agents** (agent team, parallel) \u2014 deterministic checks via vulture, radon, bandit, ruff, and `check_test_quality.py`. Catches dead code, complexity, security issues, lint violations, and obvious vacuous test patterns. Fast, cheap, runs in seconds. Risk: regex/AST-level analysis can be fooled by sophisticated gaming.\n2. **Gate 0 adversarial reviewer** (agent team, parallel with tool agents) \u2014 LLM-based judgment catches subtle gaming, architectural dishonesty, spec violations, and patterns the static tools miss. Reads the full diff against code quality standards and specs.\n3. **Gate 3 holdout scenarios** \u2014 behavioral evaluation against criteria the attractor never sees (ground truth). If the code actually works, gaming doesn't matter.\n\nThe tool agents and semantic reviewer run in parallel at Gate 0 as an agent team. Any CRITICAL finding from any agent stops the pipeline before merge. No single layer is sufficient. Tool agents catch the cheap stuff fast, the adversarial reviewer catches the clever stuff, and holdout scenarios verify actual behavior.\n\n### Iteration \u2192 Commit Model\nEach factory iteration produces ONE commit from Codex (via merge). This provides a clean diff for adversarial review and clear rollback boundaries. The commit message must include the iteration number for traceability.\n\n### CI vs. Orchestrator Roles\nCI (factory.yaml) runs validation-only on every push \u2014 Gates 1, 2, 3 + feedback compilation. CI does NOT drive the convergence loop. Claude Code drives the loop via this skill. CI results are INPUT to orchestration decisions, not orchestration themselves.\n\n**Current CI structure:**\n- `factory-self-test (push)` \u2014 factory script validation\n- `factory-self-test (PR)` \u2014 PR-specific factory validation\n- `factory-loop` \u2014 fallback convergence via Codex API (only with OPENAI_API_KEY)\n- `validate (push)` \u2014 product code validation\n- `validate (PR)` \u2014 PR-specific product validation\n\n**Future consolidation note:** Once the factory runs regularly, `factory-loop` and `validate` could be consolidated into a single workflow with better separation. Current overlap provides coverage redundancy during proof-of-concept.\n\n### NFR Gate Architecture\nGate 2 runs deterministic tool-based checks. Each check follows the pattern:\n1. Run external tool (ruff, radon, vulture, bandit)\n2. Parse output (JSON preferred, text fallback)\n3. Map findings to severity (CRITICAL/WARNING/NIT/INFO)\n4. Return structured findings\n\nAdding a new check: write `check_<name>(repo_root: Path) -> list[NFRFinding]`, register in `NFR_CHECKS` dict. The factory picks it up automatically.\n\n**Important:** All JSON parsing must include fallback handling for decode errors. Silent `pass` on JSONDecodeError hides tool failures \u2014 always emit at least a WARNING finding.\n\n### Gate 2 and LLM-Based Review\nGate 2 should stay deterministic (tool-based). LLM-based review belongs in Gate 0 (adversarial review) and Step 10 (LLM-as-judge). Mixing deterministic and non-deterministic findings in the same gate creates confusion about what's reliable vs. advisory. If LLM-based checks are added, label findings as \"advisory\" and never let them block convergence alone.\n\n### Holdout Stripping Scope\n`strip_holdout.py` removes `/scenarios/` and Makefile scenario targets. It also strips review pack artifacts (`docs/pr_review_pack.html`, `docs/pr_diff_data.json`) \u2014 the attractor has no business seeing adversarial review findings from previous iterations. The attractor's information boundary is: specs + feedback + its own code. Nothing else.\n\n### Git Hooks vs. CI Enforcement\n`.githooks/pre-commit` runs ruff + mypy on staged Python files \u2014 a local speed bump. It is NOT enforced on clean clone; developers must run `make install-hooks`. CI is the enforcement layer. The hook catches issues before they hit CI, saving iteration time. Both exist because they serve different failure modes.\n"
    },
    ".github/codex/prompts/factory_fix.md": {
      "additions": 10,
      "deletions": 2,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/.github/codex/prompts/factory_fix.md b/.github/codex/prompts/factory_fix.md\nindex 09fe049..bfc656a 100644\n--- a/.github/codex/prompts/factory_fix.md\n+++ b/.github/codex/prompts/factory_fix.md\n@@ -15,8 +15,17 @@ Read the component specifications in `/specs/` to understand what the system sho\n Read the feedback file for this iteration to understand what's broken:\n - `artifacts/factory/feedback_iter_*.md` \u2014 latest feedback with full error output\n \n+Read the decision log for architectural context from previous cranks:\n+- `docs/decisions/decision_log.json` \u2014 cumulative log of accepted architectural decisions\n+- These decisions were reviewed and approved by the project lead. Follow them unless specs or feedback explicitly contradict them.\n+\n ## Your Constraints\n \n+**NEVER modify or delete these files** (read-only context for you):\n+- `/specs/` \u2014 your requirements, read them\n+- `/docs/decisions/` \u2014 decision log, read for architectural context\n+- `/agents/` \u2014 pre-factory reference\n+\n **NEVER read, modify, or delete these files:**\n - Anything in `/scenarios/` (you should not even see this directory)\n - `/scripts/run_scenarios.py`\n@@ -24,12 +33,11 @@ Read the feedback file for this iteration to understand what's broken:\n - `/.github/workflows/factory.yaml`\n - `/.github/codex/prompts/factory_fix.md` (this file)\n - `/CLAUDE.md`\n-- `/specs/` (read-only \u2014 these are your requirements)\n-- `/agents/` (pre-factory reference, not product code)\n - `/scripts/strip_holdout.py` (holdout isolation gate)\n - `/scripts/restore_holdout.py` (holdout restoration)\n - `/scripts/nfr_checks.py` (Gate 2 NFR checker)\n - `/scripts/check_test_quality.py` (anti-vacuous scanner)\n+- `/scripts/persist_decisions.py` (decision persistence script)\n \n **DO modify** source code in:\n - `src/` \u2014 all Python source\n",
      "raw": "# Factory Fix \u2014 Codex Prompt Template\n\nYou are the coding agent (Attractor) in a dark factory convergence loop. Your job is to fix failures identified by the factory's validation system.\n\n## Your Context\n\nRead the component specifications in `/specs/` to understand what the system should do:\n- `specs/system.md` \u2014 overall system architecture\n- `specs/env.md` \u2014 MiniPong environment requirements\n- `specs/rl.md` \u2014 DQN algorithm requirements\n- `specs/training.md` \u2014 training pipeline requirements\n- `specs/dashboard.md` \u2014 dashboard requirements\n- `specs/proof.md` \u2014 learning proof and video proof requirements\n\nRead the feedback file for this iteration to understand what's broken:\n- `artifacts/factory/feedback_iter_*.md` \u2014 latest feedback with full error output\n\nRead the decision log for architectural context from previous cranks:\n- `docs/decisions/decision_log.json` \u2014 cumulative log of accepted architectural decisions\n- These decisions were reviewed and approved by the project lead. Follow them unless specs or feedback explicitly contradict them.\n\n## Your Constraints\n\n**NEVER modify or delete these files** (read-only context for you):\n- `/specs/` \u2014 your requirements, read them\n- `/docs/decisions/` \u2014 decision log, read for architectural context\n- `/agents/` \u2014 pre-factory reference\n\n**NEVER read, modify, or delete these files:**\n- Anything in `/scenarios/` (you should not even see this directory)\n- `/scripts/run_scenarios.py`\n- `/scripts/compile_feedback.py`\n- `/.github/workflows/factory.yaml`\n- `/.github/codex/prompts/factory_fix.md` (this file)\n- `/CLAUDE.md`\n- `/scripts/strip_holdout.py` (holdout isolation gate)\n- `/scripts/restore_holdout.py` (holdout restoration)\n- `/scripts/nfr_checks.py` (Gate 2 NFR checker)\n- `/scripts/check_test_quality.py` (anti-vacuous scanner)\n- `/scripts/persist_decisions.py` (decision persistence script)\n\n**DO modify** source code in:\n- `src/` \u2014 all Python source\n- `tests/` \u2014 test files\n- `configs/` \u2014 configuration files\n- `Makefile` \u2014 build targets\n- `requirements.in` / `requirements-dev.in` \u2014 dependencies\n- `infra/docker/` \u2014 Dockerfiles\n- `pyproject.toml` \u2014 project configuration\n\n## Validation Guidelines\n\nBefore considering any change complete, ensure:\n\n### Hard Constraints\n- No proprietary ROM dependencies \u2014 MiniPong is self-contained\n- Policy consumes pixels only (84\u00d784 uint8 observations)\n- `make validate` must pass (lint + typecheck + test + docker + env-smoke)\n- `make verify-learning` must pass for any training-related change\n\n### Definition of Done\n- Functional requirements from `/specs/` are implemented\n- Architectural consistency maintained (no ad-hoc patterns)\n- Integration checks pass end-to-end\n- Required artifacts generated and linked (checkpoints, metrics, videos)\n\n### Quality Checklist\n- [ ] `make lint` passes (ruff check)\n- [ ] `make typecheck` passes (mypy src)\n- [ ] `make test` passes (pytest)\n- [ ] No new dead imports or unused code introduced\n- [ ] Changes are minimal and surgical \u2014 fix what's broken, don't refactor\n\n## Anti-Gaming Rules\n\nYou are evaluated by an external holdout system you cannot see. These rules exist because the factory has adversarial review \u2014 attempts to game the system will be caught and will waste iterations.\n\n### Tests Must Be Real\n- **No vacuous tests.** Every test must exercise real behavior through real code paths. A test that passes by construction proves nothing.\n- **No mocking the system under test.** Mocks are for isolating external dependencies (network, filesystem, third-party APIs) \u2014 never for bypassing the logic you're supposed to be testing.\n- **No stub implementations.** Functions must contain real logic, not `return True`, `return 0`, `pass`, or hardcoded lookup tables that happen to match test cases.\n- **No patching away the thing being tested.** If a test patches the function it claims to test, it tests nothing.\n\n### Implementations Must Be General\n- **No hardcoded special cases** that coincidentally pass known test inputs. Example: `is_prime(x): return x in {2, 3, 5, 7, 11, 13}` is not a prime checker.\n- **No output-matching shortcuts.** If a function is supposed to compute something, it must actually compute it \u2014 not return a cached/hardcoded result.\n- **No overfitting to error messages.** If a scenario fails with a specific assertion, fix the root cause \u2014 don't just make that specific assertion pass while breaking the general case.\n\n### Integration Must Be Honest\n- If a test file requires imports from `src/`, those imports must exercise the real module, not a local redefinition.\n- Configuration files must reflect actual runtime parameters, not test-only shortcuts.\n- Docker builds must include all real dependencies \u2014 don't skip packages to speed up builds if the code needs them at runtime.\n\n## Your Approach\n\n1. Read the latest feedback file to understand all failures\n2. Read the relevant specs to understand expected behavior\n3. Fix failures in priority order:\n   - Import errors and missing modules first\n   - File/artifact production issues next\n   - Behavioral correctness last\n4. Validate locally: run `make lint && make typecheck` before finishing\n5. Do NOT add new test files that duplicate scenario evaluation logic\n6. Do NOT refactor code that isn't related to the current failures\n\n## Success Criteria\n\nThe factory will re-run validation after your changes. Your goal is to increase the satisfaction score (fraction of scenarios passing). Aim for convergence, not perfection in a single iteration.\n",
      "base": "# Factory Fix \u2014 Codex Prompt Template\n\nYou are the coding agent (Attractor) in a dark factory convergence loop. Your job is to fix failures identified by the factory's validation system.\n\n## Your Context\n\nRead the component specifications in `/specs/` to understand what the system should do:\n- `specs/system.md` \u2014 overall system architecture\n- `specs/env.md` \u2014 MiniPong environment requirements\n- `specs/rl.md` \u2014 DQN algorithm requirements\n- `specs/training.md` \u2014 training pipeline requirements\n- `specs/dashboard.md` \u2014 dashboard requirements\n- `specs/proof.md` \u2014 learning proof and video proof requirements\n\nRead the feedback file for this iteration to understand what's broken:\n- `artifacts/factory/feedback_iter_*.md` \u2014 latest feedback with full error output\n\n## Your Constraints\n\n**NEVER read, modify, or delete these files:**\n- Anything in `/scenarios/` (you should not even see this directory)\n- `/scripts/run_scenarios.py`\n- `/scripts/compile_feedback.py`\n- `/.github/workflows/factory.yaml`\n- `/.github/codex/prompts/factory_fix.md` (this file)\n- `/CLAUDE.md`\n- `/specs/` (read-only \u2014 these are your requirements)\n- `/agents/` (pre-factory reference, not product code)\n- `/scripts/strip_holdout.py` (holdout isolation gate)\n- `/scripts/restore_holdout.py` (holdout restoration)\n- `/scripts/nfr_checks.py` (Gate 2 NFR checker)\n- `/scripts/check_test_quality.py` (anti-vacuous scanner)\n\n**DO modify** source code in:\n- `src/` \u2014 all Python source\n- `tests/` \u2014 test files\n- `configs/` \u2014 configuration files\n- `Makefile` \u2014 build targets\n- `requirements.in` / `requirements-dev.in` \u2014 dependencies\n- `infra/docker/` \u2014 Dockerfiles\n- `pyproject.toml` \u2014 project configuration\n\n## Validation Guidelines\n\nBefore considering any change complete, ensure:\n\n### Hard Constraints\n- No proprietary ROM dependencies \u2014 MiniPong is self-contained\n- Policy consumes pixels only (84\u00d784 uint8 observations)\n- `make validate` must pass (lint + typecheck + test + docker + env-smoke)\n- `make verify-learning` must pass for any training-related change\n\n### Definition of Done\n- Functional requirements from `/specs/` are implemented\n- Architectural consistency maintained (no ad-hoc patterns)\n- Integration checks pass end-to-end\n- Required artifacts generated and linked (checkpoints, metrics, videos)\n\n### Quality Checklist\n- [ ] `make lint` passes (ruff check)\n- [ ] `make typecheck` passes (mypy src)\n- [ ] `make test` passes (pytest)\n- [ ] No new dead imports or unused code introduced\n- [ ] Changes are minimal and surgical \u2014 fix what's broken, don't refactor\n\n## Anti-Gaming Rules\n\nYou are evaluated by an external holdout system you cannot see. These rules exist because the factory has adversarial review \u2014 attempts to game the system will be caught and will waste iterations.\n\n### Tests Must Be Real\n- **No vacuous tests.** Every test must exercise real behavior through real code paths. A test that passes by construction proves nothing.\n- **No mocking the system under test.** Mocks are for isolating external dependencies (network, filesystem, third-party APIs) \u2014 never for bypassing the logic you're supposed to be testing.\n- **No stub implementations.** Functions must contain real logic, not `return True`, `return 0`, `pass`, or hardcoded lookup tables that happen to match test cases.\n- **No patching away the thing being tested.** If a test patches the function it claims to test, it tests nothing.\n\n### Implementations Must Be General\n- **No hardcoded special cases** that coincidentally pass known test inputs. Example: `is_prime(x): return x in {2, 3, 5, 7, 11, 13}` is not a prime checker.\n- **No output-matching shortcuts.** If a function is supposed to compute something, it must actually compute it \u2014 not return a cached/hardcoded result.\n- **No overfitting to error messages.** If a scenario fails with a specific assertion, fix the root cause \u2014 don't just make that specific assertion pass while breaking the general case.\n\n### Integration Must Be Honest\n- If a test file requires imports from `src/`, those imports must exercise the real module, not a local redefinition.\n- Configuration files must reflect actual runtime parameters, not test-only shortcuts.\n- Docker builds must include all real dependencies \u2014 don't skip packages to speed up builds if the code needs them at runtime.\n\n## Your Approach\n\n1. Read the latest feedback file to understand all failures\n2. Read the relevant specs to understand expected behavior\n3. Fix failures in priority order:\n   - Import errors and missing modules first\n   - File/artifact production issues next\n   - Behavioral correctness last\n4. Validate locally: run `make lint && make typecheck` before finishing\n5. Do NOT add new test files that duplicate scenario evaluation logic\n6. Do NOT refactor code that isn't related to the current failures\n\n## Success Criteria\n\nThe factory will re-run validation after your changes. Your goal is to increase the satisfaction score (fraction of scenarios passing). Aim for convergence, not perfection in a single iteration.\n"
    },
    "CLAUDE.md": {
      "additions": 3,
      "deletions": 0,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/CLAUDE.md b/CLAUDE.md\nindex 8c21981..83b26be 100644\n--- a/CLAUDE.md\n+++ b/CLAUDE.md\n@@ -42,6 +42,8 @@ The following files are **never touched by the Attractor (Codex)**. They are fac\n - `/scripts/check_test_quality.py` \u2014 Gate 0 test quality scanner\n - `/.github/codex/prompts/adversarial_review.md` \u2014 Gate 0 adversarial review checklist\n - `/docs/code_quality_standards.md` \u2014 universal quality standards\n+- `/docs/decisions/` \u2014 cumulative decision log (Codex reads but never modifies)\n+- `/scripts/persist_decisions.py` \u2014 decision persistence script\n - `/CLAUDE.md` \u2014 this file\n \n ## Code Quality Standards\n@@ -64,6 +66,7 @@ make compile-feedback      # compile validation results into feedback markdown\n make nfr-check             # run Gate 2 NFR checks (code quality, complexity, dead code, security)\n make factory-local         # run one factory iteration locally (Gate 1 \u2192 Gate 2 \u2192 Gate 3 \u2192 feedback)\n make factory-status        # show current iteration count and satisfaction score\n+make persist-decisions PR=6  # persist PR decisions to cumulative log\n ```\n \n ## Human Decision Log\n",
      "raw": "# MiniPong RL System \u2014 Dark Factory\n\nEnd-to-end proof that a reinforcement learning agent can learn Pong from pixels, built entirely by AI agents orchestrated through a convergence loop.\n\n## First-Time Setup\n\nAfter cloning, run these commands immediately:\n\n```bash\nmake install-hooks    # REQUIRED: sets up git hooks (ruff + mypy on every commit)\nmake deps             # install Python dependencies\n```\n\n`make install-hooks` is non-negotiable \u2014 it's the local quality gate that catches issues before they hit CI. Without it, lint/typecheck failures only surface in CI, wasting iteration time.\n\n## Operating Model\n\nThis repo is built by a **dark factory loop**, not by humans writing code. Code is treated as opaque weights \u2014 correctness is inferred exclusively from externally observable behavior, never from source inspection.\n\nThe loop: **Seed \u2192 Agent \u2192 Validate \u2192 Feedback \u2192 Repeat until satisfied.**\n\n## Source of Truth\n\n- `/specs/` \u2014 Component specifications. This is what the coding agent reads. The specs define what the system should do.\n- `/scenarios/` \u2014 Behavioral holdout evaluation criteria. These are what the system is evaluated against. **Scenarios must NEVER be modified by the coding agent (Codex).** They are the holdout set \u2014 the agent never sees its own evaluation criteria.\n- `/docs/dark_factory.md` \u2014 Full factory documentation: how the loop works, how to trigger it, how to write scenarios, when to escalate.\n\n## Factory-Protected Files\n\nThe following files are **never touched by the Attractor (Codex)**. They are factory infrastructure, not product code. The Codex-facing version of this list lives in `.github/codex/prompts/factory_fix.md` \u2014 keep both in sync when adding protected files.\n\n- `/scenarios/` \u2014 holdout evaluation criteria\n- `/scripts/run_scenarios.py` \u2014 scenario evaluation runner\n- `/scripts/compile_feedback.py` \u2014 feedback compiler\n- `/.github/workflows/factory.yaml` \u2014 factory orchestrator\n- `/.github/codex/prompts/factory_fix.md` \u2014 Codex prompt template\n- `/specs/` \u2014 component specifications (read-only for Codex)\n- `/agents/` \u2014 pre-factory agent definitions (reference only)\n- `/scripts/strip_holdout.py` \u2014 holdout stripping script (isolation gate)\n- `/scripts/restore_holdout.py` \u2014 holdout restoration script\n- `/scripts/nfr_checks.py` \u2014 Gate 2 NFR checker\n- `/scripts/check_test_quality.py` \u2014 Gate 0 test quality scanner\n- `/.github/codex/prompts/adversarial_review.md` \u2014 Gate 0 adversarial review checklist\n- `/docs/code_quality_standards.md` \u2014 universal quality standards\n- `/docs/decisions/` \u2014 cumulative decision log (Codex reads but never modifies)\n- `/scripts/persist_decisions.py` \u2014 decision persistence script\n- `/CLAUDE.md` \u2014 this file\n\n## Code Quality Standards\n\nAll code written in this repository \u2014 by Codex, Claude Code, or humans \u2014 must follow the standards in `docs/code_quality_standards.md`. This includes:\n- Anti-vacuous test rules (no mocking the system under test, no stub assertions)\n- Anti-gaming rules (no hardcoded lookup tables, no overfitting)\n- Implementation honesty (real imports, real configs, real dependencies)\n- Test hygiene and quality gates\n\nThese standards are enforced by Gate 0 (adversarial review), Gate 1 (lint/typecheck/test), Gate 2 (NFR checks), and the LLM-as-judge.\n\n## Quick Commands\n\n```bash\nmake install-hooks         # set up git hooks (ruff + mypy on every commit, no virtualenv needed)\nmake validate              # lint + typecheck + test + docker-build + docker-smoke + env-smoke\nmake run-scenarios         # run holdout scenario evaluation\nmake compile-feedback      # compile validation results into feedback markdown\nmake nfr-check             # run Gate 2 NFR checks (code quality, complexity, dead code, security)\nmake factory-local         # run one factory iteration locally (Gate 1 \u2192 Gate 2 \u2192 Gate 3 \u2192 feedback)\nmake factory-status        # show current iteration count and satisfaction score\nmake persist-decisions PR=6  # persist PR decisions to cumulative log\n```\n\n## Human Decision Log\n\n- `/ProjectLeadAsks.md` \u2014 Open questions and decisions requiring the project lead's input. **Check this file at every session start.** Update it when questions are resolved or new ones arise. This file survives context compaction \u2014 it's the canonical list of what's pending.\n\n## Stack\n\n- Python 3.12, pip-tools for dependency management\n- PyTorch, Gymnasium, NumPy for RL\n- ruff + mypy + pytest for quality\n- GitHub Actions for CI and validation\n- OpenAI Codex as the non-interactive coding agent (attractor)\n- Claude Code as factory orchestrator (skill: `/factory-orchestrate`)\n- PR review pack generator (skill: `/pr-review-pack`) \u2014 `.claude/skills/pr-review-pack/` contains the review pack generation skill with template, scripts, and reference docs\n",
      "base": "# MiniPong RL System \u2014 Dark Factory\n\nEnd-to-end proof that a reinforcement learning agent can learn Pong from pixels, built entirely by AI agents orchestrated through a convergence loop.\n\n## First-Time Setup\n\nAfter cloning, run these commands immediately:\n\n```bash\nmake install-hooks    # REQUIRED: sets up git hooks (ruff + mypy on every commit)\nmake deps             # install Python dependencies\n```\n\n`make install-hooks` is non-negotiable \u2014 it's the local quality gate that catches issues before they hit CI. Without it, lint/typecheck failures only surface in CI, wasting iteration time.\n\n## Operating Model\n\nThis repo is built by a **dark factory loop**, not by humans writing code. Code is treated as opaque weights \u2014 correctness is inferred exclusively from externally observable behavior, never from source inspection.\n\nThe loop: **Seed \u2192 Agent \u2192 Validate \u2192 Feedback \u2192 Repeat until satisfied.**\n\n## Source of Truth\n\n- `/specs/` \u2014 Component specifications. This is what the coding agent reads. The specs define what the system should do.\n- `/scenarios/` \u2014 Behavioral holdout evaluation criteria. These are what the system is evaluated against. **Scenarios must NEVER be modified by the coding agent (Codex).** They are the holdout set \u2014 the agent never sees its own evaluation criteria.\n- `/docs/dark_factory.md` \u2014 Full factory documentation: how the loop works, how to trigger it, how to write scenarios, when to escalate.\n\n## Factory-Protected Files\n\nThe following files are **never touched by the Attractor (Codex)**. They are factory infrastructure, not product code. The Codex-facing version of this list lives in `.github/codex/prompts/factory_fix.md` \u2014 keep both in sync when adding protected files.\n\n- `/scenarios/` \u2014 holdout evaluation criteria\n- `/scripts/run_scenarios.py` \u2014 scenario evaluation runner\n- `/scripts/compile_feedback.py` \u2014 feedback compiler\n- `/.github/workflows/factory.yaml` \u2014 factory orchestrator\n- `/.github/codex/prompts/factory_fix.md` \u2014 Codex prompt template\n- `/specs/` \u2014 component specifications (read-only for Codex)\n- `/agents/` \u2014 pre-factory agent definitions (reference only)\n- `/scripts/strip_holdout.py` \u2014 holdout stripping script (isolation gate)\n- `/scripts/restore_holdout.py` \u2014 holdout restoration script\n- `/scripts/nfr_checks.py` \u2014 Gate 2 NFR checker\n- `/scripts/check_test_quality.py` \u2014 Gate 0 test quality scanner\n- `/.github/codex/prompts/adversarial_review.md` \u2014 Gate 0 adversarial review checklist\n- `/docs/code_quality_standards.md` \u2014 universal quality standards\n- `/CLAUDE.md` \u2014 this file\n\n## Code Quality Standards\n\nAll code written in this repository \u2014 by Codex, Claude Code, or humans \u2014 must follow the standards in `docs/code_quality_standards.md`. This includes:\n- Anti-vacuous test rules (no mocking the system under test, no stub assertions)\n- Anti-gaming rules (no hardcoded lookup tables, no overfitting)\n- Implementation honesty (real imports, real configs, real dependencies)\n- Test hygiene and quality gates\n\nThese standards are enforced by Gate 0 (adversarial review), Gate 1 (lint/typecheck/test), Gate 2 (NFR checks), and the LLM-as-judge.\n\n## Quick Commands\n\n```bash\nmake install-hooks         # set up git hooks (ruff + mypy on every commit, no virtualenv needed)\nmake validate              # lint + typecheck + test + docker-build + docker-smoke + env-smoke\nmake run-scenarios         # run holdout scenario evaluation\nmake compile-feedback      # compile validation results into feedback markdown\nmake nfr-check             # run Gate 2 NFR checks (code quality, complexity, dead code, security)\nmake factory-local         # run one factory iteration locally (Gate 1 \u2192 Gate 2 \u2192 Gate 3 \u2192 feedback)\nmake factory-status        # show current iteration count and satisfaction score\n```\n\n## Human Decision Log\n\n- `/ProjectLeadAsks.md` \u2014 Open questions and decisions requiring the project lead's input. **Check this file at every session start.** Update it when questions are resolved or new ones arise. This file survives context compaction \u2014 it's the canonical list of what's pending.\n\n## Stack\n\n- Python 3.12, pip-tools for dependency management\n- PyTorch, Gymnasium, NumPy for RL\n- ruff + mypy + pytest for quality\n- GitHub Actions for CI and validation\n- OpenAI Codex as the non-interactive coding agent (attractor)\n- Claude Code as factory orchestrator (skill: `/factory-orchestrate`)\n- PR review pack generator (skill: `/pr-review-pack`) \u2014 `.claude/skills/pr-review-pack/` contains the review pack generation skill with template, scripts, and reference docs\n"
    },
    "Makefile": {
      "additions": 4,
      "deletions": 1,
      "status": "modified",
      "binary": false,
      "diff": "diff --git a/Makefile b/Makefile\nindex 57d93ed..edf1781 100644\n--- a/Makefile\n+++ b/Makefile\n@@ -1,4 +1,4 @@\n-.PHONY: deps install-hooks lint typecheck test docker-build docker-smoke whitepapers-acquire whitepapers-verify env-smoke train-smoke eval-smoke verify-learning dashboard validate run-scenarios compile-feedback nfr-check factory-local factory-status\n+.PHONY: deps install-hooks lint typecheck test docker-build docker-smoke whitepapers-acquire whitepapers-verify env-smoke train-smoke eval-smoke verify-learning dashboard validate run-scenarios compile-feedback nfr-check factory-local factory-status persist-decisions\n \n deps:\n \tpip-compile requirements.in\n@@ -95,6 +95,9 @@ factory-local: ## Run one factory iteration locally (Gate 1 \u2192 Gate 2 \u2192 Gate\n \t@echo \"=== Factory iteration complete ===\"\n \t@make factory-status\n \n+persist-decisions: ## Persist PR decisions to cumulative log (usage: make persist-decisions PR=6)\n+\tpython scripts/persist_decisions.py --pr $(PR)\n+\n factory-status: ## Show current iteration count and satisfaction score\n \t@echo \"--- Factory Status ---\"\n \t@if [ -f artifacts/factory/iteration_count.txt ]; then \\\n",
      "raw": ".PHONY: deps install-hooks lint typecheck test docker-build docker-smoke whitepapers-acquire whitepapers-verify env-smoke train-smoke eval-smoke verify-learning dashboard validate run-scenarios compile-feedback nfr-check factory-local factory-status persist-decisions\n\ndeps:\n\tpip-compile requirements.in\n\tpip-compile requirements-dev.in\n\tpip install -r requirements.txt -r requirements-dev.txt\n\ninstall-hooks: ## Set up git hooks (ruff + mypy on every commit, no virtualenv needed)\n\tgit config core.hooksPath .githooks\n\t@echo \"\u2705 Git hooks installed from .githooks/\"\n\n# \u2500\u2500 Quality \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nlint:\n\truff check .\n\ntypecheck:\n\tmypy src\n\ntest:\n\tpytest -q\n\n# \u2500\u2500 Docker \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndocker-build:\n\tdocker build -f infra/docker/Dockerfile.train --build-arg BASE_IMAGE=python:3.12-slim -t minipong-train .\n\tdocker build -f infra/docker/Dockerfile.demo -t minipong-demo .\n\ndocker-smoke:\n\tdocker run --rm minipong-train python -m src.train.train_dqn --help\n\tdocker run --rm minipong-demo python -m src.train.record_video --help\n\n# \u2500\u2500 Whitepapers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nwhitepapers-acquire:\n\tpython scripts/acquire_whitepapers.py\n\nwhitepapers-verify:\n\tpython scripts/verify_whitepapers.py\n\n# \u2500\u2500 Environment \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nenv-smoke:\n\tpython -c \"from src.envs.minipong import MiniPongEnv; env=MiniPongEnv(); obs,_=env.reset(seed=0); assert obs.dtype.name=='uint8'; print(obs.shape)\"\n\n# \u2500\u2500 Training \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntrain-smoke:\n\tpython -m src.train.train_dqn --config configs/dqn_minipong.yaml --run-id smoke_run\n\neval-smoke:\n\tpython -m src.train.evaluate --run-id smoke_run --episodes 2 --seeds 1 2\n\nverify-learning:\n\tpython -m src.train.verify_learning --run-id smoke_run --min-return-gain -0.1 --min-hits-gain -0.1\n\n# \u2500\u2500 Dashboard \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndashboard:\n\tstreamlit run src/dashboard/app.py\n\n# \u2500\u2500 Validation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nvalidate: lint typecheck test docker-build docker-smoke env-smoke whitepapers-verify\n\n# \u2500\u2500 Dark Factory \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nrun-scenarios: ## Run holdout scenario evaluation\n\tpython scripts/run_scenarios.py\n\ncompile-feedback: ## Compile validation results into feedback markdown\n\tpython scripts/compile_feedback.py\n\nnfr-check: ## Run Gate 2 NFR checks (non-blocking quality analysis)\n\t@mkdir -p artifacts/factory\n\tpython scripts/nfr_checks.py --output artifacts/factory/nfr_results.json\n\tpython scripts/nfr_checks.py\n\nfactory-local: ## Run one factory iteration locally (Gate 1 \u2192 Gate 2 \u2192 Gate 3 \u2192 feedback)\n\t@mkdir -p artifacts/factory\n\t@echo \"=== Gate 1: lint + typecheck + test ===\"\n\t@make lint 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tLINT_EXIT=$$?; \\\n\tmake typecheck 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tTYPE_EXIT=$$?; \\\n\tmake test 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tTEST_EXIT=$$?; \\\n\tif [ $$LINT_EXIT -ne 0 ] || [ $$TYPE_EXIT -ne 0 ] || [ $$TEST_EXIT -ne 0 ]; then \\\n\t\techo \"Gate 1 FAILED \u2014 skipping Gates 2-3\"; \\\n\t\techo '{\"total\":0,\"passed\":0,\"failed\":0,\"skipped\":0,\"satisfaction_score\":0.0,\"results\":[],\"timestamp\":\"N/A\",\"gate1_failed\":true}' > artifacts/factory/scenario_results.json; \\\n\telse \\\n\t\techo \"\"; \\\n\t\techo \"=== Gate 2: NFR checks (non-blocking) ===\"; \\\n\t\tmake nfr-check 2>&1 | tee -a artifacts/factory/ci_output.log || true; \\\n\t\techo \"\"; \\\n\t\techo \"=== Gate 3: Behavioral scenarios ===\"; \\\n\t\tpython scripts/run_scenarios.py --timeout 180 || true; \\\n\tfi\n\t@echo \"\"\n\t@echo \"=== Compiling feedback ===\"\n\t@python scripts/compile_feedback.py\n\t@echo \"\"\n\t@echo \"=== Factory iteration complete ===\"\n\t@make factory-status\n\npersist-decisions: ## Persist PR decisions to cumulative log (usage: make persist-decisions PR=6)\n\tpython scripts/persist_decisions.py --pr $(PR)\n\nfactory-status: ## Show current iteration count and satisfaction score\n\t@echo \"--- Factory Status ---\"\n\t@if [ -f artifacts/factory/iteration_count.txt ]; then \\\n\t\techo \"Iteration: $$(cat artifacts/factory/iteration_count.txt)\"; \\\n\telse \\\n\t\techo \"Iteration: 0 (not started)\"; \\\n\tfi\n\t@if [ -f artifacts/factory/scenario_results.json ]; then \\\n\t\tpython -c \"import json; r=json.load(open('artifacts/factory/scenario_results.json')); print(f'Satisfaction: {r.get(\\\"satisfaction_score\\\", 0):.0%} ({r.get(\\\"passed\\\", 0)}/{r.get(\\\"total\\\", 0)} scenarios)')\"; \\\n\telse \\\n\t\techo \"Satisfaction: N/A (no results yet)\"; \\\n\tfi\n",
      "base": ".PHONY: deps install-hooks lint typecheck test docker-build docker-smoke whitepapers-acquire whitepapers-verify env-smoke train-smoke eval-smoke verify-learning dashboard validate run-scenarios compile-feedback nfr-check factory-local factory-status\n\ndeps:\n\tpip-compile requirements.in\n\tpip-compile requirements-dev.in\n\tpip install -r requirements.txt -r requirements-dev.txt\n\ninstall-hooks: ## Set up git hooks (ruff + mypy on every commit, no virtualenv needed)\n\tgit config core.hooksPath .githooks\n\t@echo \"\u2705 Git hooks installed from .githooks/\"\n\n# \u2500\u2500 Quality \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nlint:\n\truff check .\n\ntypecheck:\n\tmypy src\n\ntest:\n\tpytest -q\n\n# \u2500\u2500 Docker \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndocker-build:\n\tdocker build -f infra/docker/Dockerfile.train --build-arg BASE_IMAGE=python:3.12-slim -t minipong-train .\n\tdocker build -f infra/docker/Dockerfile.demo -t minipong-demo .\n\ndocker-smoke:\n\tdocker run --rm minipong-train python -m src.train.train_dqn --help\n\tdocker run --rm minipong-demo python -m src.train.record_video --help\n\n# \u2500\u2500 Whitepapers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nwhitepapers-acquire:\n\tpython scripts/acquire_whitepapers.py\n\nwhitepapers-verify:\n\tpython scripts/verify_whitepapers.py\n\n# \u2500\u2500 Environment \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nenv-smoke:\n\tpython -c \"from src.envs.minipong import MiniPongEnv; env=MiniPongEnv(); obs,_=env.reset(seed=0); assert obs.dtype.name=='uint8'; print(obs.shape)\"\n\n# \u2500\u2500 Training \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntrain-smoke:\n\tpython -m src.train.train_dqn --config configs/dqn_minipong.yaml --run-id smoke_run\n\neval-smoke:\n\tpython -m src.train.evaluate --run-id smoke_run --episodes 2 --seeds 1 2\n\nverify-learning:\n\tpython -m src.train.verify_learning --run-id smoke_run --min-return-gain -0.1 --min-hits-gain -0.1\n\n# \u2500\u2500 Dashboard \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndashboard:\n\tstreamlit run src/dashboard/app.py\n\n# \u2500\u2500 Validation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nvalidate: lint typecheck test docker-build docker-smoke env-smoke whitepapers-verify\n\n# \u2500\u2500 Dark Factory \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nrun-scenarios: ## Run holdout scenario evaluation\n\tpython scripts/run_scenarios.py\n\ncompile-feedback: ## Compile validation results into feedback markdown\n\tpython scripts/compile_feedback.py\n\nnfr-check: ## Run Gate 2 NFR checks (non-blocking quality analysis)\n\t@mkdir -p artifacts/factory\n\tpython scripts/nfr_checks.py --output artifacts/factory/nfr_results.json\n\tpython scripts/nfr_checks.py\n\nfactory-local: ## Run one factory iteration locally (Gate 1 \u2192 Gate 2 \u2192 Gate 3 \u2192 feedback)\n\t@mkdir -p artifacts/factory\n\t@echo \"=== Gate 1: lint + typecheck + test ===\"\n\t@make lint 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tLINT_EXIT=$$?; \\\n\tmake typecheck 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tTYPE_EXIT=$$?; \\\n\tmake test 2>&1 | tee -a artifacts/factory/ci_output.log; \\\n\tTEST_EXIT=$$?; \\\n\tif [ $$LINT_EXIT -ne 0 ] || [ $$TYPE_EXIT -ne 0 ] || [ $$TEST_EXIT -ne 0 ]; then \\\n\t\techo \"Gate 1 FAILED \u2014 skipping Gates 2-3\"; \\\n\t\techo '{\"total\":0,\"passed\":0,\"failed\":0,\"skipped\":0,\"satisfaction_score\":0.0,\"results\":[],\"timestamp\":\"N/A\",\"gate1_failed\":true}' > artifacts/factory/scenario_results.json; \\\n\telse \\\n\t\techo \"\"; \\\n\t\techo \"=== Gate 2: NFR checks (non-blocking) ===\"; \\\n\t\tmake nfr-check 2>&1 | tee -a artifacts/factory/ci_output.log || true; \\\n\t\techo \"\"; \\\n\t\techo \"=== Gate 3: Behavioral scenarios ===\"; \\\n\t\tpython scripts/run_scenarios.py --timeout 180 || true; \\\n\tfi\n\t@echo \"\"\n\t@echo \"=== Compiling feedback ===\"\n\t@python scripts/compile_feedback.py\n\t@echo \"\"\n\t@echo \"=== Factory iteration complete ===\"\n\t@make factory-status\n\nfactory-status: ## Show current iteration count and satisfaction score\n\t@echo \"--- Factory Status ---\"\n\t@if [ -f artifacts/factory/iteration_count.txt ]; then \\\n\t\techo \"Iteration: $$(cat artifacts/factory/iteration_count.txt)\"; \\\n\telse \\\n\t\techo \"Iteration: 0 (not started)\"; \\\n\tfi\n\t@if [ -f artifacts/factory/scenario_results.json ]; then \\\n\t\tpython -c \"import json; r=json.load(open('artifacts/factory/scenario_results.json')); print(f'Satisfaction: {r.get(\\\"satisfaction_score\\\", 0):.0%} ({r.get(\\\"passed\\\", 0)}/{r.get(\\\"total\\\", 0)} scenarios)')\"; \\\n\telse \\\n\t\techo \"Satisfaction: N/A (no results yet)\"; \\\n\tfi\n"
    },
    "docs/decisions/decision_log.json": {
      "additions": 53,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/docs/decisions/decision_log.json b/docs/decisions/decision_log.json\nnew file mode 100644\nindex 0000000..1f89c9f\n--- /dev/null\n+++ b/docs/decisions/decision_log.json\n@@ -0,0 +1,53 @@\n+{\n+  \"version\": 1,\n+  \"decisions\": [\n+    {\n+      \"id\": \"PR6-1\",\n+      \"globalSeq\": 1,\n+      \"prNumber\": 6,\n+      \"title\": \"Wire existing record_video() rather than rewriting\",\n+      \"rationale\": \"The video recording function was already implemented and tested \u2014 it just wasn't called from the training loop.\",\n+      \"body\": \"Rather than creating a new video recording mechanism or modifying the evaluation flow, we simply imported the existing <code>record_video()</code> function and called it at the same point where checkpoints and metrics are already saved. This is the minimal change that fixes the scenario failure.\",\n+      \"zones\": [\n+        \"training\"\n+      ],\n+      \"files\": [\n+        {\n+          \"path\": \"src/train/train_dqn.py\",\n+          \"change\": \"Added import and call to record_video() in eval block\"\n+        },\n+        {\n+          \"path\": \"src/train/record_video.py\",\n+          \"change\": \"Added frame_stack parameter for config compatibility\"\n+        }\n+      ],\n+      \"verified\": true,\n+      \"mergedAt\": \"2026-02-26T04:26:25Z\",\n+      \"prUrl\": \"https://github.com/joeyfezster/building_ai_w_ai/pull/6\",\n+      \"headSha\": \"bce032a\",\n+      \"status\": \"active\"\n+    },\n+    {\n+      \"id\": \"PR6-2\",\n+      \"globalSeq\": 2,\n+      \"prNumber\": 6,\n+      \"title\": \"Parameterize frame_stack instead of hardcoding\",\n+      \"rationale\": \"Hardcoded frame_stack=4 caused shape mismatches when training config used a different value.\",\n+      \"body\": \"The original <code>record_video()</code> hardcoded <code>frame_stack=4</code>. When the test suite runs with <code>frame_stack=2</code>, loading a checkpoint into a model with mismatched input channels causes a RuntimeError. Adding the parameter with default=4 maintains backward compatibility while allowing the training loop to pass the correct value.\",\n+      \"zones\": [\n+        \"training\"\n+      ],\n+      \"files\": [\n+        {\n+          \"path\": \"src/train/record_video.py\",\n+          \"change\": \"Added frame_stack parameter with default=4\"\n+        }\n+      ],\n+      \"verified\": true,\n+      \"mergedAt\": \"2026-02-26T04:26:25Z\",\n+      \"prUrl\": \"https://github.com/joeyfezster/building_ai_w_ai/pull/6\",\n+      \"headSha\": \"bce032a\",\n+      \"status\": \"active\"\n+    }\n+  ]\n+}\n",
      "raw": "{\n  \"version\": 1,\n  \"decisions\": [\n    {\n      \"id\": \"PR6-1\",\n      \"globalSeq\": 1,\n      \"prNumber\": 6,\n      \"title\": \"Wire existing record_video() rather than rewriting\",\n      \"rationale\": \"The video recording function was already implemented and tested \u2014 it just wasn't called from the training loop.\",\n      \"body\": \"Rather than creating a new video recording mechanism or modifying the evaluation flow, we simply imported the existing <code>record_video()</code> function and called it at the same point where checkpoints and metrics are already saved. This is the minimal change that fixes the scenario failure.\",\n      \"zones\": [\n        \"training\"\n      ],\n      \"files\": [\n        {\n          \"path\": \"src/train/train_dqn.py\",\n          \"change\": \"Added import and call to record_video() in eval block\"\n        },\n        {\n          \"path\": \"src/train/record_video.py\",\n          \"change\": \"Added frame_stack parameter for config compatibility\"\n        }\n      ],\n      \"verified\": true,\n      \"mergedAt\": \"2026-02-26T04:26:25Z\",\n      \"prUrl\": \"https://github.com/joeyfezster/building_ai_w_ai/pull/6\",\n      \"headSha\": \"bce032a\",\n      \"status\": \"active\"\n    },\n    {\n      \"id\": \"PR6-2\",\n      \"globalSeq\": 2,\n      \"prNumber\": 6,\n      \"title\": \"Parameterize frame_stack instead of hardcoding\",\n      \"rationale\": \"Hardcoded frame_stack=4 caused shape mismatches when training config used a different value.\",\n      \"body\": \"The original <code>record_video()</code> hardcoded <code>frame_stack=4</code>. When the test suite runs with <code>frame_stack=2</code>, loading a checkpoint into a model with mismatched input channels causes a RuntimeError. Adding the parameter with default=4 maintains backward compatibility while allowing the training loop to pass the correct value.\",\n      \"zones\": [\n        \"training\"\n      ],\n      \"files\": [\n        {\n          \"path\": \"src/train/record_video.py\",\n          \"change\": \"Added frame_stack parameter with default=4\"\n        }\n      ],\n      \"verified\": true,\n      \"mergedAt\": \"2026-02-26T04:26:25Z\",\n      \"prUrl\": \"https://github.com/joeyfezster/building_ai_w_ai/pull/6\",\n      \"headSha\": \"bce032a\",\n      \"status\": \"active\"\n    }\n  ]\n}\n",
      "base": ""
    },
    "scripts/persist_decisions.py": {
      "additions": 241,
      "deletions": 0,
      "status": "added",
      "binary": false,
      "diff": "diff --git a/scripts/persist_decisions.py b/scripts/persist_decisions.py\nnew file mode 100644\nindex 0000000..2aec4d1\n--- /dev/null\n+++ b/scripts/persist_decisions.py\n@@ -0,0 +1,241 @@\n+#!/usr/bin/env python3\n+\"\"\"Persist PR review pack decisions to the cumulative decision log.\n+\n+Extracts decisions from ReviewPackData JSON (or from rendered HTML as fallback)\n+and appends them to docs/decisions/decision_log.json.\n+\n+Usage:\n+    python scripts/persist_decisions.py --pr 6                           # persist from HTML\n+    python scripts/persist_decisions.py --pr 6 --data /tmp/pr6_data.json # persist from JSON\n+    python scripts/persist_decisions.py --pr 6 --dry-run                 # preview only\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import re\n+import subprocess\n+import sys\n+from pathlib import Path\n+\n+REPO_ROOT = Path(__file__).resolve().parent.parent\n+DEFAULT_LOG = REPO_ROOT / \"docs\" / \"decisions\" / \"decision_log.json\"\n+REPO_SLUG = \"joeyfezster/building_ai_w_ai\"\n+\n+\n+def load_decision_log(path: Path) -> dict:\n+    \"\"\"Load the existing decision log, or create the initial structure.\"\"\"\n+    if path.exists():\n+        return json.loads(path.read_text())\n+    return {\"version\": 1, \"decisions\": []}\n+\n+\n+def next_global_seq(log: dict) -> int:\n+    \"\"\"Compute the next globalSeq from the existing log.\"\"\"\n+    if not log[\"decisions\"]:\n+        return 1\n+    return max(d[\"globalSeq\"] for d in log[\"decisions\"]) + 1\n+\n+\n+def existing_ids(log: dict) -> set[str]:\n+    \"\"\"Return the set of decision IDs already in the log.\"\"\"\n+    return {d[\"id\"] for d in log[\"decisions\"]}\n+\n+\n+def extract_decisions_from_json(path: Path) -> tuple[list[dict], dict]:\n+    \"\"\"Extract decisions and header from a ReviewPackData JSON file.\"\"\"\n+    data = json.loads(path.read_text())\n+    return data.get(\"decisions\", []), data.get(\"header\", {})\n+\n+\n+def extract_decisions_from_html(path: Path) -> tuple[list[dict], dict]:\n+    \"\"\"Extract decisions from rendered HTML by parsing the embedded DATA object.\n+\n+    The review pack HTML contains a `const DATA = {...};` block with the full\n+    ReviewPackData JSON. We extract it with a regex and parse it.\n+    \"\"\"\n+    html = path.read_text()\n+    match = re.search(r\"const DATA = ({.*?});\\s*\\n\", html, re.DOTALL)\n+    if not match:\n+        print(\"ERROR: Could not find 'const DATA = {...}' in HTML file.\")\n+        sys.exit(1)\n+\n+    # The embedded JSON may contain <\\/script> escapes from the renderer.\n+    raw = match.group(1).replace(r\"<\\/script\", \"</script\")\n+\n+    data = json.loads(raw)\n+    return data.get(\"decisions\", []), data.get(\"header\", {})\n+\n+\n+def get_merge_timestamp(pr_number: int) -> str:\n+    \"\"\"Get the merge timestamp for a PR via gh CLI.\n+\n+    Fails explicitly if the timestamp cannot be determined \u2014 never\n+    fabricates a timestamp, as that would corrupt the decision log's\n+    chronology and auditability.\n+    \"\"\"\n+    try:\n+        result = subprocess.run(\n+            [\"gh\", \"pr\", \"view\", str(pr_number), \"--json\", \"mergedAt\", \"-q\", \".mergedAt\"],\n+            capture_output=True,\n+            text=True,\n+            timeout=15,\n+        )\n+        if result.returncode == 0 and result.stdout.strip():\n+            return result.stdout.strip()\n+    except Exception:\n+        pass\n+\n+    # PR may not be merged yet \u2014 use HEAD commit timestamp as deterministic fallback\n+    try:\n+        result = subprocess.run(\n+            [\"git\", \"log\", \"-1\", \"--format=%aI\"],\n+            capture_output=True,\n+            text=True,\n+            timeout=10,\n+        )\n+        if result.returncode == 0 and result.stdout.strip():\n+            return result.stdout.strip()\n+    except Exception:\n+        pass\n+\n+    print(\"ERROR: Could not determine merge timestamp from gh or git log.\")\n+    print(\"  Ensure 'gh' is authenticated or run from within the git repo.\")\n+    sys.exit(1)\n+\n+\n+def get_pr_url(pr_number: int) -> str:\n+    \"\"\"Build the PR URL.\"\"\"\n+    return f\"https://github.com/{REPO_SLUG}/pull/{pr_number}\"\n+\n+\n+def build_persisted_decision(\n+    raw: dict,\n+    pr_number: int,\n+    global_seq: int,\n+    merged_at: str,\n+    head_sha: str,\n+) -> dict:\n+    \"\"\"Convert a raw Decision (from review pack) to the persisted format.\"\"\"\n+    local_seq = raw[\"number\"]\n+    zones_raw = raw.get(\"zones\", \"\")\n+    zones = zones_raw.split() if isinstance(zones_raw, str) else zones_raw\n+\n+    return {\n+        \"id\": f\"PR{pr_number}-{local_seq}\",\n+        \"globalSeq\": global_seq,\n+        \"prNumber\": pr_number,\n+        \"title\": raw[\"title\"],\n+        \"rationale\": raw.get(\"rationale\", \"\"),\n+        \"body\": raw.get(\"body\", \"\"),\n+        \"zones\": zones,\n+        \"files\": raw.get(\"files\", []),\n+        \"verified\": raw.get(\"verified\", False),\n+        \"mergedAt\": merged_at,\n+        \"prUrl\": get_pr_url(pr_number),\n+        \"headSha\": head_sha,\n+        \"status\": \"active\",\n+    }\n+\n+\n+def main() -> int:\n+    parser = argparse.ArgumentParser(\n+        description=\"Persist PR review pack decisions to the decision log\"\n+    )\n+    parser.add_argument(\n+        \"--pr\", type=int, required=True, help=\"PR number whose decisions to persist\"\n+    )\n+    parser.add_argument(\n+        \"--data\",\n+        type=str,\n+        default=None,\n+        help=\"Path to ReviewPackData JSON file (falls back to rendered HTML)\",\n+    )\n+    parser.add_argument(\n+        \"--log\",\n+        type=str,\n+        default=None,\n+        help=f\"Path to decision log (default: {DEFAULT_LOG})\",\n+    )\n+    parser.add_argument(\n+        \"--dry-run\", action=\"store_true\", help=\"Preview without modifying the log\"\n+    )\n+    args = parser.parse_args()\n+\n+    log_path = Path(args.log) if args.log else DEFAULT_LOG\n+\n+    # --- Load decisions from source ---\n+    decisions: list[dict]\n+    header: dict\n+\n+    if args.data:\n+        data_path = Path(args.data)\n+        if not data_path.exists():\n+            print(f\"ERROR: Data file not found: {data_path}\")\n+            return 1\n+        decisions, header = extract_decisions_from_json(data_path)\n+    else:\n+        # Fall back to rendered HTML\n+        html_path = REPO_ROOT / \"docs\" / f\"pr{args.pr}_review_pack.html\"\n+        if not html_path.exists():\n+            print(f\"ERROR: No data file provided and no HTML found at {html_path}\")\n+            print(f\"  Provide --data /path/to/pr{args.pr}_review_pack_data.json\")\n+            return 1\n+        print(f\"No --data provided, extracting from HTML: {html_path}\")\n+        decisions, header = extract_decisions_from_html(html_path)\n+\n+    if not decisions:\n+        print(f\"No decisions found for PR #{args.pr}. Nothing to persist.\")\n+        return 0\n+\n+    # --- Load existing log ---\n+    log = load_decision_log(log_path)\n+    known = existing_ids(log)\n+    seq = next_global_seq(log)\n+\n+    # --- Get metadata ---\n+    merged_at = get_merge_timestamp(args.pr)\n+    head_sha = header.get(\"headSha\", \"unknown\")\n+\n+    # --- Build and append ---\n+    added = 0\n+    skipped = 0\n+    for raw in decisions:\n+        local_seq = raw[\"number\"]\n+        decision_id = f\"PR{args.pr}-{local_seq}\"\n+\n+        if decision_id in known:\n+            print(f\"  SKIP: {decision_id} already exists in log\")\n+            skipped += 1\n+            continue\n+\n+        persisted = build_persisted_decision(raw, args.pr, seq, merged_at, head_sha)\n+\n+        if args.dry_run:\n+            print(f\"\\n  WOULD ADD: {decision_id} (globalSeq={seq})\")\n+            print(f\"    Title: {persisted['title']}\")\n+            print(f\"    Zones: {persisted['zones']}\")\n+            print(f\"    Files: {len(persisted['files'])}\")\n+        else:\n+            log[\"decisions\"].append(persisted)\n+            print(f\"  ADD: {decision_id} (globalSeq={seq}) \u2014 {persisted['title']}\")\n+\n+        seq += 1\n+        added += 1\n+\n+    # --- Write ---\n+    if not args.dry_run and added > 0:\n+        log_path.parent.mkdir(parents=True, exist_ok=True)\n+        log_path.write_text(json.dumps(log, indent=2, ensure_ascii=False) + \"\\n\")\n+        print(f\"\\nPersisted {added} decisions from PR #{args.pr} to {log_path}\")\n+    elif args.dry_run:\n+        print(f\"\\nDry run: {added} decisions would be added, {skipped} skipped\")\n+    else:\n+        print(f\"\\nNo new decisions to add ({skipped} already in log)\")\n+\n+    return 0\n+\n+\n+if __name__ == \"__main__\":\n+    sys.exit(main())\n",
      "raw": "#!/usr/bin/env python3\n\"\"\"Persist PR review pack decisions to the cumulative decision log.\n\nExtracts decisions from ReviewPackData JSON (or from rendered HTML as fallback)\nand appends them to docs/decisions/decision_log.json.\n\nUsage:\n    python scripts/persist_decisions.py --pr 6                           # persist from HTML\n    python scripts/persist_decisions.py --pr 6 --data /tmp/pr6_data.json # persist from JSON\n    python scripts/persist_decisions.py --pr 6 --dry-run                 # preview only\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nREPO_ROOT = Path(__file__).resolve().parent.parent\nDEFAULT_LOG = REPO_ROOT / \"docs\" / \"decisions\" / \"decision_log.json\"\nREPO_SLUG = \"joeyfezster/building_ai_w_ai\"\n\n\ndef load_decision_log(path: Path) -> dict:\n    \"\"\"Load the existing decision log, or create the initial structure.\"\"\"\n    if path.exists():\n        return json.loads(path.read_text())\n    return {\"version\": 1, \"decisions\": []}\n\n\ndef next_global_seq(log: dict) -> int:\n    \"\"\"Compute the next globalSeq from the existing log.\"\"\"\n    if not log[\"decisions\"]:\n        return 1\n    return max(d[\"globalSeq\"] for d in log[\"decisions\"]) + 1\n\n\ndef existing_ids(log: dict) -> set[str]:\n    \"\"\"Return the set of decision IDs already in the log.\"\"\"\n    return {d[\"id\"] for d in log[\"decisions\"]}\n\n\ndef extract_decisions_from_json(path: Path) -> tuple[list[dict], dict]:\n    \"\"\"Extract decisions and header from a ReviewPackData JSON file.\"\"\"\n    data = json.loads(path.read_text())\n    return data.get(\"decisions\", []), data.get(\"header\", {})\n\n\ndef extract_decisions_from_html(path: Path) -> tuple[list[dict], dict]:\n    \"\"\"Extract decisions from rendered HTML by parsing the embedded DATA object.\n\n    The review pack HTML contains a `const DATA = {...};` block with the full\n    ReviewPackData JSON. We extract it with a regex and parse it.\n    \"\"\"\n    html = path.read_text()\n    match = re.search(r\"const DATA = ({.*?});\\s*\\n\", html, re.DOTALL)\n    if not match:\n        print(\"ERROR: Could not find 'const DATA = {...}' in HTML file.\")\n        sys.exit(1)\n\n    # The embedded JSON may contain <\\/script> escapes from the renderer.\n    raw = match.group(1).replace(r\"<\\/script\", \"</script\")\n\n    data = json.loads(raw)\n    return data.get(\"decisions\", []), data.get(\"header\", {})\n\n\ndef get_merge_timestamp(pr_number: int) -> str:\n    \"\"\"Get the merge timestamp for a PR via gh CLI.\n\n    Fails explicitly if the timestamp cannot be determined \u2014 never\n    fabricates a timestamp, as that would corrupt the decision log's\n    chronology and auditability.\n    \"\"\"\n    try:\n        result = subprocess.run(\n            [\"gh\", \"pr\", \"view\", str(pr_number), \"--json\", \"mergedAt\", \"-q\", \".mergedAt\"],\n            capture_output=True,\n            text=True,\n            timeout=15,\n        )\n        if result.returncode == 0 and result.stdout.strip():\n            return result.stdout.strip()\n    except Exception:\n        pass\n\n    # PR may not be merged yet \u2014 use HEAD commit timestamp as deterministic fallback\n    try:\n        result = subprocess.run(\n            [\"git\", \"log\", \"-1\", \"--format=%aI\"],\n            capture_output=True,\n            text=True,\n            timeout=10,\n        )\n        if result.returncode == 0 and result.stdout.strip():\n            return result.stdout.strip()\n    except Exception:\n        pass\n\n    print(\"ERROR: Could not determine merge timestamp from gh or git log.\")\n    print(\"  Ensure 'gh' is authenticated or run from within the git repo.\")\n    sys.exit(1)\n\n\ndef get_pr_url(pr_number: int) -> str:\n    \"\"\"Build the PR URL.\"\"\"\n    return f\"https://github.com/{REPO_SLUG}/pull/{pr_number}\"\n\n\ndef build_persisted_decision(\n    raw: dict,\n    pr_number: int,\n    global_seq: int,\n    merged_at: str,\n    head_sha: str,\n) -> dict:\n    \"\"\"Convert a raw Decision (from review pack) to the persisted format.\"\"\"\n    local_seq = raw[\"number\"]\n    zones_raw = raw.get(\"zones\", \"\")\n    zones = zones_raw.split() if isinstance(zones_raw, str) else zones_raw\n\n    return {\n        \"id\": f\"PR{pr_number}-{local_seq}\",\n        \"globalSeq\": global_seq,\n        \"prNumber\": pr_number,\n        \"title\": raw[\"title\"],\n        \"rationale\": raw.get(\"rationale\", \"\"),\n        \"body\": raw.get(\"body\", \"\"),\n        \"zones\": zones,\n        \"files\": raw.get(\"files\", []),\n        \"verified\": raw.get(\"verified\", False),\n        \"mergedAt\": merged_at,\n        \"prUrl\": get_pr_url(pr_number),\n        \"headSha\": head_sha,\n        \"status\": \"active\",\n    }\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(\n        description=\"Persist PR review pack decisions to the decision log\"\n    )\n    parser.add_argument(\n        \"--pr\", type=int, required=True, help=\"PR number whose decisions to persist\"\n    )\n    parser.add_argument(\n        \"--data\",\n        type=str,\n        default=None,\n        help=\"Path to ReviewPackData JSON file (falls back to rendered HTML)\",\n    )\n    parser.add_argument(\n        \"--log\",\n        type=str,\n        default=None,\n        help=f\"Path to decision log (default: {DEFAULT_LOG})\",\n    )\n    parser.add_argument(\n        \"--dry-run\", action=\"store_true\", help=\"Preview without modifying the log\"\n    )\n    args = parser.parse_args()\n\n    log_path = Path(args.log) if args.log else DEFAULT_LOG\n\n    # --- Load decisions from source ---\n    decisions: list[dict]\n    header: dict\n\n    if args.data:\n        data_path = Path(args.data)\n        if not data_path.exists():\n            print(f\"ERROR: Data file not found: {data_path}\")\n            return 1\n        decisions, header = extract_decisions_from_json(data_path)\n    else:\n        # Fall back to rendered HTML\n        html_path = REPO_ROOT / \"docs\" / f\"pr{args.pr}_review_pack.html\"\n        if not html_path.exists():\n            print(f\"ERROR: No data file provided and no HTML found at {html_path}\")\n            print(f\"  Provide --data /path/to/pr{args.pr}_review_pack_data.json\")\n            return 1\n        print(f\"No --data provided, extracting from HTML: {html_path}\")\n        decisions, header = extract_decisions_from_html(html_path)\n\n    if not decisions:\n        print(f\"No decisions found for PR #{args.pr}. Nothing to persist.\")\n        return 0\n\n    # --- Load existing log ---\n    log = load_decision_log(log_path)\n    known = existing_ids(log)\n    seq = next_global_seq(log)\n\n    # --- Get metadata ---\n    merged_at = get_merge_timestamp(args.pr)\n    head_sha = header.get(\"headSha\", \"unknown\")\n\n    # --- Build and append ---\n    added = 0\n    skipped = 0\n    for raw in decisions:\n        local_seq = raw[\"number\"]\n        decision_id = f\"PR{args.pr}-{local_seq}\"\n\n        if decision_id in known:\n            print(f\"  SKIP: {decision_id} already exists in log\")\n            skipped += 1\n            continue\n\n        persisted = build_persisted_decision(raw, args.pr, seq, merged_at, head_sha)\n\n        if args.dry_run:\n            print(f\"\\n  WOULD ADD: {decision_id} (globalSeq={seq})\")\n            print(f\"    Title: {persisted['title']}\")\n            print(f\"    Zones: {persisted['zones']}\")\n            print(f\"    Files: {len(persisted['files'])}\")\n        else:\n            log[\"decisions\"].append(persisted)\n            print(f\"  ADD: {decision_id} (globalSeq={seq}) \u2014 {persisted['title']}\")\n\n        seq += 1\n        added += 1\n\n    # --- Write ---\n    if not args.dry_run and added > 0:\n        log_path.parent.mkdir(parents=True, exist_ok=True)\n        log_path.write_text(json.dumps(log, indent=2, ensure_ascii=False) + \"\\n\")\n        print(f\"\\nPersisted {added} decisions from PR #{args.pr} to {log_path}\")\n    elif args.dry_run:\n        print(f\"\\nDry run: {added} decisions would be added, {skipped} skipped\")\n    else:\n        print(f\"\\nNo new decisions to add ({skipped} already in log)\")\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
      "base": ""
    }
  }
}